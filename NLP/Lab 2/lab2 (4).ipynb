{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d28eaf7",
   "metadata": {},
   "source": [
    "# 1. Summarize the paper on BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c9d55",
   "metadata": {},
   "source": [
    "# 2. In regular expressions, what does \\d, \\D, \\w, \\W, \\s, \\S, {n}, {n,m}, {n,}, {,m} mean?\n",
    "\n",
    "\\d - matches any decimal digit\n",
    "\n",
    "\\D - matches any non-digit character\n",
    "\n",
    "\\w - matches alphanumeric characters\n",
    "\n",
    "\\W - matches nonword character\n",
    "\n",
    "\\s - matches whitespace character\n",
    "\n",
    "\\S - matches any non-whitespace character\n",
    "\n",
    "The brackets specify how many copy of the previous re should be matched. so {n} mean exactly n copies of the previous re should be matched. {n,m} means to match from n to m copies of the preceding re. Omitting m gives an infinite upper bound of repetitions, and omitting n gives a lower bound of 0 repititions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5143dd",
   "metadata": {},
   "source": [
    "# 3. Create a Python based tokenizer in NLTK that replaces contractions (I’m, You’re, didn’t) into the expanded forms (I am, You are, did not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8591e5ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'glad', 'you', 'are', 'here', '.', 'did', 'not', 'think', 'you', 'would', 'make', 'it', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\17023\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# I used ChatGPT for part of this function but it didn't work at first so I had to fix it.\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def expand_contractions(sentence):\n",
    "    contractions = {\n",
    "        \"i'm\": \"i am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"i'd\": \"i would\"\n",
    "    }\n",
    "    tk = WhitespaceTokenizer()\n",
    "    words = tk.tokenize(sentence)\n",
    "    expanded = []\n",
    "    for word in words:\n",
    "        if word.lower() in contractions:\n",
    "            expanded.append(contractions[word.lower()])\n",
    "        else:\n",
    "            expanded.append(word)\n",
    "    expanded = \" \".join(expanded)\n",
    "    return word_tokenize(expanded)\n",
    "\n",
    "# Example usage:\n",
    "sentence = \"I'm glad you're here. Didn't think you'd make it.\"\n",
    "expanded = expand_contractions(sentence)\n",
    "print(expanded)  # Output: \"I am glad you are here. did not think you would make it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b02e9a",
   "metadata": {},
   "source": [
    "# 4. Implement the BPE algorithm with the following interface\n",
    "\n",
    "Your normalization should expand the contractions you implemented in problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "355354c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "import nltk\n",
    "\n",
    "USE_STOP_WORDS = False\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "40e9c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def normalize(self, text):\n",
    "        contractions = {\n",
    "            \"i'm\": \"i am\",\n",
    "            \"you're\": \"you are\",\n",
    "            \"he's\": \"he is\",\n",
    "            \"she's\": \"she is\",\n",
    "            \"it's\": \"it is\",\n",
    "            \"we're\": \"we are\",\n",
    "            \"they're\": \"they are\",\n",
    "            \"that's\": \"that is\",\n",
    "            \"there's\": \"there is\",\n",
    "            \"don't\": \"do not\",\n",
    "            \"doesn't\": \"does not\",\n",
    "            \"didn't\": \"did not\",\n",
    "            \"can't\": \"cannot\",\n",
    "            \"couldn't\": \"could not\",\n",
    "            \"won't\": \"will not\",\n",
    "            \"wouldn't\": \"would not\",\n",
    "            \"shouldn't\": \"should not\",\n",
    "            \"mightn't\": \"might not\",\n",
    "            \"might've\": \"might have\",\n",
    "            \"mustn't\": \"must not\",\n",
    "            \"you'd\": \"you would\",\n",
    "            \"i'd\": \"i would\"\n",
    "        }\n",
    "        #tk = WhitespaceTokenizer()\n",
    "        #words = tk.tokenize(sentence)\n",
    "        expanded = []\n",
    "        for word in text:\n",
    "            if word.lower() in contractions:\n",
    "                expanded.append(contractions[word.lower()])\n",
    "            else:\n",
    "                expanded.append(word)\n",
    "        expanded = \" \".join(expanded)\n",
    "        return word_tokenize(expanded)\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        tokens = [s.split() for s in corpus]\n",
    "        if USE_STOP_WORDS:\n",
    "            stopword_list=nltk.corpus.stopwords.words('english')\n",
    "            tokens = [[w for w in s if w.lower() not in stopword_list] for s in tokens]\n",
    "\n",
    "        vocab_set = set(''.join(corpus)).union(['</w>'])\n",
    "\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for s in tokens:\n",
    "            for w in s:\n",
    "                vocab[' '.join(list(w)) + ' </w>'] += 1\n",
    "\n",
    "        num_merges = self.vocab_size\n",
    "        for i in range(num_merges):\n",
    "#             if i % 100 == 0:\n",
    "#                 print(f\"Iteration = {i}\")\n",
    "            pairs = get_stats(vocab)\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = merge_vocab(best, vocab)\n",
    "            vocab_set = vocab_set.union([''.join(best)])\n",
    "            \n",
    "        word2int = {}\n",
    "        for w in vocab_set:\n",
    "            word2int[w] = len(word2int)\n",
    "            \n",
    "        return tokens, vocab_set, word2int\n",
    "    \n",
    "    def encode(self, word, vocab_set, tokens, word2int):\n",
    "        subunits = list(word) + ['</w>']\n",
    "        changed = True\n",
    "        while changed:\n",
    "            token = ''\n",
    "            tokens = []\n",
    "            for unit in subunits:\n",
    "                if token + unit in vocab_set:\n",
    "                    token = token + unit\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "                    token = unit\n",
    "            tokens.append(token)\n",
    "            changed = len(subunits) != len(tokens)\n",
    "            subunits = tokens\n",
    "        return [word2int.get(token, 0) for token in tokens], tokens\n",
    "    \n",
    "    def decode(self, num, word2int):\n",
    "        return list(word2int.keys())[list(word2int.values()).index(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d40ce2a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161192 56057\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "text = brown.words()\n",
    "\n",
    "print(len(text), len(set(text)))\n",
    "\n",
    "tokenizer = Tokenizer(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e8a17020",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_text = tokenizer.normalize(text)\n",
    "brown_tokens, v_set, w2int = tokenizer.train(norm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7c3d3fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([38], ['would</w>'])\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode('would', v_set, brown_tokens, w2int)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1240f519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would</w>\n"
     ]
    }
   ],
   "source": [
    "num = 38\n",
    "decoding = tokenizer.decode(num, w2int)\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2856a5e",
   "metadata": {},
   "source": [
    "# 5. Using the package https://www.nltk.org/api/nltk.chat.html#module-nltk.chat, you will create a regular expression based chatbot that answers the following questions.  \n",
    "\n",
    "    a. What’s the temperature today? (check for the temperature at weather channel)\n",
    "    b. What’s my zip code? (based on the person’s location)\n",
    "    c. How much is $19.99 in <currency>? (You can search Google for “exchange rate between us and <currency>”)\n",
    "    d. What’s the definition of <word>? (you will need to look for the definition at Wikipedia or dictionary)\n",
    "    \n",
    "The chatbot structure from NLTK uses static string responses, you have to modify it (as in the example below) to allow for functional objects that can parse the web, for example.\n",
    "\n",
    "Note that you will need to process HTML to create the answers, and you will use Beautiful Soup to do that. \n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.html?highlight=select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "05098c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "                                  Chatbot!                                 \n",
      "***************************************************************************\n",
      "Welcome.\n",
      ">hello what is the temperature\n",
      "The current temperature in Santa Clara is 53°\n",
      ">Cool. Where am I?\n",
      "The current ZIP code is 95138.\n",
      ">how much money is 13 dollars in yuan\n",
      "The correct amount is 89.84 Chinese Yuan\n",
      ">Great. What is the definition of beautiful?\n",
      "The definition(s) of beautiful is/are: \n",
      "1. having qualities of beauty : exciting aesthetic pleasure\n",
      "2. generally pleasing : excellent\n",
      "I hope this answers your question.\n",
      ">quit\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk.chat.util import Chat, reflections\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import geocoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def get_temp(prompt):\n",
    "    location_url = \"https://weather.com/weather/today/l/4fb2eb4b20edcb606887ba1528d1e7f0ca27f832876ed59016bbbf08547ad493\"\n",
    "    response = requests.get(location_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    temperature = soup.find('span', {'class': 'CurrentConditions--tempValue--MHmYY'}).get_text()\n",
    "    return f\"The current temperature in Santa Clara is {temperature}\"\n",
    "\n",
    "def get_zip_code(prompt):\n",
    "    g = geocoder.ip('me')\n",
    "    zipcode = g.postal\n",
    "\n",
    "    return f\"The current ZIP code is {zipcode}.\"\n",
    "\n",
    "def google_search_url(query):\n",
    "    query = query.replace(' ', '+')\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    return url\n",
    "\n",
    "def currency_convert(prompt):\n",
    "    prompt_url = google_search_url(prompt)\n",
    "    response = requests.get(prompt_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    amount = soup.find('div', {'class': 'BNeawe iBp4i AP7Wnd'}).find('div', {'class': 'BNeawe iBp4i AP7Wnd'}).get_text()\n",
    "    return f\"The correct amount is {amount}\"\n",
    "\n",
    "def get_word(prompt):\n",
    "    tokens = word_tokenize(prompt)\n",
    "    index = -1\n",
    "    while re.match(r\"[\\.\\?\\!);:]|mean(ing)?\",tokens[index]):\n",
    "        index -= 1\n",
    "    return tokens[index]\n",
    "\n",
    "def get_definition(prompt):\n",
    "    word = get_word(prompt)\n",
    "    api_key = \"3ac25ed7-e378-4c27-aaa4-1128d7d12335\"\n",
    "    url = f\"https://dictionaryapi.com/api/v3/references/collegiate/json/{word}?key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        definitions = response.json()[0].get(\"shortdef\")\n",
    "        print(f\"The definition(s) of {word} is/are: \")\n",
    "        for i, definition in enumerate(definitions):\n",
    "            print(f\"{i+1}. {definition}\")\n",
    "        return \"I hope this answers your question.\"\n",
    "    else:\n",
    "        return \"Sorry, the word could not be found in the dictionary.\"\n",
    "\n",
    "responses = (\n",
    "    (\n",
    "        r\"(hello(.*))|(good [a-zA-Z]+)|((.*)[Tt]emperature(.*))\",\n",
    "        (\n",
    "            get_temp,\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        r\"((.*)[zZ]ip [cC]ode(.*))|(.*)[zZ]ip(.*)|(.*)[lL]ocation(.*)|(.*)[wW]here(.*)\",\n",
    "        (\n",
    "            get_zip_code,\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        r\"(.*)\\$?\\d+(\\.\\d+)?%?(.*)|(.*)[dD]ollars?(.*)\",\n",
    "        (\n",
    "            currency_convert,\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        r\"(.*)defin(e|ition)(.*)|(.*)[Ww]ord(.*)|(.*)mean(ing)?(.*)\",\n",
    "        (\n",
    "            get_definition,\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "def respond(self, str):\n",
    "    \"\"\"\n",
    "    Generate a response to the user input.\n",
    "    :type str: str\n",
    "    :param str: The string to be mapped\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    # check each pattern\n",
    "    for (pattern, response) in self._pairs:\n",
    "        match = pattern.match(str)\n",
    "\n",
    "        # did the pattern match?\n",
    "        if match:\n",
    "            resp = response[0]\n",
    "            resp = resp(str)\n",
    "            resp = self._wildcards(resp, match)  # process wildcards\n",
    "\n",
    "            # fix munged punctuation at the end\n",
    "            if resp[-2:] == \"?.\":\n",
    "                resp = resp[:-2] + \".\"\n",
    "            if resp[-2:] == \"??\":\n",
    "                resp = resp[:-2] + \"?\"\n",
    "            return resp\n",
    "\n",
    "chatbot = Chat(responses, reflections)\n",
    "Chat.respond = respond\n",
    "\n",
    "def chat():\n",
    "    print(\"*\" * 75)\n",
    "    print(\"Chatbot!\".center(75))\n",
    "    print(\"*\" * 75)\n",
    "    print(\"Welcome.\")\n",
    "\n",
    "    chatbot.converse()\n",
    "    \n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8eca0",
   "metadata": {},
   "source": [
    "# 6. Implement the spell checker for cell phones\n",
    "\n",
    "Have you ever tried to type in something quickly and because the keyboard size in the cell phone is much smaller than your finger, it typing in the neighboring letters?\n",
    "\n",
    "   a) You will implement the spell checker from the site: https://norvig.com/spell-correct.html\n",
    "\n",
    "   b) You will change your code to consider that replacements would only occur to neighboring words. For example, in the picture, the letter 'u' can be replaced by 'y' or 'i'.\n",
    "\n",
    "![Keyboard iPhone](keyboard.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4875bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2880c764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watch\n",
      "boy\n",
      "that\n"
     ]
    }
   ],
   "source": [
    "print(correction('katch'))\n",
    "print(correction('boz'))\n",
    "print(correction('ehat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3085533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "neighbors = {  # dictionary of neighboring words\n",
    "    'a' : ('s', ['q', 'w', 'd']),\n",
    "    'b' : (['v', 'n'], ['h', 'c', 'm']),\n",
    "    'c' : (['x', 'v'], ['f', 'z', 'b']),\n",
    "    'd' : (['s', 'f'], ['e', 'r', 'a', 'g', 'x']),\n",
    "    'e' : (['w', 'r'], ['s', 'd', 'q', 't']),\n",
    "    'f' : (['d', 'g'], ['r', 't', 'c', 's', 'h']),\n",
    "    'g' : (['f', 'h'], ['t', 'y', 'v', 'd', 'j']),\n",
    "    'h' : (['g', 'j'], ['y', 'u', 'b', 'f', 'k']),\n",
    "    'i' : (['u', 'o'], ['j', 'k', 'y', 'p']),\n",
    "    'j' : (['h', 'k'], ['u', 'i', 'n', 'g', 'l']),\n",
    "    'k' : (['j', 'l'], ['i', 'o', 'm', 'h']),\n",
    "    'l' : ('k', ['o', 'p', 'j']),\n",
    "    'm' : ('n', ['k', 'b']),\n",
    "    'n' : (['b', 'm'], ['j', 'v']),\n",
    "    'o' : (['i', 'p'], ['k', 'l', 'u']),\n",
    "    'p' : ('o', ['l', 'i']),\n",
    "    'q' : ('w', ['a', 'e']),\n",
    "    'r' : (['e', 't'], ['d', 'f', 'w', 'y']),\n",
    "    's' : (['a', 'd'], ['w', 'e', 'z', 'f']),\n",
    "    't' : (['r', 'y'], ['f', 'g', 'e', 'u']),\n",
    "    'u' : (['y', 'i'], ['h', 'j', 't', 'o']),\n",
    "    'v' : (['c', 'b'], ['g', 'x', 'n']),\n",
    "    'w' : (['q', 'e'], ['a', 's', 'r']),\n",
    "    'x' : (['z', 'c'], ['d', 'v']),\n",
    "    'y' : (['t', 'u'], ['g', 'h', 'r', 'i']),\n",
    "    'z' : ('x', ['s', 'c'])\n",
    "}\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in neighbors[R[0]][0]]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0bffd208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latch\n",
      "box\n",
      "what\n"
     ]
    }
   ],
   "source": [
    "print(correction('katch'))\n",
    "print(correction('boz'))\n",
    "print(correction('ehat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e6f23",
   "metadata": {},
   "source": [
    "# 7. Implement the Weighted Mininum Edit Distance\n",
    "\n",
    "In this problem, you will implement the weighted minimum edit distance algorithm of slide 94.\n",
    "\n",
    "You will consider the following.\n",
    "\n",
    "- delete and insertion cost is 1\n",
    "- substitution cost is 1 if it is in adjacent in the keyboard, like in problem 6.b\n",
    "- substitution cost is 2 if it is below or above, or two characters to the right or left (for example, in the example of 6.b, replacing a 'u' by a 't' or 'o' would have a cost of 2\n",
    "- substitution cost is infinity if that does not apply\n",
    "\n",
    "Run the algorithm for the two words: \n",
    "\n",
    "- 'caft' vs 'cat' \n",
    "- 'coffee' vs 'voffrt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a4feb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sub_cost(word1, word2, i, j):\n",
    "    if word2[j-1] in neighbors[word1[i-1]][0]:\n",
    "        return 1\n",
    "    elif word2[j-1] in neighbors[word1[i-1]][1]:\n",
    "        return 2\n",
    "    elif word1[i-1] == word2[j-1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1000000000\n",
    "\n",
    "def min_weight_dist(word1, word2):\n",
    "    m, n = len(word1), len(word2)\n",
    "    D = np.zeros((m+1,n+1))\n",
    "    D[0,0] = 0\n",
    "    \n",
    "    for i in range(m+1):\n",
    "        D[i,0] = i\n",
    "    for j in range(n+1):\n",
    "        D[0,j] = j\n",
    "    \n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            delete = D[i-1,j] + 1\n",
    "            insert = D[i,j-1] + 1\n",
    "            substitute = D[i-1,j-1] + sub_cost(word1,word2,i,j)\n",
    "            D[i,j] = min([delete, insert, substitute])\n",
    "            #print(f\"i = {i}, j = {j}, min = {min([delete, insert, substitute])}\")\n",
    "            #print(f\"Matrix = \\n{D}\")\n",
    "    \n",
    "    return int(D[m,n])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d691014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(min_weight_dist('caft','cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8317b582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(min_weight_dist('coffee','voffrt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ec525dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(min_weight_dist('intention','execution'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:elen644]",
   "language": "python",
   "name": "conda-env-elen644-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
