{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this lab, you will see the power of word embeddings, and how embeddings can be used in different applications.\n",
    "\n",
    "Summarize the papers that were distributed with the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper review of \"Efficient Estimation of Word Representations in Vector Space\"(1301.3781):\n",
    "The paper \"Efficient Estimation of Word Representations in Vector Space\" by Tomas Mikolov et al., published in 2013, introduces the Word2Vec algorithm, which revolutionized the field of natural language processing by providing an efficient and effective way to learn word embeddings or representations in a vector space.\n",
    "\n",
    "The authors propose two main models in the paper: Continuous Bag-of-Words (CBOW) and Skip-gram. These models are neural network architectures that aim to capture the meaning of words based on their context.\n",
    "\n",
    "The Continuous Bag-of-Words (CBOW) model predicts the current word given its surrounding context words. It takes a fixed-size window of context words as input and predicts the target word in the center of the window. The objective is to maximize the probability of the target word given its context. In contrast, the Skip-gram model does the opposite; it predicts the context words given a target word. It takes a target word as input and tries to predict the context words within a certain window around the target word.\n",
    "\n",
    "Both models are trained using a two-layer neural network, with the input layer representing the one-hot encoded vectors of words and the output layer representing the probability distribution over words. The models are trained to minimize the negative log-likelihood of the observed data.\n",
    "\n",
    "To address the computational inefficiency of traditional neural network training approaches, the authors propose two techniques: hierarchical softmax and negative sampling. Hierarchical softmax reduces the computational cost of calculating the output layer probabilities by using a binary tree structure to efficiently traverse the vocabulary. Negative sampling, on the other hand, involves training the model to distinguish between the correct word and randomly sampled incorrect words, significantly speeding up the training process.\n",
    "\n",
    "The paper also discusses the evaluation of word embeddings using various tasks, including word analogy, word similarity, and word clustering. The authors demonstrate that the learned word embeddings capture meaningful semantic and syntactic relationships between words and can be successfully applied to various downstream natural language processing tasks.\n",
    "\n",
    "Since its publication, the Word2Vec algorithm has had a significant impact on the field of natural language processing. It has become a foundational method and has been widely adopted for a range of applications, such as language modeling, information retrieval, sentiment analysis, and machine translation. The efficiency and effectiveness of Word2Vec have paved the way for further advancements in word representation learning and have contributed to the development of more sophisticated language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper review of \"Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation\"(1406.1078):\n",
    "The paper \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\" by Kyunghyun Cho et al., published in 2014, introduces a novel approach for statistical machine translation using a recurrent neural network (RNN) encoder-decoder framework.\n",
    "\n",
    "The authors propose a model that consists of two main components: an encoder and a decoder. The encoder is an RNN that reads the input sentence in the source language and generates a fixed-length vector representation known as the \"thought vector\" or \"context vector.\" This vector aims to capture the semantic and syntactic information of the source sentence. The decoder, also an RNN, takes the context vector as input and generates the translated sentence in the target language.\n",
    "\n",
    "During the training process, the model is trained on a parallel corpus, where each sentence in the source language is paired with its translation in the target language. The model is trained to maximize the conditional probability of generating the correct translation given the source sentence. This is achieved by minimizing the negative log-likelihood of the target sentence under the model's predicted distribution.\n",
    "\n",
    "A key contribution of the paper is the introduction of the \"soft\" attention mechanism. This mechanism allows the decoder to dynamically focus on different parts of the source sentence while generating each word of the translation. By attending to relevant words in the source sentence, the model can effectively align words in the source and target languages, leading to improved translation quality.\n",
    "\n",
    "The authors evaluate their model on various language pairs, including English-French and English-German, and compare it to traditional phrase-based and hierarchical models. The results demonstrate that their RNN encoder-decoder model with attention achieves significant improvements in translation quality, outperforming the traditional approaches.\n",
    "\n",
    "The paper's contributions have had a substantial impact on the field of machine translation. The RNN encoder-decoder framework with attention has become a fundamental building block for many subsequent neural machine translation models. It has paved the way for the development of more advanced architectures, such as transformer-based models, which have further pushed the boundaries of translation quality and performance. The paper has sparked further research in the area of neural machine translation and has significantly influenced the field's progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper review of \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\"(1607.06520):\n",
    "\n",
    "The paper \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\" by Tolga Bolukbasi et al., published in 2016, addresses the issue of bias in word embeddings and proposes techniques to mitigate and debias these biases.\n",
    "\n",
    "The authors highlight that word embeddings, such as those generated by the popular Word2Vec algorithm, can inadvertently capture and amplify societal biases present in the training data. They demonstrate this by showing that the learned word embeddings exhibit biased associations, such as \"man is to computer programmer as woman is to homemaker.\"\n",
    "\n",
    "To tackle this problem, the authors propose two debiasing methods. The first method, termed \"hard debiasing,\" aims to neutralize the biased gender direction in the embeddings. It involves identifying a gender subspace by calculating the principal components associated with gender-related words and then projecting the embeddings onto a subspace orthogonal to this gender direction. This process ensures that the gender-neutral words are not influenced by the gender bias present in the embeddings.\n",
    "\n",
    "The second method, called \"soft debiasing,\" seeks to reduce bias while preserving certain gender-related information. It involves modifying the original word embeddings by explicitly specifying gender-specific concepts and adjusting the word vectors accordingly. This approach allows for the preservation of useful gender-related information while mitigating the biased associations.\n",
    "\n",
    "To evaluate the effectiveness of their debiasing methods, the authors conduct several experiments. They measure the bias in gender analogies, such as \"man is to doctor as woman is to nurse,\" and observe significant reductions in bias after applying the debiasing techniques. Furthermore, they perform bias tests on occupation and name lists, demonstrating that the debiasing methods lead to fairer representations and less biased associations.\n",
    "\n",
    "The contributions of the paper extend beyond the specific debiasing methods proposed. It highlights the presence of bias in word embeddings and emphasizes the importance of addressing this issue. The paper stimulates further research and discussions on bias in machine learning models and algorithms, encouraging researchers and practitioners to consider the ethical implications and strive for more fair and unbiased natural language processing applications.\n",
    "\n",
    "Overall, the paper provides practical techniques for mitigating bias in word embeddings, fostering a greater understanding of the challenges associated with bias in language representations and contributing to the development of more equitable and inclusive natural language processing systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper review of \"Distributed Representations of Words and Phrases and their Compositionality\":\n",
    "\"Distributed Representations of Words and Phrases and their Compositionality\" is a research paper by Tomas Mikolov et al., published in 2013. The paper introduces the Continuous Bag-of-Words (CBOW) and Skip-gram models, which are widely used for learning distributed representations of words and phrases.\n",
    "\n",
    "The authors highlight the limitations of traditional language models that represent words as discrete symbols, which often fail to capture the semantic relationships and compositionality of words. In response to this, they propose the use of continuous distributed representations, also known as word embeddings or word vectors, which represent words as dense vectors in a high-dimensional space.\n",
    "\n",
    "The Continuous Bag-of-Words (CBOW) model is introduced as a method for learning word embeddings. It predicts the target word based on its surrounding context words. The model takes a fixed-size window of context words as input and predicts the target word in the center of the window. The objective is to maximize the probability of the target word given its context.\n",
    "\n",
    "The Skip-gram model, on the other hand, takes a target word as input and aims to predict the context words within a certain window around the target word. This model captures the inverse relationship between CBOW and Skip-gram, and both models are trained using a neural network.\n",
    "\n",
    "The paper also presents a technique called negative sampling, which addresses the computational inefficiency of traditional neural network training approaches. Negative sampling involves training the model to distinguish between the correct word and randomly sampled incorrect words, making the training process more efficient.\n",
    "\n",
    "The authors evaluate the performance of the proposed models on various tasks, including word similarity, word analogy, and phrase similarity. The results demonstrate that the learned word and phrase embeddings capture meaningful relationships and compositionality. These embeddings can be used to improve performance in various natural language processing tasks, such as language modeling, named entity recognition, and part-of-speech tagging.\n",
    "\n",
    "The paper's contributions have had a significant impact on the field of natural language processing. The Continuous Bag-of-Words (CBOW) and Skip-gram models, along with the concept of word embeddings, have become widely adopted and influential. They have paved the way for the development of more advanced techniques, such as transformer-based models, and have greatly improved the representation and understanding of words and phrases in natural language processing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper review of \"GloVe: Global Vectors for Word Representation\":\n",
    "\"GloVe: Global Vectors for Word Representation\" is a research paper by Jeffrey Pennington, Richard Socher, and Christopher D. Manning, published in 2014. The paper introduces the GloVe (Global Vectors) model, which is a method for learning word representations or word embeddings.\n",
    "\n",
    "The authors highlight the importance of word embeddings in natural language processing tasks and the limitations of existing methods. They propose the GloVe model as an approach that combines the advantages of both global matrix factorization methods and local context window methods.\n",
    "\n",
    "The GloVe model aims to capture the semantic and syntactic relationships between words by leveraging the global statistical information of word co-occurrence patterns. It starts by constructing a word co-occurrence matrix based on a large corpus. Each entry in the matrix represents the number of times two words co-occur within a defined context window. The matrix is then factorized to obtain word vectors that encode the relative meanings of words.\n",
    "\n",
    "The key insight of the GloVe model is the use of a weighted least squares objective, which balances the importance of frequently co-occurring words and rare word pairs. By incorporating both global and local context information, the GloVe model is able to capture fine-grained semantic relationships while avoiding the computational inefficiency of traditional local context window methods.\n",
    "\n",
    "The authors evaluate the performance of the GloVe model on various word analogy and word similarity tasks, showing that it outperforms existing methods and achieves state-of-the-art results. They also demonstrate that the learned word embeddings exhibit meaningful geometric properties, such as linear substructures and syntactic regularities.\n",
    "\n",
    "The contributions of the paper have had a significant impact on the field of natural language processing. The GloVe model has become a widely adopted method for learning word representations, with applications in various tasks, including machine translation, sentiment analysis, and named entity recognition. It has contributed to advancing the understanding of word semantics and has become an essential tool for natural language understanding and generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper review of \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\":\n",
    "The paper \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\" by Omar Levy , Yoav Goldberg, and Ido Dagan was published in 2015 in the Transactions of the Association for Computational Linguistics (TACL) journal.\n",
    "\n",
    "The authors examine the commonly used distributional models for word representations and show that they underperform compared to neural network-inspired word embeddings. They, then, investigate the factors that contribute to the success of word embeddings and find that it is due to certain system design choices and hyperparameter optimizations.\n",
    "\n",
    "The authors propose a new distributional similarity model that combines aspects of traditional distributional models with those of neural-network-inspired word embeddings , resulting in superior performance to both models on various benchmark tasks. They evaluate their model through a series of experiments and show that it significantly outperforms both the traditional models and the word embeddings on various tasks.\n",
    "\n",
    "Overall, the paper provides useful insights into the development and optimization of word embeddings and has significant implications for the field of computational linguistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GloVe\n",
    "\n",
    "We will first read imdb movie reviews to train a GloVe embeddings.\n",
    "\n",
    "GloVe is computed from a co-occurrence matrix $X$ as follows:\n",
    "\n",
    "$\n",
    "J = \\sum_{i=1,j=1}^{V,V} f(X_{ij}) (w_i^T w_j + b_i + b_j - log(X_{ij}))^2\n",
    "$\n",
    "\n",
    "$f(X_{ij}) = (X_{ij} / X_{\\max})^\\alpha$ if $X_{ij} < X_{\\max}$; otherwise it is $1$.\n",
    "\n",
    "$\n",
    "\\nabla_{w_i} J = f(X_{ij}) w_j (w_i^T w_j + b_i + b_j - log(X_{ij}))\n",
    "$\n",
    "\n",
    "$\n",
    "\\nabla_{w_j} J = f(X_{ij}) w_i (w_i^T w_j + b_i + b_j - log(X_{ij}))\n",
    "$\n",
    "\n",
    "$\n",
    "\\nabla_{b_i} J = \\nabla_{b_j} J = f(X_{ij}) (w_i^T w_j + b_i + b_j - log(X_{ij}))\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/LeroyJoseph/opt/anaconda3/bin/pip\", line 6, in <module>\n",
      "    from pip._internal.cli.main import main\n",
      "  File \"/Users/LeroyJoseph/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/cli/main.py\", line 10, in <module>\n",
      "    from pip._internal.cli.autocompletion import autocomplete\n",
      "  File \"/Users/LeroyJoseph/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/cli/autocompletion.py\", line 9, in <module>\n",
      "    from pip._internal.cli.main_parser import create_main_parser\n",
      "  File \"/Users/LeroyJoseph/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/cli/main_parser.py\", line 7, in <module>\n",
      "    from pip._internal.cli import cmdoptions\n",
      "  File \"/Users/LeroyJoseph/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/cli/cmdoptions.py\", line 25, in <module>\n",
      "    from pip._internal.locations import USER_CACHE_DIR, get_src_prefix\n",
      "  File \"/Users/LeroyJoseph/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/locations/__init__.py\", line 9, in <module>\n",
      "    from pip._internal.models.scheme import SCHEME_KEYS, Scheme\n",
      "ImportError: cannot import name 'SCHEME_KEYS' from 'pip._internal.models.scheme' (/Users/LeroyJoseph/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/models/scheme.py)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/LeroyJoseph/opt/anaconda3/bin/pip\", line 6, in <module>\n",
      "    from pip._internal.cli.main import main\n",
      "  File \"/Users/LeroyJoseph/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/cli/main.py\", line 10, in <module>\n",
      "    from pip._internal.cli.autocompletion import autocomplete\n",
      "  File \"/Users/LeroyJoseph/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/cli/autocompletion.py\", line 9, in <module>\n",
      "    from pip._internal.cli.main_parser import create_main_parser\n",
      "  File \"/Users/LeroyJoseph/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/cli/main_parser.py\", line 7, in <module>\n",
      "    from pip._internal.cli import cmdoptions\n",
      "  File \"/Users/LeroyJoseph/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/cli/cmdoptions.py\", line 25, in <module>\n",
      "    from pip._internal.locations import USER_CACHE_DIR, get_src_prefix\n",
      "  File \"/Users/LeroyJoseph/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/locations/__init__.py\", line 9, in <module>\n",
      "    from pip._internal.models.scheme import SCHEME_KEYS, Scheme\n",
      "ImportError: cannot import name 'SCHEME_KEYS' from 'pip._internal.models.scheme' (/Users/LeroyJoseph/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/models/scheme.py)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "from keras import preprocessing\n",
    "from keras.datasets import imdb\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 500\n",
    "maxlen = 200\n",
    "emb_size = 16\n",
    "\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, _), _ = imdb.load_data(\n",
    "    num_words=num_words, maxlen=maxlen, start_char=start_char, oov_char=oov_char, index_from=index_from\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "inverted_word_index = dict(\n",
    "    (i + index_from, word) for (word, i) in word_index.items()\n",
    ")\n",
    "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "inverted_word_index[start_char] = \"[START]\"\n",
    "inverted_word_index[oov_char] = \"[OOV]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[START] big [OOV] big [OOV] bad music and a [OOV] [OOV] [OOV] these are the [OOV] to best [OOV] this terrible movie i love [OOV] horror movies and i've seen [OOV] but this had got to be on of the worst ever made the plot is [OOV] [OOV] and [OOV] the acting is an [OOV] the script is completely [OOV] the best is the end [OOV] with the [OOV] and how he [OOV] out who the killer is it's just so [OOV] [OOV] written the [OOV] are [OOV] and funny in [OOV] [OOV] the [OOV] is big [OOV] of [OOV] [OOV] men [OOV] those [OOV] [OOV] [OOV] that show off their [OOV] [OOV] that men actually [OOV] them and the music is just [OOV] [OOV] that plays over and over again in almost every scene there is [OOV] music [OOV] and [OOV] [OOV] away [OOV] and the [OOV] still doesn't close for [OOV] all [OOV] [OOV] this is a truly bad film [OOV] only [OOV] is to look back on the [OOV] that was the [OOV] and have a good old laugh at how bad everything was back then\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the first sequence in the dataset\n",
    "decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])\n",
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"big big bad music and a these are the to best this terrible movie i love horror movies and i've seen but this had got to be on of the worst ever made the plot is and the acting is an the script is completely the best is the end with the and how he out who the killer is it's just so written the are and funny in the is big of men those that show off their that men actually them and the music is just that plays over and over again in almost every scene there is music and away and the still doesn't close for all this is a truly bad film only is to look back on the that was the and have a good old laugh at how bad everything was back then\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_remove = [word_index['br']+index_from, oov_char, start_char]\n",
    "\n",
    "x_train = [[w for w in x_train[i] if w not in words_to_remove] for i in range(len(x_train))]\n",
    "decoded_sequence = \" \".join([inverted_word_index[i] for i in x_train[0]])\n",
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words we want to analyze\n",
    "\n",
    "w1 = 'good'\n",
    "w2 = 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 49, 75)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = num_words\n",
    "assert max(word_index[w1], word_index[w2]) < V\n",
    "V, word_index[w1], word_index[w2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get the dictionary of words to indexes and indexes to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating 500 words\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in word_index if word_index[w] < V]\n",
    "print(f'generating {V} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the co-occurrence matrix. Our implementation will not be the most efficient one, but it will serve the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity is 0.085212\n"
     ]
    }
   ],
   "source": [
    "# Initialize co-occurrence matrix\n",
    "X = np.zeros((V, V))\n",
    "for s in x_train:\n",
    "    for i in range(1, len(s)):\n",
    "        j_indexes = i - np.arange(1, window+1)\n",
    "        j_indexes = j_indexes[j_indexes >= 0]\n",
    "        for j in j_indexes:\n",
    "            inc = 1.0 / (i - j)\n",
    "            X[s[i], s[j]] += inc\n",
    "            X[s[j], s[i]] += inc\n",
    "print(f'sparsity is {np.mean(X.flatten() == 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29396.116666667273, 228697, 250000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xp = X.flatten()\n",
    "Xp = Xp[Xp > 0]\n",
    "plt.hist(np.log(Xp))\n",
    "np.max(Xp), len(Xp), V*V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228697,\n",
       " [(4, 4),\n",
       "  (4, 5),\n",
       "  (4, 6),\n",
       "  (4, 7),\n",
       "  (4, 8),\n",
       "  (4, 9),\n",
       "  (4, 11),\n",
       "  (4, 12),\n",
       "  (4, 13),\n",
       "  (4, 14)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xmax = 1000\n",
    "eps=1e-3\n",
    "lr = 0.1\n",
    "beta = 0.99\n",
    "epochs = 200\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "w = 2 * (np.random.rand(2*V, emb_size) - 0.5) / (emb_size + 1)\n",
    "b = 2 * (np.random.rand(2*V) - 0.5) / (emb_size + 1)\n",
    "g_w_s = np.ones((2 * V, emb_size), dtype=np.float32)\n",
    "g_b_s = np.ones(2 * V, dtype=np.float32)\n",
    " \n",
    "indexes = []\n",
    "all_idx = np.arange(V)\n",
    "for i in range(V):\n",
    "    mask = X[i] != 0\n",
    "    if np.sum(mask) == 0: continue\n",
    "    for j in all_idx[mask]:\n",
    "        indexes.append((i, j))\n",
    "len(indexes), indexes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29957.06 0.9792\n",
      "1 6287.94 0.7221\n",
      "2 4805.34 0.6776\n",
      "3 3927.7 0.6516\n",
      "4 3605.85 0.5763\n",
      "5 3383.98 0.651\n",
      "6 3306.56 0.6419\n",
      "7 3183.43 0.5875\n",
      "8 3099.82 0.6161\n",
      "9 3036.5 0.5268\n"
     ]
    }
   ],
   "source": [
    "def J():\n",
    "    result = 0\n",
    "    for i in range(V):\n",
    "        for j in range(V):\n",
    "            result += f(X[i][j]) * np.power(\n",
    "                np.dot(w[i], w[j+V]) + b[i] + b[j+V] - np.log1p(X[i][j]), 2)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def f(x, alpha=0.75):\n",
    "    if x < Xmax:\n",
    "        return np.power(x / Xmax, alpha)\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "def W(word):\n",
    "    ww = w[word_index[word]]\n",
    "    return ww / np.linalg.norm(ww)\n",
    "\n",
    "all_js = []\n",
    "for e in range(epochs):\n",
    "    cost = 0\n",
    "    shuffle(indexes)\n",
    "    for i, jj in indexes:\n",
    "        j = jj + V\n",
    "        weight = f(X[i][jj])\n",
    "        inner = (np.dot(w[i], w[j]) + b[i] + b[j] - np.log(X[i][jj]))\n",
    "        dwi = w[j] * weight * inner\n",
    "        dwj = w[i] * weight * inner\n",
    "        dbi = dbj = weight * inner\n",
    "        cost += weight * inner ** 2\n",
    "        w[i] -= np.clip(lr * dwi / np.sqrt(g_w_s[i] + eps), -1, 1)\n",
    "        w[j] -= np.clip(lr * dwj / np.sqrt(g_w_s[j] + eps), -1, 1)\n",
    "        b[i] -= np.clip(lr * dbi / np.sqrt(g_b_s[i] + eps), -1, 1)\n",
    "        b[j] -= np.clip(lr * dbj / np.sqrt(g_b_s[j] + eps), -1, 1)\n",
    "\n",
    "        g_w_s[i] = beta * g_w_s[i] + np.square(dwi)\n",
    "        g_w_s[j] = beta * g_w_s[j] + np.square(dwj)\n",
    "        g_b_s[i] = beta * g_b_s[i] + np.square(dbi)\n",
    "        g_b_s[j] = beta * g_b_s[j] + np.square(dbj)\n",
    "            \n",
    "    all_js.append(cost)\n",
    "    \n",
    "    print(e, np.round(cost, 2), np.round(W('good').dot(W('bad')), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now explain with your own words how Glove works.  Find which word is a synonym for `positive` and an antonym for `positive`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe (Global Vectors for Word Representation) algorithm is a method for learning word embeddings, which are dense vector representations of words in a high-dimensional space. These word embeddings capture semantic and syntactic relationships between words.\n",
    "\n",
    "The GloVe algorithm aims to capture the co-occurrence statistics of words within a large corpus of text. It leverages the intuition that words that frequently co-occur in similar contexts have related meanings. The main idea behind GloVe is to factorize a co-occurrence matrix, which contains information about how often words co-occur with each other in the corpus.\n",
    "\n",
    "To train the word embeddings using GloVe, the algorithm iterates over the co-occurrence matrix and updates the word vectors based on the observed co-occurrence patterns. The training objective is to find word vectors that, when combined, can accurately reconstruct the observed co-occurrence statistics.\n",
    "\n",
    "During training, GloVe takes into account both global and local information. Global information refers to the overall co-occurrence patterns of words in the entire corpus, while local information refers to the specific co-occurrence patterns of words in a local context window.\n",
    "\n",
    "By learning word embeddings with GloVe, we can represent words in a continuous vector space, where words with similar meanings are located closer to each other. These embeddings can then be used as input features for various natural language processing tasks, such as sentiment analysis, document classification, and machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "word_embeddings = np.load('path/to/glove_embeddings.npy')\n",
    "word_index = imdb.get_word_index()\n",
    "inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n",
    "\n",
    "# Target word\n",
    "target_word = 'positive'\n",
    "\n",
    "# Find synonyms\n",
    "target_embedding = word_embeddings[word_index[target_word]]\n",
    "similarities = cosine_similarity([target_embedding], word_embeddings)\n",
    "most_similar_indices = np.argsort(-similarities)[0][1:6]  # Top 5 similar words\n",
    "\n",
    "synonyms = [inverted_word_index[idx] for idx in most_similar_indices]\n",
    "\n",
    "# Find antonyms\n",
    "opposite_embedding = -target_embedding  # Negate the target embedding\n",
    "similarities = cosine_similarity([opposite_embedding], word_embeddings)\n",
    "most_similar_indices = np.argsort(-similarities)[0][:5]  # Top 5 dissimilar words\n",
    "\n",
    "antonyms = [inverted_word_index[idx] for idx in most_similar_indices]\n",
    "\n",
    "print(f\"Synonyms for '{target_word}':\")\n",
    "for synonym in synonyms:\n",
    "    print(synonym)\n",
    "\n",
    "print(f\"\\nAntonyms for '{target_word}':\")\n",
    "for antonym in antonyms:\n",
    "    print(antonym)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convolution Based NLP\n",
    "\n",
    "In an attempt to create our first generator network before we start using transformers, you will build a large language model using a Convolutional (causal) and Embeddings.\n",
    "\n",
    "We will split this task into two task.\n",
    "\n",
    "- First task is to try to predict the next word.\n",
    "- Second task is to implement and train a network that will use the head to predict the sentiment of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 23:52:11.169421: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import keras\n",
    "from keras import preprocessing\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 76, 83, 83, 86, 7, 94, 86, 89, 83, 75]\n",
      "['[START]', '[OOV]', '[OOV]', 'h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.chars = ['\\00', '\\01'] + sorted(list(set(string.printable)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        \n",
    "        self.itos[0] = '[START]'\n",
    "        self.itos[1] = '[OOV]'\n",
    "                            \n",
    "    def encode(self, sentence):\n",
    "        return [self.stoi[c] if c in self.stoi else 1 for c in sentence]\n",
    "    \n",
    "    def decode(self, indexes):\n",
    "        return [self.itos[i] for i in indexes]\n",
    "    \n",
    "    def start(self): return '\\00'\n",
    "\n",
    "    def oov(self): return '\\01'\n",
    "    \n",
    "tokenizer = Tokenizer()\n",
    "print(tokenizer.encode('hello world'))\n",
    "print(tokenizer.decode([0] + [1] + tokenizer.encode('\\x96hello world')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 1024\n",
    "emb_size = 50\n",
    "\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    start_char=start_char, oov_char=oov_char, index_from=index_from\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "inverted_word_index = dict(\n",
    "    (i + index_from, word) for (word, i) in word_index.items()\n",
    ")\n",
    "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "inverted_word_index[start_char] = tokenizer.start()\n",
    "inverted_word_index[oov_char] = tokenizer.oov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def retokenize(text):\n",
    "#    return [tokenizer.encode('  '.join([inverted_word_index[idx] for idx in text[i]])) \n",
    " #           for i in range(len(text))]\n",
    "\n",
    "#x_train = retokenize(x_train)\n",
    "#x_test = retokenize(x_test)\n",
    "#np.array(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_c = np.concatenate(x_train)\n",
    "x_test_c = np.concatenate(x_test)\n",
    "x_train_c.shape, x_test_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, embedding_size, window, dilation):\n",
    "        super().__init__()\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            embedding_size, embedding_size, \n",
    "            kernel_size=window, groups=embedding_size,\n",
    "            padding=(window-1)*dilation, dilation=dilation)\n",
    "        self.ln = nn.LayerNorm(embedding_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pytorch requires C to be in dimension 1\n",
    "        x = x.permute(0, 2, 1) # (B, C, T)\n",
    "        # we do convolution from C -> C, but with only one group\n",
    "        # that means that we are doing a depthwise convolution\n",
    "        # so that we use the same filter for each embedding\n",
    "        x = self.conv1d(x)\n",
    "        # after the convolution, we need to restore the dimension\n",
    "        # and remove the extra right padding\n",
    "        x = x[:, :, :-self.conv1d.padding[0]]\n",
    "        x = x.permute(0, 2, 1) # (B, T, C)\n",
    "        return self.ln(x)\n",
    "    \n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, max_len, window, dropout):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.wte = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.wpe = nn.Embedding(max_len, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv1d_0 = CausalConv1d(embedding_size, window, dilation=1)\n",
    "        self.conv1d_1 = CausalConv1d(embedding_size, window, dilation=1)\n",
    "        self.lin_0 = nn.Linear(embedding_size, embedding_size)\n",
    "        self.lin_1 = nn.Linear(embedding_size, embedding_size)\n",
    "        self.lin_2 = nn.Linear(embedding_size, embedding_size)\n",
    "        self.ln_0 = nn.LayerNorm(embedding_size)\n",
    "        self.time_shift = nn.ZeroPad2d((0,0,1,0)) # TRICK: time-mix\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        assert T <= self.max_len\n",
    "\n",
    "        tok_emb = self.wte(idx) # (B, T, C)\n",
    "        \n",
    "        B, T, C = tok_emb.shape\n",
    "        \n",
    "        pos = torch.arange(\n",
    "            0, T, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        pos_emb = self.wpe(pos)\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "        x = self.ln_0(x)\n",
    "        x = torch.cat([self.time_shift(x)[:,:T,:C//2], x[:,:T,C//2:]], dim=2) # TRICK: time-mix\n",
    "        # idea is to do Linear C dimension\n",
    "        # followed by a causal filter on the T dimension\n",
    "        x = nn.GELU(approximate='tanh')(self.lin_0(x))\n",
    "        x = nn.GELU(approximate='tanh')(self.conv1d_0(x))\n",
    "\n",
    "        x = nn.GELU(approximate='tanh')(self.lin_1(x))\n",
    "        x = nn.GELU(approximate='tanh')(self.conv1d_1(x))\n",
    "\n",
    "        x = nn.GELU(approximate='tanh')(self.lin_2(x))\n",
    "        return x\n",
    "\n",
    "class LLM(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(embedding_size)\n",
    "        self.linv = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linv(self.ln(x))\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = Head(emb_size, vocab_size, maxlen, maxlen, 0.15)\n",
    "head = head.to(device)\n",
    "model = LLM(emb_size, vocab_size)\n",
    "model = model.to(device)\n",
    "for name, params in head.named_parameters():\n",
    "    print(name, params.shape)\n",
    "print()\n",
    "for name, params in model.named_parameters():\n",
    "    print(name, params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "loss_f = F.cross_entropy\n",
    "\n",
    "def get_batch(data):\n",
    "    ix = torch.randint(len(data) - maxlen, (batch_size,))\n",
    "    x = torch.stack([\n",
    "        torch.from_numpy(data[i:i+maxlen].astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([\n",
    "        torch.from_numpy(data[i+1:i+maxlen+1].astype(np.int64)) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(x_train_c, x_test_c, eval_iters=100):\n",
    "    def _internal(model):\n",
    "        dataset = {'train': x_train_c, 'val': x_test_c}\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(dataset[split])\n",
    "                p = model(head(X))\n",
    "                B, T, C = p.shape\n",
    "                loss = loss_f(p.view(B*T, C), Y.view(B*T))\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "        model.train()\n",
    "        return out\n",
    "    return _internal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = get_batch(x_train_c)\n",
    "x1[0], y1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_lr_func(warmup_iters, learning_rate, lr_decay_iters,  min_lr):\n",
    "    def __get_lr__(it):\n",
    "        nonlocal decay_ratio\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < warmup_iters:\n",
    "            return learning_rate * it / warmup_iters\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if it > lr_decay_iters:\n",
    "            return min_lr\n",
    "        # 3) in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "        return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "    decay_ratio = 0\n",
    "    return __get_lr__\n",
    "\n",
    "learning_rate = 0.005\n",
    "get_lr = get_lr_func(1000, learning_rate, 100000, learning_rate/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 0\n",
    "logs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50000\n",
    "\n",
    "params = list(head.parameters()) + list(model.parameters())\n",
    "estimate_loss_f = estimate_loss(x_train_c, x_test_c)\n",
    "optimizer = torch.optim.AdamW(params, lr=learning_rate)\n",
    "\n",
    "for iter_num in range(iter_num, iter_num + epochs):\n",
    "    learning_rate = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = learning_rate\n",
    "    if iter_num % 1000 == 0:\n",
    "        losses = estimate_loss_f(model)\n",
    "        print(\n",
    "            f'step {iter_num}: train loss {losses[\"train\"]:.4f}, '\n",
    "            f'val loss {losses[\"val\"]:.4f}')\n",
    "        logs.append((iter_num, losses['train'], losses['val']))\n",
    "    \n",
    "    xb, yb = get_batch(x_train_c)\n",
    "    pb = model(head(xb))\n",
    "    B, T, C = pb.shape\n",
    "    loss = loss_f(pb.view(B*T, C), yb.view(B*T))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([v[1] for v in logs][3:], label='train')\n",
    "plt.plot([v[2] for v in logs][3:], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_c[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(tokenizer.decode(x_test_c[0:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -maxlen:]\n",
    "        logits = model(head(idx_cond))\n",
    "        logits = logits[:, 0, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        if idx_next == 0: break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array([x_test_c[0:20]]).astype(np.int64)\n",
    "result = generate(torch.from_numpy(idx).to(device), max_new_tokens=100).detach().cpu().numpy()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sequence = ''.join(tokenizer.decode(result[0]))\n",
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
