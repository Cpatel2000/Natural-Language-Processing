{"cells":[{"cell_type":"markdown","metadata":{"id":"t-W-XkwVZDoz"},"source":["## Setup\n","For this lab you need to install __gensim__ and __nltk__."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3564,"status":"ok","timestamp":1653001178175,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"9HgUQT-3ZLsu","outputId":"546168ef-c5fe-4a3d-bf56-e161dd630289"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (6.0.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n"]}],"source":["!pip install gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2381,"status":"ok","timestamp":1653001180531,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"37XYa1oFZL9T","outputId":"919e98a3-660e-49b6-e00a-992ad8018edf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0bEeJ1fZDo-"},"outputs":[],"source":["import gensim.downloader as api\n","\n","from tensorflow.keras.datasets import imdb\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from random import shuffle\n","import pandas as pd\n","from sklearn.manifold import TSNE\n","from tensorflow.keras.layers import *\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"mGUXhz8cZDpB"},"source":["# 1. Introduction\n","\n","In this homework, you will see the power of word embeddings, and how embeddings can be used to improve results of movie sentiment analysis from previous homeworks.\n","\n","The data set can be found in `https://keras.io/api/datasets/imdb/`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3727,"status":"ok","timestamp":1653001187433,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"xMMJisYfZDpC","outputId":"45a325b4-96f6-4d10-d558-09a057695207"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n","17473536/17464789 [==============================] - 0s 0us/step\n"]},{"output_type":"execute_result","data":{"text/plain":["(14244,)"]},"metadata":{},"execution_count":4}],"source":["NUM_WORDS=1000 # only use top 1000 words\n","INDEX_FROM=3   # word index offset\n","MAX_LEN=200    # maximum sentence length\n","\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(\n","    num_words=NUM_WORDS, index_from=INDEX_FROM, maxlen=MAX_LEN)\n","x_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPARYQwGZDpD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653001188028,"user_tz":420,"elapsed":604,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"}},"outputId":"dbbc8101-89e7-4cce-99fe-f7fc7babfd4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1646592/1641221 [==============================] - 0s 0us/step\n","1654784/1641221 [==============================] - 0s 0us/step\n"]}],"source":["word_index = imdb.get_word_index()\n","word_index = {k:(v+INDEX_FROM) for k,v in word_index.items()}\n","word_index[\"<PAD>\"] = 0\n","word_index[\"<START>\"] = 1\n","word_index[\"<UNK>\"] = 2\n","inverted_word_index = dict((i, word) for (word, i) in word_index.items())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1653001188031,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"b96jsL6aZDpE","outputId":"65c47785-c922-4a4f-f451-5409801820ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  1, 194,   2, 194,   2,  78, 228,   5,   6,   2,   2,   2, 134,\n","        26,   4, 715,   8, 118,   2,  14, 394,  20,  13, 119, 954, 189,\n","       102,   5, 207, 110,   2,  21,  14,  69, 188,   8,  30,  23,   7,\n","         4, 249, 126,  93,   4, 114,   9,   2,   2,   5, 647,   4, 116,\n","         9,  35,   2,   4, 229,   9, 340,   2,   4, 118,   9,   4, 130,\n","         2,  19,   4,   2,   5,  89,  29, 952,  46,  37,   4, 455,   9,\n","        45,  43,  38,   2,   2, 398,   4,   2,  26,   2,   5, 163,  11,\n","         2,   2,   4,   2,   9, 194, 775,   7,   2,   2, 349,   2, 148,\n","       605,   2,   2,  15, 123, 125,  68,   2,   2,  15, 349, 165,   2,\n","        98,   5,   4, 228,   9,  43,   2,   2,  15, 299, 120,   5, 120,\n","       174,  11, 220, 175, 136,  50,   9,   2, 228,   2,   5,   2, 656,\n","       245,   2,   5,   4,   2, 131, 152, 491,  18,   2,  32,   2,   2,\n","        14,   9,   6, 371,  78,  22, 625,  64,   2,   9,   8, 168, 145,\n","        23,   4,   2,  15,  16,   4,   2,   5,  28,   6,  52, 154, 462,\n","        33,  89,  78, 285,  16, 145,  95])"]},"metadata":{},"execution_count":6}],"source":["np.array(x_train[0])"]},{"cell_type":"markdown","metadata":{"id":"GydS6CJdZDpF"},"source":["# 2. GloVe\n","\n","We will first read imdb movie reviews to train a GloVe embeddings.\n","\n","GloVe is computed from a co-occurrence matrix $X$ as follows:\n","\n","$$\n","J = \\sum_{i=1,j=1}^{V,V} f(X_{ij}) (w_i^T w_j + b_i + b_j - log(X_{ij}))^2\n","$$\n","\n","$f(X_{ij}) = (X_{ij} / X_{\\max})^\\alpha$ if $X_{ij} < X_{\\max}$; otherwise it is $1$.\n","\n","$$\n","\\nabla_{w_i} J = f(X_{ij}) w_j (w_i^T w_j + b_i + b_j - log(X_{ij}))\n","$$\n","\n","$$\n","\\nabla_{w_j} J = f(X_{ij}) w_i (w_i^T w_j + b_i + b_j - log(X_{ij}))\n","$$\n","\n","$$\n","\\nabla_{b_i} J = \\nabla_{b_j} J = f(X_{ij}) (w_i^T w_j + b_i + b_j - log(X_{ij}))\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"272ibtWuZDpH"},"outputs":[],"source":["word_counts = NUM_WORDS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1653001188034,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"bTLNlArwZDpI","outputId":"71c629fa-1550-4499-e40f-5197222c4db7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{},"execution_count":8}],"source":["V = NUM_WORDS\n","V"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1653001188035,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"d2qbY8ngZDpJ","outputId":"65a11b51-de6c-4a26-8c3f-28ec104a6e6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["14244\n","\n","[189, 141, 147, 43, 123, 130, 99, 117, 109, 129, 163, 177, 129, 140, 93, 142, 193, 171, 174, 162, 51, 139, 142, 132, 122, 55, 103, 186, 113, 169, 138, 146, 59, 107, 152, 186, 147, 118, 132, 167, 115, 95, 158, 156, 82, 190, 174, 60, 145, 170, 107, 171, 158, 145, 67, 123, 195, 91, 183, 50, 118, 147, 141, 60, 56, 144, 129, 153, 55, 92, 174, 187, 183, 165, 78, 198, 156, 127, 61, 84, 57, 176, 159, 57, 159, 165, 194, 149, 130, 19, 98, 130, 153, 143, 136, 125, 142, 148, 198, 193]\n"]}],"source":["print(len(x_train))\n","print()\n","print([len(s) for s in x_train[0:100]])\n","\n","S = 2000\n","\n","if S < len(x_train):\n","    x_train = x_train[:S]"]},{"cell_type":"markdown","metadata":{"id":"kJ0VxG4cZDpK"},"source":["Let's now get the dictionary of words to indexes and indexes to words."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1653001188036,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"tZP7jIYNZDpL","outputId":"8c2a3b38-af54-4a0c-fa78-11a68d54e574","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 194, 2, 194, 2, 78, 228, 5, 6, 2, 2, 2, 134, 26, 4, 715, 8, 118, 2, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2, 2, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 2, 4, 118, 9, 4, 130, 2, 19, 4, 2, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 2, 2, 398, 4, 2, 26, 2, 5, 163, 11, 2, 2, 4, 2, 9, 194, 775, 7, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 2, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 656, 245, 2, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 9, 6, 371, 78, 22, 625, 64, 2, 9, 8, 168, 145, 23, 4, 2, 15, 16, 4, 2, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"]},{"output_type":"execute_result","data":{"text/plain":["\"<START> big <UNK> big <UNK> bad music and a <UNK> <UNK> <UNK> these are the words to best <UNK> this terrible movie i love cheesy horror movies and i've seen <UNK> but this had got to be on of the worst ever made the plot is <UNK> <UNK> and ridiculous the acting is an <UNK> the script is completely <UNK> the best is the end <UNK> with the <UNK> and how he worked out who the killer is it's just so <UNK> <UNK> written the <UNK> are <UNK> and funny in <UNK> <UNK> the <UNK> is big lots of <UNK> <UNK> men <UNK> those cut <UNK> <UNK> that show off their <UNK> <UNK> that men actually <UNK> them and the music is just <UNK> <UNK> that plays over and over again in almost every scene there is <UNK> music <UNK> and <UNK> taking away <UNK> and the <UNK> still doesn't close for <UNK> all <UNK> <UNK> this is a truly bad film whose only <UNK> is to look back on the <UNK> that was the <UNK> and have a good old laugh at how bad everything was back then\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}],"source":["# Decode the first sequence in the dataset\n","decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])\n","print(x_train[0])\n","decoded_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1653001188038,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"nzSlmOkOZDpM","outputId":"067f6432-2d7b-4656-f332-2fe66020b6f6","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["generating 1000 words\n"]}],"source":["words = [w for w in word_index if word_index[w] < V]\n","print(f'generating {V} words')"]},{"cell_type":"markdown","metadata":{"id":"sdefEGphZDpM"},"source":["Let's build the co-occurrence matrix. Our implementation will not be the most efficient one, but it will serve the purpose."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INQFkr-dZDpN"},"outputs":[],"source":["window = 5\n","emb_size = 15"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10275,"status":"ok","timestamp":1653001198297,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"tCjhSlSEZDpO","outputId":"cac94770-b3e1-4e6c-d3b2-d1f6451bdc9d","scrolled":false},"outputs":[{"output_type":"stream","name":"stdout","text":["sparsity is 0.747415\n"]}],"source":["# Initialize co-occurrence matrix\n","X = np.zeros((V, V))\n","for s in x_train:\n","    for i in range(1, len(s)):\n","        j_indexes = i - np.arange(1, window+1)\n","        j_indexes = j_indexes[j_indexes >= 0]\n","        for j in j_indexes:\n","            inc = 1.0 / (i - j)\n","            X[s[i], s[j]] += inc\n","            X[s[j], s[i]] += inc\n","print(f'sparsity is {np.mean(X.flatten() == 0)}')\n","\n","# Co-occurence matrix will be sparse. Which means it is not very efficient "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":611,"status":"ok","timestamp":1653001198304,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"hV5E2Oo5ZDpO","outputId":"957373ff-4b36-4a8a-bcc7-8638ff109ae3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(62045.199999996235, 252585, 1000000)"]},"metadata":{},"execution_count":14},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# This shows distribution of words\n","Xp = X.flatten()\n","Xp = Xp[Xp > 0]\n","plt.hist(np.log(Xp))\n","np.max(Xp), len(Xp), V*V"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1653001198306,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"58whlgWiZDpQ","outputId":"c83c85da-14dc-4698-fe98-5228aaa278fa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(252585,\n"," [(1, 2),\n","  (1, 4),\n","  (1, 5),\n","  (1, 6),\n","  (1, 7),\n","  (1, 8),\n","  (1, 9),\n","  (1, 10),\n","  (1, 11),\n","  (1, 12)])"]},"metadata":{},"execution_count":15}],"source":["Xmax = 1000\n","eps=1e-3\n","lr = 0.01 # Consider using different learning rates and betas \n","beta = 0.9\n","epochs = 100\n","\n","np.random.seed(42)\n","\n","w = 2 * (np.random.rand(2*V, emb_size) - 0.5) / (emb_size + 1)\n","b = 2 * (np.random.rand(2*V) - 0.5) / (emb_size + 1)\n","g_w_s = np.ones((2 * V, emb_size), dtype=np.float32)\n","g_b_s = np.ones(2 * V, dtype=np.float32)\n"," \n","indexes = []\n","all_idx = np.arange(V)\n","for i in range(V):\n","    mask = X[i] != 0\n","    if np.sum(mask) == 0: continue\n","    for j in all_idx[mask]:\n","        indexes.append((i, j))\n","len(indexes), indexes[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1312173,"status":"ok","timestamp":1653002509918,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"1PuiWs2KZDpR","outputId":"9ac8eb43-a4ff-42bb-84a7-6691ada415c5","scrolled":false},"outputs":[{"output_type":"stream","name":"stdout","text":["0 11955.81 0.9726\n","1 2518.79 0.976\n","2 1889.88 0.9688\n","3 1689.09 0.9736\n","4 1575.24 0.9681\n","5 1485.62 0.9594\n","6 1404.03 0.9474\n","7 1338.15 0.9387\n","8 1280.12 0.9333\n","9 1231.59 0.9365\n","10 1193.03 0.9352\n","11 1157.64 0.9389\n","12 1126.08 0.9404\n","13 1106.2 0.9459\n","14 1082.73 0.9482\n","15 1062.85 0.9442\n","16 1043.76 0.9353\n","17 1024.79 0.9415\n","18 1009.74 0.9454\n","19 993.82 0.942\n","20 991.49 0.9396\n","21 974.27 0.9343\n","22 961.15 0.9343\n","23 955.12 0.9337\n","24 947.84 0.9344\n","25 938.2 0.9333\n","26 932.02 0.9282\n","27 925.63 0.9288\n","28 922.69 0.9247\n","29 913.7 0.9256\n","30 913.56 0.9231\n","31 902.99 0.9175\n","32 901.15 0.9221\n","33 899.34 0.9174\n","34 890.63 0.9182\n","35 888.35 0.9256\n","36 887.53 0.9149\n","37 880.8 0.9211\n","38 879.18 0.9139\n","39 876.06 0.9225\n","40 876.15 0.922\n","41 869.01 0.9229\n","42 871.43 0.9203\n","43 868.06 0.9174\n","44 863.64 0.9211\n","45 861.78 0.9148\n","46 862.24 0.9247\n","47 857.98 0.9203\n","48 861.51 0.916\n","49 855.57 0.9188\n","50 851.14 0.9216\n","51 857.46 0.9217\n","52 849.21 0.9187\n","53 852.86 0.9172\n","54 848.84 0.9193\n","55 848.06 0.9216\n","56 845.96 0.9204\n","57 844.38 0.9214\n","58 841.56 0.9227\n","59 841.43 0.923\n","60 843.83 0.9206\n","61 842.36 0.915\n","62 837.61 0.9186\n","63 838.99 0.9202\n","64 838.63 0.9207\n","65 836.62 0.915\n","66 839.79 0.9111\n","67 835.33 0.9111\n","68 834.92 0.9122\n","69 838.25 0.9097\n","70 832.25 0.9144\n","71 832.53 0.9175\n","72 832.16 0.919\n","73 830.17 0.9181\n","74 829.17 0.9101\n","75 843.01 0.9137\n","76 831.93 0.9147\n","77 829.79 0.9101\n","78 829.56 0.9162\n","79 825.22 0.9152\n","80 828.12 0.9164\n","81 832.18 0.9127\n","82 825.91 0.9115\n","83 827.07 0.9104\n","84 826.51 0.9155\n","85 826.45 0.9131\n","86 824.74 0.9132\n","87 828.5 0.9137\n","88 822.3 0.91\n","89 826.47 0.9106\n","90 825.64 0.9067\n","91 821.47 0.9097\n","92 823.86 0.9062\n","93 818.53 0.9051\n","94 825.85 0.9079\n","95 817.27 0.9065\n","96 825.86 0.9036\n","97 823.85 0.9005\n","98 817.51 0.9093\n","99 818.9 0.91\n"]}],"source":["# Training the model \n","def J():\n","    result = 0\n","    for i in range(V):\n","        for j in range(V):\n","            result += f(X[i][j]) * np.power(\n","                np.dot(w[i], w[j+V]) + b[i] + b[j+V] - np.log1p(X[i][j]), 2)\n","            \n","    return result\n","\n","def f(x, alpha=0.75):\n","    if x < Xmax:\n","        return np.power(x / Xmax, alpha)\n","    else:\n","        return 1.0\n","\n","# Gets the word index of desired word     \n","def W(word):\n","    ww = w[word_index[word]]\n","    return ww / np.linalg.norm(ww) # Dividing by L2 norm\n","\n","all_js = []\n","for e in range(epochs): #tqdm(range(epochs)):\n","    cost = 0\n","    shuffle(indexes)\n","    for i, jj in indexes:\n","        j = jj + V\n","        weight = f(X[i][jj]) # Co-occurence matrix \n","        inner = (np.dot(w[i], w[j]) + b[i] + b[j] - np.log(X[i][jj]))\n","        dwi = w[j] * weight * inner\n","        dwj = w[i] * weight * inner\n","        dbi = dbj = weight * inner\n","        cost += weight * inner ** 2\n","        \n","        # this is a variation of gradient descent\n","        w[i] -= lr * dwi / np.sqrt(g_w_s[i] + eps)\n","        w[j] -= lr * dwj / np.sqrt(g_w_s[j] + eps)\n","        b[i] -= lr * dbi / np.sqrt(g_b_s[i] + eps)\n","        b[j] -= lr * dbj / np.sqrt(g_b_s[j] + eps)\n","\n","        g_w_s[i] = beta * g_w_s[i] + np.square(dwi)\n","        g_w_s[j] = beta * g_w_s[j] + np.square(dwj)\n","        g_b_s[i] = beta * g_b_s[i] + np.square(dbi)\n","        g_b_s[j] = beta * g_b_s[j] + np.square(dbj)\n","            \n","    all_js.append(cost)\n","    \n","    print(e, np.round(cost, 2), np.round(W('good').dot(W('bad')), 4))"]},{"cell_type":"markdown","metadata":{"id":"W_KPPY-jZDpT"},"source":["# 3. Word2Vec\n","\n","Let's first download a word2vec dictionary with a vector dimension 300 for each word from Google News."]},{"cell_type":"code","source":["import gensim.downloader as api"],"metadata":{"id":"vSxJfKwVk6n9","executionInfo":{"status":"ok","timestamp":1685817899432,"user_tz":420,"elapsed":960,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":367965,"status":"ok","timestamp":1685818269201,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"},"user_tz":420},"id":"27a0PS2sZDpT","outputId":"5a578f4a-ebe6-4b49-fee8-7ecd277d7a6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"]}],"source":["wv = api.load(\"word2vec-google-news-300\")"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1685818512672,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"},"user_tz":420},"id":"1nbW9vx6-xTC","outputId":"84302252-a4ac-46fa-c8b1-f44b536df553"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<gensim.models.keyedvectors.KeyedVectors at 0x7f1da1f69060>"]},"metadata":{},"execution_count":17}],"source":["wv"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":834,"status":"ok","timestamp":1685818511848,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"},"user_tz":420},"id":"9tVEtZovZDpU","outputId":"b1855451-6047-4f85-e604-346e89e0e9ee"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(300,)"]},"metadata":{},"execution_count":16}],"source":["positive = wv[\"positive\"]\n","\n","positive.shape"]},{"cell_type":"markdown","metadata":{"id":"k5ezLC3iZDpU"},"source":["Because each word is represented as a vector in the space $\\cal{R}^{300}$, we can compute similarity between words."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3783,"status":"ok","timestamp":1653003694216,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"durtSMVsZDpV","outputId":"676bae0c-d549-468b-81ba-a3fae61d9bbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('monarch', 0.7630466222763062), ('prince', 0.7122635841369629), ('princess', 0.6952193379402161), ('royals', 0.691109836101532), ('princes', 0.6675853729248047), ('kings', 0.6575640439987183), ('queens', 0.6341845989227295), ('crown_prince', 0.6330041289329529), ('Queen_Consort', 0.6233130693435669), ('NYC_anglophiles_aflutter', 0.6210921406745911)]\n"]}],"source":["print(wv.most_similar(positive=[\"king\", \"queen\", 'royal'], topn=10))"]},{"cell_type":"markdown","metadata":{"id":"tMWRxlPyZDpW"},"source":["In this case, you can simply compute the cosine distance between two words."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1653003694216,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"VbDFHON6ZDpX","outputId":"2a268053-a4c4-40a7-e83e-350bea9e6e93","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["0.6054363\n"]}],"source":["print(wv.similarity(\"suv\", \"car\" ))"]},{"cell_type":"markdown","metadata":{"id":"CALIYQnyZDpY"},"source":["And you can compute words that match vector operations such as $\\tt{vec(king)} - \\tt{vec(man)} + \\tt{vec(woman)} = ?$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1653003694431,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"AyI_ujr5ZDpY","outputId":"e280a8dd-3087-415f-f53b-81d37d0fc8dd","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["[('queens', 0.595018744468689), ('monarch', 0.5815044641494751), ('kings', 0.5612993240356445), ('royal', 0.5204525589942932), ('princess', 0.5191516876220703), ('princes', 0.5086391568183899), ('NYC_anglophiles_aflutter', 0.5057314038276672), ('Queen_Consort', 0.49256715178489685), ('Queen', 0.48225677013397217), ('royals', 0.4781743586063385)]\n"]}],"source":["print(wv.most_similar(positive=[\"king\", \"queen\"], negative=[\"man\"]))"]},{"cell_type":"markdown","metadata":{"id":"jZ-EL5aoZDpZ"},"source":["Because every word is represented by a vector, at the end a ML model will only be able to represent vectors or differences in vectors as represented by the embedding. If the training set contains bias, the representation of the vectors will be as good as the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":210,"status":"ok","timestamp":1653003694638,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"9g0GEAwIZDpZ","outputId":"00e2771a-5684-460d-84a5-413b66bf63a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('served', 0.43512803316116333), ('serving', 0.4316672682762146), ('Serving', 0.42612528800964355), ('serves', 0.42303547263145447), ('queen', 0.3676721155643463), ('kings', 0.330525279045105), ('Sir_Francis_Walsingham', 0.32565033435821533), ('Sow_Tracey_Ullman', 0.32548266649246216), ('Centrepoint_patron', 0.3240561783313751), ('Serves', 0.3230326473712921)]\n"]}],"source":["print(wv.most_similar(positive=[\"king\", \"serve\"], negative=[\"rule\"]))"]},{"cell_type":"markdown","metadata":{"id":"sQF3XtZZZDpZ"},"source":["Let's see what's happening here by visualizing these 4 points."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":545},"executionInfo":{"elapsed":215,"status":"ok","timestamp":1653003694846,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"},"user_tz":420},"id":"Bgt5bmFLZDpZ","outputId":"795457d4-117d-4b33-b33f-4829602c00b5","scrolled":false},"outputs":[{"output_type":"stream","name":"stdout","text":["[t-SNE] Computing 3 nearest neighbors...\n","[t-SNE] Indexed 4 samples in 0.000s...\n","[t-SNE] Computed neighbors for 4 samples in 0.001s...\n","[t-SNE] Computed conditional probabilities for sample 4 / 4\n","[t-SNE] Mean sigma: 1125899906842624.000000\n","[t-SNE] KL divergence after 250 iterations with early exaggeration: 43.593246\n","[t-SNE] KL divergence after 300 iterations: 0.048801\n","[[-201.32593   -42.881374]\n"," [  -5.586175  -19.825941]\n"," [ -92.09412  -126.23455 ]\n"," [-113.7346     63.29282 ]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n","  FutureWarning,\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["king = wv['king']\n","queen = wv['queen']\n","serve = wv['serve']\n","rule = wv['rule']\n","\n","labels = ['king', 'queen', 'serve', 'rule']\n","x = np.array([king, queen, serve, rule])\n","\n","tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n","tsne_results = tsne.fit_transform(x)\n","\n","print(tsne_results)\n","\n","for label, coord in zip(labels, tsne_results):\n","    plt.plot(coord[0], coord[1], '*', label=label)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Mm5pTOH6ZDpa"},"source":["Why is it important to remove bias from the embeddings?\n","\n","Imagine in a situation where you are analyzing if you are going to give loan to a person based on the difference of two embeddings $\\sum (v_1 - v_2)$, and suppose that the same difference appears in $\\sum (v_3 - v_4)$.\n","\n","For example, $v_1$ could be derived a sentence implying the person has good credit, and $v_2$ could be derived from a sentece where the person has no outstanding loans, whereas $v_3$ could represent a person's race, and $v_4$ could be derived from the person not paying the loan.\n","\n","In our system, we will give a score based on the following result $v_1 - v_2 + v_3$ (very simple logistic classifier). The answer to the system could be a score that represents _the person will pay the loan_ or _the person will not pay the loan_. What will happen in this case?"]},{"cell_type":"markdown","metadata":{"id":"RR1NCdKJqyQf"},"source":["## Answer: \n","The result could be that the person is not given a loan based on the person's race. This is an instance where the model has allocation bias. Allocation bias can be seen as an economic issue, where resources are unfairly allocated to certain groups over others.<br>\n","\n","This problem is due to bias present in the dataset, which stems from bias present in society. "]},{"cell_type":"markdown","metadata":{"id":"MbeafnxAZDpa"},"source":["# 4. Doing Sentiment Analysis of IMDB Movies Using Convolutional Networks\n","\n","In this part of the homework, you will implement sentiment analysis of IMDB movies using the network:\n","\n","```python\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import *\n","\n","# need to truncate sentences, do zero-padding with <PAD>.\n","\n","max_sent_size = MY_MAX_SENT_SIZE\n","emb_size = MY_EMBEDDING_SIZE\n","\n","xin = x = Input((MY_MAX_SENT_SIZE,))\n","\n","model = Sequential([\n","    Embedding(NUM_WORDS, emb_size, input_length=max_sent_size),\n","    Conv1D(128, 5, activation='relu'),\n","    MaxPooling1D(5),\n","    Conv1D(128, 5, activation='relu'),\n","    MaxPooling1D(5),\n","    Conv1D(128, 5, activation='relu'),\n","    MaxPooling1D(5),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","             \n","model.compile(loss=“binary_crossentropy”, optimizer=“adam”)\n","\n","model.fit(...)\n","```\n","\n","Compute the accuracy with the following cases:\n","\n","- emb_size = 15, 25, 50, training embedding with network\n","\n","- use emb_size of 300, and pre-load the embedding from 'word2vec-google-news-300'. You will need to search the internet to check how to preload the embeddings into an Embedding layer. You will need to do the tokenizer that will match the mappings from words to integers in this case, or write a conversion function from ids from imdb to word2vec embeddings from gensim.\n","\n","You will probably need to run these models in a GPU or TPU. If you need assistance, please send me an email."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a19XErrR8bTy"},"outputs":[],"source":["from tensorflow.keras.layers import *\n","from tensorflow.keras.models import *\n","from gensim.models import Word2Vec\n","import gensim\n","from keras import preprocessing\n","from keras.datasets import imdb\n","from tensorflow import keras"]},{"cell_type":"markdown","source":["### Using emb_size of 15 "],"metadata":{"id":"d4qNegYwoerb"}},{"cell_type":"code","source":["# Creating the model\n","# Using emb_size of 15 \n","def CreateModel():\n","    #embedding_matrix = get_weights()\n","\n","    x_i = Input(300) \n","    x   = Embedding(10000, 15, \n","                    input_length=maxlen,\n","                    trainable=False)(x_i)\n","                              \n","\n","    x = Conv1D(128, 5, activation='relu')(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Conv1D(128, 5, activation='relu')(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Conv1D(128, 5, activation='relu')(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Flatten()(x)\n","  \n","    x = Dense(1, activation=\"sigmoid\")(x)\n","\n","    return keras.models.Model(x_i, x)\n"],"metadata":{"id":"gzNlNRhlod3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading the data\n","num_words = 10000\n","maxlen = 300\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n","\n","# Creating the model and preprocessing the data\n","# We pad the sentences here with zeros so it becomes a matrix. \n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)    \n","\n","model = CreateModel()\n","model.summary()\n","model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n","\n","# Fitting the model \n","model.fit(\n","    x_train, y_train, epochs=20, batch_size=64,\n","    validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mC8zp1egomps","executionInfo":{"status":"ok","timestamp":1653004347638,"user_tz":420,"elapsed":51024,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"}},"outputId":"76cdf3ec-8162-44d2-e7fb-6bd2cd951510"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 300)]             0         \n","                                                                 \n"," embedding (Embedding)       (None, 300, 15)           150000    \n","                                                                 \n"," conv1d (Conv1D)             (None, 296, 128)          9728      \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 59, 128)          0         \n"," )                                                               \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 55, 128)           82048     \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 11, 128)          0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 7, 128)            82048     \n","                                                                 \n"," max_pooling1d_2 (MaxPooling  (None, 1, 128)           0         \n"," 1D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 128)               0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 323,953\n","Trainable params: 173,953\n","Non-trainable params: 150,000\n","_________________________________________________________________\n","Epoch 1/20\n","313/313 [==============================] - 14s 6ms/step - loss: 0.6933 - acc: 0.4961 - val_loss: 0.6924 - val_acc: 0.5328\n","Epoch 2/20\n","313/313 [==============================] - 1s 5ms/step - loss: 0.6869 - acc: 0.5430 - val_loss: 0.6645 - val_acc: 0.5994\n","Epoch 3/20\n","313/313 [==============================] - 2s 5ms/step - loss: 0.6581 - acc: 0.6058 - val_loss: 0.6351 - val_acc: 0.6412\n","Epoch 4/20\n","313/313 [==============================] - 2s 5ms/step - loss: 0.6186 - acc: 0.6551 - val_loss: 0.6063 - val_acc: 0.6650\n","Epoch 5/20\n","313/313 [==============================] - 1s 5ms/step - loss: 0.5783 - acc: 0.6959 - val_loss: 0.5919 - val_acc: 0.6794\n","Epoch 6/20\n","313/313 [==============================] - 2s 5ms/step - loss: 0.5398 - acc: 0.7250 - val_loss: 0.5894 - val_acc: 0.6844\n","Epoch 7/20\n","313/313 [==============================] - 2s 5ms/step - loss: 0.5176 - acc: 0.7419 - val_loss: 0.5887 - val_acc: 0.6880\n","Epoch 8/20\n","313/313 [==============================] - 1s 5ms/step - loss: 0.4917 - acc: 0.7583 - val_loss: 0.5803 - val_acc: 0.6916\n","Epoch 9/20\n","313/313 [==============================] - 1s 5ms/step - loss: 0.4534 - acc: 0.7843 - val_loss: 0.6194 - val_acc: 0.6802\n","Epoch 10/20\n","313/313 [==============================] - 2s 5ms/step - loss: 0.4251 - acc: 0.8002 - val_loss: 0.6298 - val_acc: 0.6792\n","Epoch 11/20\n","313/313 [==============================] - 2s 5ms/step - loss: 0.3968 - acc: 0.8181 - val_loss: 0.6601 - val_acc: 0.6750\n","Epoch 12/20\n","313/313 [==============================] - 1s 5ms/step - loss: 0.3665 - acc: 0.8324 - val_loss: 0.6579 - val_acc: 0.6784\n","Epoch 13/20\n","313/313 [==============================] - 2s 5ms/step - loss: 0.3353 - acc: 0.8521 - val_loss: 0.7037 - val_acc: 0.6770\n","Epoch 14/20\n","313/313 [==============================] - 2s 5ms/step - loss: 0.3054 - acc: 0.8679 - val_loss: 0.7629 - val_acc: 0.6664\n","Epoch 15/20\n","313/313 [==============================] - 2s 5ms/step - loss: 0.2844 - acc: 0.8773 - val_loss: 0.7496 - val_acc: 0.6802\n","Epoch 16/20\n","313/313 [==============================] - 2s 5ms/step - loss: 0.2496 - acc: 0.8953 - val_loss: 0.7853 - val_acc: 0.6774\n","Epoch 17/20\n","313/313 [==============================] - 1s 5ms/step - loss: 0.2152 - acc: 0.9144 - val_loss: 0.8845 - val_acc: 0.6654\n","Epoch 18/20\n","313/313 [==============================] - 1s 5ms/step - loss: 0.1837 - acc: 0.9304 - val_loss: 0.9001 - val_acc: 0.6688\n","Epoch 19/20\n","313/313 [==============================] - 1s 5ms/step - loss: 0.1568 - acc: 0.9424 - val_loss: 0.9826 - val_acc: 0.6690\n","Epoch 20/20\n","313/313 [==============================] - 2s 5ms/step - loss: 0.1317 - acc: 0.9533 - val_loss: 1.0767 - val_acc: 0.6634\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f2b44bcf450>"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["### Using emb_size of 25"],"metadata":{"id":"OUKOIiysraVt"}},{"cell_type":"code","source":["# Creating the model\n","# Using emb_size of 25 \n","def CreateModel():\n","    #embedding_matrix = get_weights()\n","\n","    x_i = Input(300) \n","    x   = Embedding(10000, 25, \n","                    input_length=maxlen,\n","                    trainable=False)(x_i)\n","                              \n","\n","    x = Conv1D(128, 5, activation='relu')(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Conv1D(128, 5, activation='relu')(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Conv1D(128, 5, activation='relu')(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Flatten()(x)\n","  \n","    x = Dense(1, activation=\"sigmoid\")(x)\n","\n","    return keras.models.Model(x_i, x)\n"],"metadata":{"id":"j0LdjjL7rkKQ","executionInfo":{"status":"ok","timestamp":1685817094718,"user_tz":420,"elapsed":828,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Loading the data\n","num_words = 10000\n","maxlen = 300\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n","\n","# Creating the model and preprocessing the data\n","# We pad the sentences here with zeros so it becomes a matrix. \n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)    \n","\n","model = CreateModel()\n","model.summary()\n","model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n","\n","# Fitting the model \n","model.fit(\n","    x_train, y_train, epochs=20, batch_size=64,\n","    validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"id":"ToK3blTfroH-","executionInfo":{"status":"error","timestamp":1685817095069,"user_tz":420,"elapsed":7,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}},"outputId":"a6f79d44-4a9a-4613-f113-aabafd7354bc"},"execution_count":11,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-e137c9b60cf1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Creating the model and preprocessing the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'imdb' is not defined"]}]},{"cell_type":"markdown","source":["### Using emb_size of 50"],"metadata":{"id":"A2_8uW1LrkvP"}},{"cell_type":"code","source":["# Creating the model\n","# Using emb_size of 50 \n","def CreateModel():\n","    #embedding_matrix = get_weights()\n","\n","    x_i = Input(300) \n","    x   = Embedding(10000, 50, \n","                    input_length=maxlen,\n","                    trainable=False)(x_i)\n","                              \n","\n","    x = Conv1D(128, 5, activation='relu')(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Conv1D(128, 5, activation='relu')(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Conv1D(128, 5, activation='relu')(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Flatten()(x)\n","  \n","    x = Dense(1, activation=\"sigmoid\")(x)\n","\n","    return keras.models.Model(x_i, x)"],"metadata":{"id":"8sAs7Ahirnbj","executionInfo":{"status":"ok","timestamp":1685817089948,"user_tz":420,"elapsed":809,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Loading the data\n","num_words = 10000\n","maxlen = 300\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n","\n","# Creating the model and preprocessing the data\n","# We pad the sentences here with zeros so it becomes a matrix. \n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)    \n","\n","model = CreateModel()\n","model.summary()\n","model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n","\n","# Fitting the model \n","model.fit(\n","    x_train, y_train, epochs=20, batch_size=64,\n","    validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"id":"3GTEVXOkrnlg","executionInfo":{"status":"error","timestamp":1685817090294,"user_tz":420,"elapsed":2,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}},"outputId":"d6bdd2b9-666e-4eeb-9f68-bc091fdd661f"},"execution_count":9,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-e137c9b60cf1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Creating the model and preprocessing the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'imdb' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"WbylDhd7ImKX"},"source":["### Using emb_size of 300 and word2vec-google-news-300"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1grXAlxkIq32"},"outputs":[],"source":["wv_news = api.load(\"word2vec-google-news-300\")"]},{"cell_type":"code","source":["print(type(wv_news.vectors))\n","wv_news.vectors.shape"],"metadata":{"id":"E2Pfpe--e_3T"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbBjc35WFoxL"},"outputs":[],"source":["def get_weights():\n","  \n","    # Loading GoogleNews \n","    keyed_vectors = api.load(\"word2vec-google-news-300\")\n","    # vectors themselves, a 2D numpy array\n","    weights = keyed_vectors.vectors      \n","\n","    return weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFxe1d2qFo2p"},"outputs":[],"source":["# Creating the model\n","# Using emb_size of 300 and word2vec-google-news-300\n","def CreateModel():\n","    embedding_matrix = get_weights()\n","\n","    x_i = Input(300) \n","    x   = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], \n","                    weights=[embedding_matrix], \n","                    input_length=maxlen,\n","                    trainable=False)(x_i)\n","                              \n","\n","    x = Conv1D(128, 5, activation='relu')(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Conv1D(128, 5, activation='relu')(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Conv1D(128, 5, activation='relu')(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Flatten()(x)\n","  \n","    x = Dense(1, activation=\"sigmoid\")(x)\n","\n","    return keras.models.Model(x_i, x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9SUmpUjFo5m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653005807391,"user_tz":420,"elapsed":106589,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"}},"outputId":"4f1d9094-f321-46da-80c7-87ad8f9292eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, 300)]             0         \n","                                                                 \n"," embedding_3 (Embedding)     (None, 300, 300)          900000000 \n","                                                                 \n"," conv1d_9 (Conv1D)           (None, 296, 128)          192128    \n","                                                                 \n"," max_pooling1d_9 (MaxPooling  (None, 59, 128)          0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_10 (Conv1D)          (None, 55, 128)           82048     \n","                                                                 \n"," max_pooling1d_10 (MaxPoolin  (None, 11, 128)          0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_11 (Conv1D)          (None, 7, 128)            82048     \n","                                                                 \n"," max_pooling1d_11 (MaxPoolin  (None, 1, 128)           0         \n"," g1D)                                                            \n","                                                                 \n"," flatten_3 (Flatten)         (None, 128)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 900,356,353\n","Trainable params: 356,353\n","Non-trainable params: 900,000,000\n","_________________________________________________________________\n"]}],"source":["# Loading the data\n","num_words = 10000\n","maxlen = 300\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n","\n","# Creating the model and preprocessing the data\n","# We pad the sentences here with zeros so it becomes a matrix. \n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)    \n","\n","model = CreateModel()\n","model.summary()\n","model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHXRuTOyEWF-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653005859623,"user_tz":420,"elapsed":51615,"user":{"displayName":"Justin Goh","userId":"02749663324647441700"}},"outputId":"962b03b7-980f-4d82-8b13-b31988d4e324"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","313/313 [==============================] - 3s 9ms/step - loss: 0.6824 - acc: 0.5488 - val_loss: 0.6283 - val_acc: 0.6528\n","Epoch 2/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.5713 - acc: 0.6978 - val_loss: 0.5241 - val_acc: 0.7322\n","Epoch 3/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.4162 - acc: 0.8074 - val_loss: 0.4800 - val_acc: 0.7618\n","Epoch 4/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.2463 - acc: 0.8969 - val_loss: 0.6574 - val_acc: 0.7366\n","Epoch 5/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.1139 - acc: 0.9531 - val_loss: 0.7378 - val_acc: 0.7636\n","Epoch 6/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0738 - acc: 0.9700 - val_loss: 0.8795 - val_acc: 0.7644\n","Epoch 7/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0506 - acc: 0.9778 - val_loss: 1.2006 - val_acc: 0.7424\n","Epoch 8/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0581 - acc: 0.9753 - val_loss: 1.2148 - val_acc: 0.7410\n","Epoch 9/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0677 - acc: 0.9712 - val_loss: 1.2000 - val_acc: 0.7368\n","Epoch 10/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0565 - acc: 0.9755 - val_loss: 1.1049 - val_acc: 0.7576\n","Epoch 11/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0287 - acc: 0.9862 - val_loss: 1.2753 - val_acc: 0.7606\n","Epoch 12/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0217 - acc: 0.9880 - val_loss: 1.3612 - val_acc: 0.7638\n","Epoch 13/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0195 - acc: 0.9889 - val_loss: 1.4527 - val_acc: 0.7614\n","Epoch 14/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0189 - acc: 0.9889 - val_loss: 1.5276 - val_acc: 0.7650\n","Epoch 15/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0186 - acc: 0.9888 - val_loss: 1.5890 - val_acc: 0.7658\n","Epoch 16/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0184 - acc: 0.9888 - val_loss: 1.6688 - val_acc: 0.7628\n","Epoch 17/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0184 - acc: 0.9887 - val_loss: 1.7422 - val_acc: 0.7640\n","Epoch 18/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0183 - acc: 0.9890 - val_loss: 1.7984 - val_acc: 0.7630\n","Epoch 19/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0182 - acc: 0.9890 - val_loss: 1.8397 - val_acc: 0.7670\n","Epoch 20/20\n","313/313 [==============================] - 3s 8ms/step - loss: 0.0180 - acc: 0.9891 - val_loss: 1.9119 - val_acc: 0.7646\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f2a7a35ba50>"]},"metadata":{},"execution_count":35}],"source":["# Fitting the model \n","model.fit(\n","    x_train, y_train, epochs=20, batch_size=64,\n","    validation_split=0.2)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["OUKOIiysraVt","A2_8uW1LrkvP"],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}