{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3bizL-eHWGM0","outputId":"3a5a1f23-5367-45fc-c373-94cf920ca855"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-05-18 22:17:15.954060: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-05-18 22:17:16.207061: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-05-18 22:17:16.211008: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-18 22:17:17.223558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.preprocessing.text import Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"gXBU7lpVWGM2"},"source":["# 1. Introduction\n","\n","In this lab, you will see the power of word embeddings, and how embeddings can be used in different applications.\n","\n","Summarize the papers that were distributed with the module."]},{"cell_type":"markdown","source":["**Efficient Estimation of Word Representations in Vector Space**\n","\n","The renowned word2vec paper introduces a concept that has now become standard: representing words in a continuous vector space (ranging from 20 to 300 dimensions). This model enables the preservation of linguistic regularities like syntax and semantic differences, and it allows for intriguing operations such as computing analogies using vector addition and cosine similarity. \n","\n","The method employed is unsupervised, meaning it only relies on natural language corpora and doesn't necessitate labeled data. This is feasible due to the distributional hypothesis, which posits that words found in similar contexts often carry similar meanings or connotations. The paper investigates several methods with an emphasis on reducing computational complexity. It begins with neural language models (specifically NNLM and the recurrent RNNLM), and then introduces log-linear models such as the continuous bag-of-words (CBOW) and skip-gram.\n","\n","The vocabulary size in the datasets used for this study can be significantly large, with counts of 30,000, 82,000, and even 1 million words. If managed using a straightforward approach, this can lead to a slowdown in the output layer, creating a bottleneck in the process. To prevent this issue, the authors implemented a technique known as hierarchical softmax. This method arranges the vocabulary into a Huffman binary tree, placing more frequently used words nearer to the root of the tree. This approach effectively reduces the complexity of managing the vocabulary to a logarithmic order, denoted as O(log(V)).\n","\n","The focus here is on log-linear models, an approach that aims to economize computational resources by eliminating the large fully-connected layers of the Neural Network Language Model (NNLM). It substitutes these with a more straightforward log-linear model to learn word vectors. An NNLM can then be applied on top of these vectors for further training (for more nuanced details, please refer to the original paper).\n","\n","Two models are discussed. The first one is Continuous Bag of Words (CBOW), which learns by predicting a target word based on the context of 8 surrounding words (4 preceding and 4 following). It's named a bag-of-words model because it averages the 8 context word vectors, effectively disregarding any word order information. The second model, Skip-gram, can be seen as the inverse of CBOW. Given a single word, it aims to predict surrounding words within a certain range (a range randomly selected up to a given limit, say 5). This method seemingly makes better use of the information derived from word proximity as it updates the central word's vector representation using information from several surrounding words.\n","\n","In tasks related to word associations, the skip-gram method generally outperforms others, although CBOW slightly excels in handling syntactic relationships as compared to semantic ones. When it comes to completing sentences, an RNNLM initiated with skip-gram word vectors proves to be the most effective. The study also provides insights about the training process and parallelization, in addition to demonstrating examples of word pair associations."],"metadata":{"id":"lD9Jqy9QlQfJ"}},{"cell_type":"markdown","source":["**Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation**\n","\n","\n","In this paper, the authors introduce a novel RNN Encoder-Decoder model. This model comprises two interconnected recurrent neural networks (RNNs) that are trained in conjunction with each other. The first RNN takes on the role of an encoder, transforming a sequence of symbols into a fixed-length vector representation. Conversely, the second RNN acts as a decoder, converting the fixed-length vector back into a sequence of symbols.\n","\n","The paper also proposes an innovative type of hidden unit that is capable of adaptively remembering and forgetting, serving as an alternative to the commonly used LSTM unit. A novel neural network architecture is proposed that learns to encode a variable-length sequence into a fixed-length vector representation and to decode a given fixed-length vector representation back into a variable-length sequence.The decoder of the proposed model is another RNN which is trained to generate the output sequence by predicting the next symbol yt given the hidden state h<t>.\n","\n","The baseline phrase-based SMT system was built using Moses with default settings. This system achieves a BLEU score of 30.64 and 33.3 on the development and test sets, respectively. The proposed RNN Encoder-Decoder uses 1000 hidden units with the proposed gates at the encoder and at the decoder.At each update, 64 randomly selected phrase pairs are used. The model was trained for approximately three days which outperforms baseline.The CSLM model is trained on 7-grams from the target corpus. Each input word was projected into the embedding space R512, and they were concatenated to form a 3072-dimensional vector.The best performance was achieved when using both CSLM and the phrase scores from the RNN Encoder-Decoder. This suggests that the contributions of the CSLM and the RNN Encoder-Decoder are not too correlated. Furthermore, Word Penalization (WP) is used which is to penalize the number of words that are unknown to the neural networks. Quantitive results showed that the source phrases that are long (more than 3 words per source phrase) and frequent are selected. The choices of the target phrases by the RNN Encoder-Decoder are closer to actual or literal translations and the RNN Encoder-Decoder prefers shorter phrases in general. The RNN Encoder–Decoder is able to propose well-formed target phrases without looking at the actual phrase table. Importantly, the generated phrases do not overlap completely with the target phrases from the phrase table."],"metadata":{"id":"C3UyWTXYmhYK"}},{"cell_type":"markdown","source":["**Man is to Computer Programmer as Woman is to Homemaker?\n","Debiasing Word Embeddings**\n","\n","\n","The study tackles the issue of gender bias in machine learning, arising due to the employment of biased training data, and puts forth a potential solution for rectifying this bias within the model. This review offers a broad synopsis of the study and delves into salient outcomes via the lens of its Python implementation. The authors of the paper used Word Embedding Model to demonstrate gender bias in the training data. The Word Embedding Model was trained on Google News articles and the vector representation of all words were stored in the ‘w2v_gnews_small.txt’ file. The w2v_gnews_small contains words from the Google News articles which are represented as vector in 300 dimensions.\n","\n","Therefore, if we consider words as vectors, their proximity can be determined using cosine similarity, giving us a way to perceive the semantic likeness between them.The article explores the concept of employing vectors to symbolize words for the purpose of analogy detection. For instance, we're seeking to solve an analogy like \"Man is to ? as Woman is to ?\". From the perspective of linear algebra, we would calculate the difference between the vectors of Man and Woman, and then look for a pair of vectors whose difference closely matches that of the Man-Woman difference.The pair of vectors whose difference comes closest to this specific difference is most likely to provide the ideal solution to the analogy, which would be \"Man is to x as Woman is to y\".\n","\n","The best analogy for Man is to ? as Woman is to ?, is Man is to King as Woman is to Queen. Word Embeddings are powerful to capture variety of relationships in corpus using simple vector arithmetic.However, the authors identify the implicit sexism in the trained data that the word embedding model amplifies. For example, sexist analogies returned by the model are:\n","\n","- Man is to computer programmer as woman is to homemaker.\n","\n","- Father is to doctor as mother is to nurse.\n","\n","Thus, there is implicit gender bias in the Google news articles that word embedding model identifies and potentially exaggerates. The paper discusses how the gender bias in the trained data was evaluated and proposed methods to remove or minimize the gender bias. To quantify bias, the authors compared a word vector to a pair of gender-specific words. Consider the word vector “nurse” and gender-specific pair words of Father-Mother. The distances between the vectors Nurse-Mother and Nurse-Father are computed using cosine similarity. Thus, the embeddings imply that nurse is more feminine and closer to the mother vector. This is incorrect since nurses can be both male or female.\n","\n","If it is direct bias the gender-neutral words are projected on the gender axis and should, ideally be neutral. If it is skewed towards male or female, then there is gender bias and if it is Indirect bias the gender-neutral words are projected on extreme-she extreme-he profession axis. The paper specifies that debiasing the model, reduced stereotypical analogies (like man=programmer and woman=homemaker) and preserves the usability of the embeddings.\n","\n","The piece delved into methods for rectifying bias and the measurements used to ascertain its presence in data. Ultimately, it was concluded that Word Embeddings could be employed to detect gender-related associations and bias. Upon recognizing these biases, steps can be taken to eradicate them, thus reducing the propagation of gender bias. It is strongly suggested that the original paper be read, as it is exceptionally well composed.\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"NniBXe-rpQmo"}},{"cell_type":"markdown","source":["**Distributed Representations of Words and Phrases and their Compositionality**\n","\n","This article essentially serves as a mixed bag of various techniques that the authors discovered to enhance their skip-gram model:\n","\n","By sub-sampling common words, they managed to increase processing speed (roughly 2–10 times faster) and also accuracy when dealing with less common words.\n","They introduced negative sampling as a less complex replacement to hierarchical softmax.\n","They proposed a method to identify phrases (for instance, \"Canada Air\" versus \"Canada\" and \"Air\") and process them as individual tokens.\n","They provided an explanation for word vector addition, which isn't exactly a technique — it seems like it was just an intriguing concept that required some elaboration.\n","\n","In the skip-gram model Skip-gram model\n","Objective is to find word representations that are useful for predicting the surrounding words in a sentence or a document. Given a sequence of words \n","w\n","1\n",",\n","w\n","2\n",",\n","…\n",",\n","w\n","T\n",", the Skip-gram model aims to max the average log probability. Larger c results in more training examples and thus can lead to a higher accuracy at the expense of increased training time. The probability \n","p\n","(\n","w\n","O\n","|\n","w\n","I\n",")\n"," is represented with a softmax.\n","\n","Negative sampling is a simplified version of a method known as Noise Contrastive Estimation (NCE), developed by Gutmann and Hyvärinen in 2012. The underlying principle is the ability to separate positive instances from negative ones using logistic regression, also known as binary classification, instead of trying to identify the correct class from the entire vocabulary. This approach bypasses the need for hierarchical softmax. To achieve this, the method involves drawing samples 'k' times from a \"noise distribution\" across the vocabulary, then training the model to select the right word. While the number 'k' varied from approximately 2 to 20, larger values were used for smaller data sets. Although NCE uses the numerical probabilities of the noise distribution for statistical assurances, it was found that the simpler approach of negative sampling was adequate. An interesting observation was the use of the unigram distribution's ¾ power for negative samples, which empirically outperformed other distributions tested. It is intriguing how this specific distribution was chosen and the steps involved in its selection, considering it's not an intuitive result one would naturally arrive at.\n","\n","Subsampling frequent words is an idea that, when you think about it, makes perfect sense. For instance, you see the word \"the\" about ten times more frequently than other words. Wouldn't it be more beneficial to focus on predicting words that are a bit more intriguing? The concept is straightforward, but its actual execution tends to be more complex and engaging. The subsampling approach involves removing training words based on a certain probability. The term 't' represents a threshold (approximately 1e-5) and 'f' stands for the word frequency. It may seem clear-cut, but visualizing this process can be a bit challenging. One thing to note is that when the word frequency is rarer than 't', the term becomes negative, which I interpret as setting the discard probability to zero or below. This ensures we never discard rare words. On the other hand, this method does eliminate common words: for instance, 99% of words with an f value of 0.1 and 90% with an f value of 0.001. The paper's authors maintain that this method keeps the frequency ranking intact.\n","\n","The authors of the cited study adopted a straightforward, yet effective method. This approach scans the training data and assigns scores to bigrams - pairs of words that occur together - based on the ratio of the bigram occurrence to the individual word counts, factoring in a variable discount rate to prevent the grouping of rare words that occur adjacently. If a bigram scores beyond a set threshold, it's merged into a single unit. You might question, doesn't this limit us to phrases composed of only two words? Indeed, it does. However, the procedure can be repeated multiple times, which has proven to be quite effective in practice. For instance, \"San Jose\" becomes \"San Jose Mercury News\", and \"Cincinnati\" transforms into \"Cincinnati Enquirer\". More impressively, \"Belgium\" is associated with \"Brussels Airlines\", and \"Greece\" with \"Aegean Airlines\".\n"],"metadata":{"id":"GeFC0HT9l1ud"}},{"cell_type":"markdown","source":["**Improving Distributional Similarity with Lessons Learned from Wrod Embeddings**\n","\n","\n","The use of neural networks for word embedding, such as Skipgram, appears to surpass conventional count-based distributional models in performance. However, this study suggests that the current dominance of word2vec isn't necessarily due to the inherent superiority of the algorithm itself, but rather a result of effective system design decisions and optimizations of hyperparameters.\n","\n","The conventional technique for word representation is count-based representation, often referred to as the \"bag-of-contexts\" approach. This method involves counting the times two words appear together within a specific window, creating a sparse matrix M, where the rows represent vocabulary words, and the columns represent context. A commonly used metric for this association is pointwise mutual information (PMI). However, PMI can result in negative values, so a modified version, called positive PMI (PPMI), is frequently utilized to overcome this issue. Since word representation of count-based model can be too sparse, Singular Value Decomposition(SVD) is used on PPMI matrix to reduce the dimension.\n","\n","In the study, the author analyzed different parameters for PPMI, SVD, SGNS (Skip-gram with negative sampling), and GloVe, adjusting these variables to understand their impacts. The parameters were classified into three categories: hyperparameters used during the pre-processing stage, those utilized in the association metric calculation, and ones applied during the post-processing phase. The pre-processing parameters included dynamic context window (dyn), subsampling (sub), and the removal of infrequent words (del). The association metric hyperparameters were Shifted PMI (neg) and context distribution smoothing (cds). Lastly, the post-processing hyperparameters included the addition of context vectors (w+c), eigenvalue weighting (eig), and vector normalization (nrm).\n","\n","This study meticulously listed all conceivable hyperparameter options for every embedding technique. The efficacy of each embedding was evaluated based on two distinct tasks. The task of word similarity was examined using datasets such as WordSim Similarity, WordSim Relatedness, MEN, Mechanical Turk, Rare Words, and SimLex-999. The task of word analogy was evaluated using the MSR and Google's analogy dataset, with two different metrics employed, namely 3CosAdd and 3CosMul.\n","\n","The key finding indicated that no specific algorithm was consistently superior to the rest. The effectiveness of each algorithm fluctuated depending on the datasets used and the arrangement of hyperparameters. The paper provided useful setups and valuable advice for practical applications.\n","\n","\n","\n","\n"],"metadata":{"id":"qvmKnKFct_kF"}},{"cell_type":"markdown","metadata":{"id":"wC6badjMWGM3"},"source":["# 2. GloVe\n","\n","We will first read imdb movie reviews to train a GloVe embeddings.\n","\n","GloVe is computed from a co-occurrence matrix $X$ as follows:\n","\n","$\n","J = \\sum_{i=1,j=1}^{V,V} f(X_{ij}) (w_i^T w_j + b_i + b_j - log(X_{ij}))^2\n","$\n","\n","$f(X_{ij}) = (X_{ij} / X_{\\max})^\\alpha$ if $X_{ij} < X_{\\max}$; otherwise it is $1$.\n","\n","$\n","\\nabla_{w_i} J = f(X_{ij}) w_j (w_i^T w_j + b_i + b_j - log(X_{ij}))\n","$\n","\n","$\n","\\nabla_{w_j} J = f(X_{ij}) w_i (w_i^T w_j + b_i + b_j - log(X_{ij}))\n","$\n","\n","$\n","\\nabla_{b_i} J = \\nabla_{b_j} J = f(X_{ij}) (w_i^T w_j + b_i + b_j - log(X_{ij}))\n","$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9Z7VPojWGM3","outputId":"930f2db9-97dc-4865-93e6-0967aa129c7d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685811240975,"user_tz":420,"elapsed":17655,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"]}],"source":["!pip install keras\n","!pip install tensorflow\n","from keras import preprocessing\n","from keras.datasets import imdb\n","from random import shuffle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aNUpuUZTWGM3"},"outputs":[],"source":["num_words = 500\n","maxlen = 200\n","emb_size = 16\n","\n","start_char = 1\n","oov_char = 2\n","index_from = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WH_zs0lDWGM4","outputId":"38aa96ec-f9c3-46f4-e2b1-b8408df77a6c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685811260040,"user_tz":420,"elapsed":4000,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n"]}],"source":["(x_train, _), _ = imdb.load_data(\n","    num_words=num_words, maxlen=maxlen, start_char=start_char, oov_char=oov_char, index_from=index_from\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQzWvnKkWGM4","outputId":"6b4d52c0-fa6f-4bc1-b1be-bce1976ad27d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685811260278,"user_tz":420,"elapsed":249,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1641221/1641221 [==============================] - 0s 0us/step\n"]}],"source":["word_index = imdb.get_word_index()\n","inverted_word_index = dict(\n","    (i + index_from, word) for (word, i) in word_index.items()\n",")\n","# Update `inverted_word_index` to include `start_char` and `oov_char`\n","inverted_word_index[start_char] = \"[START]\"\n","inverted_word_index[oov_char] = \"[OOV]\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a47QVV-vWGM4","outputId":"2492efb4-4c35-4617-c9a2-bd3499c7b8c7","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1685811262232,"user_tz":420,"elapsed":6,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"[START] big [OOV] big [OOV] bad music and a [OOV] [OOV] [OOV] these are the [OOV] to best [OOV] this terrible movie i love [OOV] horror movies and i've seen [OOV] but this had got to be on of the worst ever made the plot is [OOV] [OOV] and [OOV] the acting is an [OOV] the script is completely [OOV] the best is the end [OOV] with the [OOV] and how he [OOV] out who the killer is it's just so [OOV] [OOV] written the [OOV] are [OOV] and funny in [OOV] [OOV] the [OOV] is big [OOV] of [OOV] [OOV] men [OOV] those [OOV] [OOV] [OOV] that show off their [OOV] [OOV] that men actually [OOV] them and the music is just [OOV] [OOV] that plays over and over again in almost every scene there is [OOV] music [OOV] and [OOV] [OOV] away [OOV] and the [OOV] still doesn't close for [OOV] all [OOV] [OOV] this is a truly bad film [OOV] only [OOV] is to look back on the [OOV] that was the [OOV] and have a good old laugh at how bad everything was back then\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["# Decode the first sequence in the dataset\n","decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])\n","decoded_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IL3lGiE-WGM4","outputId":"4d1bf70c-bec0-48a6-cd5e-4e302b5a9216","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1685811264630,"user_tz":420,"elapsed":480,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"big big bad music and a these are the to best this terrible movie i love horror movies and i've seen but this had got to be on of the worst ever made the plot is and the acting is an the script is completely the best is the end with the and how he out who the killer is it's just so written the are and funny in the is big of men those that show off their that men actually them and the music is just that plays over and over again in almost every scene there is music and away and the still doesn't close for all this is a truly bad film only is to look back on the that was the and have a good old laugh at how bad everything was back then\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["words_to_remove = [word_index['br']+index_from, oov_char, start_char]\n","\n","x_train = [[w for w in x_train[i] if w not in words_to_remove] for i in range(len(x_train))]\n","decoded_sequence = \" \".join([inverted_word_index[i] for i in x_train[0]])\n","decoded_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RkF56xivWGM5"},"outputs":[],"source":["# Words we want to analyze\n","\n","w1 = 'good'\n","w2 = 'bad'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOtCab2_WGM5","outputId":"019d038e-8257-47c9-c2bb-614ceaff776f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685811269932,"user_tz":420,"elapsed":191,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(500, 49, 75)"]},"metadata":{},"execution_count":8}],"source":["V = num_words\n","assert max(word_index[w1], word_index[w2]) < V\n","V, word_index[w1], word_index[w2]"]},{"cell_type":"markdown","metadata":{"id":"7GaVLzzxWGM5"},"source":["Let's now get the dictionary of words to indexes and indexes to words."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"uh5JbDtKWGM5","outputId":"bb615ad9-54a2-4314-fa40-725d99cad0f3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685811271445,"user_tz":420,"elapsed":169,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["generating 500 words\n"]}],"source":["words = [w for w in word_index if word_index[w] < V]\n","print(f'generating {V} words')"]},{"cell_type":"markdown","metadata":{"id":"vGhlqzspWGM6"},"source":["Let's build the co-occurrence matrix. Our implementation will not be the most efficient one, but it will serve the purpose."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yAFM7ktoWGM6"},"outputs":[],"source":["window = 5"]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"WJokg_HfLrJ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vj-3rQgnWGM6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685811346215,"user_tz":420,"elapsed":48540,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}},"outputId":"9f858fb3-55e3-45e0-b75d-3dc278726810"},"outputs":[{"output_type":"stream","name":"stdout","text":["sparsity is 0.085212\n"]}],"source":["# Initialize co-occurrence matrix\n","X = np.zeros((V, V))\n","for s in x_train:\n","    for i in range(1, len(s)):\n","        j_indexes = i - np.arange(1, window+1)\n","        j_indexes = j_indexes[j_indexes >= 0]\n","        for j in j_indexes:\n","            inc = 1.0 / (i - j)\n","            X[s[i], s[j]] += inc\n","            X[s[j], s[i]] += inc\n","print(f'sparsity is {np.mean(X.flatten() == 0)}')"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n"],"metadata":{"id":"E3He2f5bMBz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWvD-_k-WGM6","colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"status":"ok","timestamp":1685811394856,"user_tz":420,"elapsed":363,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}},"outputId":"2080b055-45c6-433b-dea9-6bcb38a700c3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(29396.116666667273, 228697, 250000)"]},"metadata":{},"execution_count":16},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["Xp = X.flatten()\n","Xp = Xp[Xp > 0]\n","plt.hist(np.log(Xp))\n","np.max(Xp), len(Xp), V*V"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J67_tkidWGM6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685811402261,"user_tz":420,"elapsed":169,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}},"outputId":"ed036f65-6945-440d-dea8-21107c67729b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(228697,\n"," [(4, 4),\n","  (4, 5),\n","  (4, 6),\n","  (4, 7),\n","  (4, 8),\n","  (4, 9),\n","  (4, 11),\n","  (4, 12),\n","  (4, 13),\n","  (4, 14)])"]},"metadata":{},"execution_count":17}],"source":["Xmax = 1000\n","eps=1e-3\n","lr = 0.1\n","beta = 0.99\n","epochs = 200\n","\n","np.random.seed(42)\n","\n","w = 2 * (np.random.rand(2*V, emb_size) - 0.5) / (emb_size + 1)\n","b = 2 * (np.random.rand(2*V) - 0.5) / (emb_size + 1)\n","g_w_s = np.ones((2 * V, emb_size), dtype=np.float32)\n","g_b_s = np.ones(2 * V, dtype=np.float32)\n"," \n","indexes = []\n","all_idx = np.arange(V)\n","for i in range(V):\n","    mask = X[i] != 0\n","    if np.sum(mask) == 0: continue\n","    for j in all_idx[mask]:\n","        indexes.append((i, j))\n","len(indexes), indexes[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rxbQSwTlWGM6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685817947870,"user_tz":420,"elapsed":5698759,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}},"outputId":"011a3e25-d48e-4fbb-f743-137c18e94033"},"outputs":[{"output_type":"stream","name":"stdout","text":["0 29978.7 0.96\n","1 6306.61 0.7312\n","2 4673.76 0.7734\n","3 3930.75 0.6656\n","4 3575.53 0.6964\n","5 3414.95 0.5974\n","6 3287.7 0.5878\n","7 3143.21 0.6264\n","8 3078.18 0.5868\n","9 3040.62 0.437\n","10 3010.33 0.5977\n","11 3005.79 0.5824\n","12 2969.58 0.4419\n","13 2967.89 0.4481\n","14 2931.86 0.4276\n","15 2948.27 0.3699\n","16 2955.48 0.2377\n","17 2911.93 0.3027\n","18 2906.78 0.3768\n","19 2902.77 0.0399\n","20 2857.9 0.1827\n","21 2956.64 0.3651\n","22 2876.25 0.2185\n","23 2886.31 0.226\n","24 2908.73 0.2327\n","25 2836.38 0.4753\n","26 2897.12 0.2324\n","27 2923.13 0.3834\n","28 2890.52 0.455\n","29 2871.67 0.2835\n","30 2912.67 0.197\n","31 2849.54 0.2066\n","32 2879.6 0.1966\n","33 2866.18 0.1665\n","34 2861.77 0.1818\n","35 2946.15 0.35\n","36 2872.07 0.2856\n","37 2857.89 0.2061\n","38 2940.61 0.3015\n","39 2881.55 0.5004\n","40 2868.95 0.3097\n","41 2876.45 0.3201\n","42 2866.94 0.3836\n","43 2881.56 0.4944\n","44 2902.43 0.4314\n","45 2869.15 0.2953\n","46 2882.85 0.3548\n","47 2876.21 0.3284\n","48 2912.74 0.2729\n","49 2861.83 0.4405\n","50 2881.59 0.4387\n","51 2884.03 0.1751\n","52 2879.5 0.1777\n","53 2875.59 0.2191\n","54 2849.6 0.3727\n","55 2865.17 0.402\n","56 2862.3 0.3916\n","57 2813.31 0.3763\n","58 2800.41 0.4684\n","59 2851.16 0.2891\n","60 2847.62 0.3222\n","61 2865.35 0.3977\n","62 2860.36 0.313\n","63 2860.56 0.3457\n","64 2858.2 0.1375\n","65 2862.26 0.3847\n","66 2865.08 0.3484\n","67 2956.03 0.2345\n","68 2834.61 0.3809\n","69 2927.87 0.2604\n","70 2910.83 0.2783\n","71 2832.31 0.3618\n","72 2838.51 0.3798\n","73 2881.65 0.2403\n","74 2913.26 0.1486\n","75 2881.71 0.1568\n","76 2847.8 0.4097\n","77 2868.47 0.1659\n","78 2887.88 0.2891\n","79 2832.57 0.4706\n","80 2830.7 0.1214\n","81 2870.35 0.3012\n","82 2882.16 0.2768\n","83 2871.08 0.2381\n","84 2855.18 0.0984\n","85 2922.78 0.2508\n","86 2876.14 0.36\n","87 2843.97 0.3884\n","88 2837.64 0.299\n","89 2900.89 0.1327\n","90 2847.15 0.2893\n","91 2862.59 0.3075\n","92 2861.8 0.2757\n","93 2899.01 0.2541\n","94 2886.59 0.2263\n","95 2854.66 0.381\n","96 2824.05 0.3456\n","97 2879.34 0.3491\n","98 2893.78 0.169\n","99 2865.82 0.3316\n","100 2895.37 0.3339\n","101 2859.98 0.1948\n","102 2849.69 0.2125\n","103 2889.46 0.3736\n","104 2857.69 0.3694\n","105 2825.73 0.3238\n","106 2901.76 0.2983\n","107 2842.61 0.2934\n","108 2827.6 0.2734\n","109 2904.63 0.2147\n","110 2933.02 0.275\n","111 2869.61 0.3313\n","112 2821.81 0.3029\n","113 2911.46 0.0312\n","114 2839.77 0.1791\n","115 2849.93 0.033\n","116 2885.0 0.3441\n","117 2867.95 0.183\n","118 2849.31 0.3489\n","119 2856.57 0.1972\n","120 2880.64 0.438\n","121 2813.87 0.264\n","122 2892.76 0.2627\n","123 2971.38 0.3522\n","124 2823.28 0.2476\n","125 2815.02 0.3534\n","126 2846.42 0.3363\n","127 2865.53 0.3865\n","128 2856.1 0.2594\n","129 2843.07 0.2361\n","130 2876.7 0.2812\n","131 2889.23 0.4802\n","132 2876.41 0.2\n","133 2824.29 0.34\n","134 2889.43 0.2275\n","135 2865.48 0.4076\n","136 2849.52 0.3684\n","137 2829.29 0.3596\n","138 2853.84 0.4211\n","139 2866.1 0.3038\n","140 2891.68 0.2161\n","141 2859.17 0.2763\n","142 2849.41 0.196\n","143 2839.06 0.0746\n","144 2874.02 0.3296\n","145 2863.66 0.2447\n","146 2830.21 0.2167\n","147 2866.9 0.3455\n","148 2840.57 0.3054\n","149 2839.48 0.2949\n","150 2821.95 0.2358\n","151 2815.38 0.1675\n","152 2870.51 0.3323\n","153 2834.72 0.4232\n","154 2861.02 0.3716\n","155 2836.34 0.288\n","156 2833.27 0.2536\n","157 2854.48 0.1953\n","158 2841.6 0.3754\n","159 2863.33 0.4279\n","160 2848.83 0.3095\n","161 2822.02 0.2684\n","162 2878.3 0.3203\n","163 2865.36 0.2088\n","164 2848.89 0.3203\n","165 2816.2 0.4395\n","166 2906.7 0.3686\n","167 2832.97 0.311\n","168 2845.89 0.3093\n","169 2887.58 0.2002\n","170 2858.54 0.2101\n","171 2816.37 0.4413\n","172 2823.83 0.2465\n","173 2837.64 0.3087\n","174 2834.88 0.4504\n","175 2814.82 0.3248\n","176 2905.07 0.233\n","177 2830.42 0.3006\n","178 2877.48 0.2692\n","179 2801.26 0.275\n","180 2824.0 0.446\n","181 2876.83 0.3312\n","182 2862.66 0.2592\n","183 2879.12 0.3077\n","184 2839.8 0.3242\n","185 2842.4 0.2849\n","186 2840.95 0.2466\n","187 2819.74 0.1793\n","188 2892.74 0.369\n","189 2851.94 0.2949\n","190 2850.28 0.2955\n","191 2857.11 0.3267\n","192 2843.39 0.2186\n","193 2875.39 0.3552\n","194 2893.71 0.2153\n","195 2841.67 0.261\n","196 2844.36 0.317\n","197 2861.23 0.2639\n","198 2862.63 0.443\n","199 2847.74 0.3118\n"]}],"source":["def J():\n","    result = 0\n","    for i in range(V):\n","        for j in range(V):\n","            result += f(X[i][j]) * np.power(\n","                np.dot(w[i], w[j+V]) + b[i] + b[j+V] - np.log1p(X[i][j]), 2)\n","            \n","    return result\n","\n","def f(x, alpha=0.75):\n","    if x < Xmax:\n","        return np.power(x / Xmax, alpha)\n","    else:\n","        return 1.0\n","    \n","def W(word):\n","    ww = w[word_index[word]]\n","    return ww / np.linalg.norm(ww)\n","\n","all_js = []\n","for e in range(epochs):\n","    cost = 0\n","    shuffle(indexes)\n","    for i, jj in indexes:\n","        j = jj + V\n","        weight = f(X[i][jj])\n","        inner = (np.dot(w[i], w[j]) + b[i] + b[j] - np.log(X[i][jj]))\n","        dwi = w[j] * weight * inner\n","        dwj = w[i] * weight * inner\n","        dbi = dbj = weight * inner\n","        cost += weight * inner ** 2\n","        w[i] -= np.clip(lr * dwi / np.sqrt(g_w_s[i] + eps), -1, 1)\n","        w[j] -= np.clip(lr * dwj / np.sqrt(g_w_s[j] + eps), -1, 1)\n","        b[i] -= np.clip(lr * dbi / np.sqrt(g_b_s[i] + eps), -1, 1)\n","        b[j] -= np.clip(lr * dbj / np.sqrt(g_b_s[j] + eps), -1, 1)\n","\n","        g_w_s[i] = beta * g_w_s[i] + np.square(dwi)\n","        g_w_s[j] = beta * g_w_s[j] + np.square(dwj)\n","        g_b_s[i] = beta * g_b_s[i] + np.square(dbi)\n","        g_b_s[j] = beta * g_b_s[j] + np.square(dbj)\n","            \n","    all_js.append(cost)\n","    \n","    print(e, np.round(cost, 2), np.round(W('good').dot(W('bad')), 4))"]},{"cell_type":"markdown","metadata":{"id":"wzRRKWMhWGM6"},"source":["Now explain with your own words how Glove works.  Find which word is a synonym for `positive` and an antonym for `positive`."]},{"cell_type":"markdown","source":["GloVe, or Global Vectors for Word Representation, is a word embedding technique that represents words as dense vectors in a high-dimensional space. This method captures the semantic meaning of words by leveraging the global statistical information of the corpus, unlike other methods like Word2Vec which primarily depend on local information. In other words, GloVe effectively encapsulates the co-occurrence statistics of the entire corpus into a square word-word co-occurrence matrix, allowing it to capture more nuanced meanings.\n","\n","Here's how it works:\n","\n","Word-Word Co-occurrence Matrix: GloVe starts by creating a word-word co-occurrence matrix X, where each element Xij represents the frequency with which word j appears in the context of word i. The context can be defined by a window around the word i.\n","\n","Matrix Factorization: The aim of GloVe is to factorize this co-occurrence matrix into two smaller matrices, where each row in one matrix represents the vector representation of the corresponding word.\n","\n","Objective Function: GloVe learns these vector representations by minimizing a cost function that represents the difference between the dot product of the vector representations of two words and the logarithm of the number of times these two words co-occur.\n","\n","The vectors learned this way tend to place similar words close to each other and opposite words far from each other in the vector space, hence can be used to find synonyms and antonyms.\n","\n"],"metadata":{"id":"Vtmzf1ncnf5T"}},{"cell_type":"code","source":["synonym = ('',0)\n","antonym = ('',1)\n","for word in words:\n","    if word != 'first':\n","        similarity = W(word).dot(W('first'))\n","        if similarity > synonym[1]:\n","            synonym = (word,similarity)\n","        if similarity < antonym[1]:\n","            antonym = (word,similarity)\n","            \n","print(f\"Synonym for first: {synonym[0]}  Dot product: {synonym[1]}\")\n","print(f\"Antonym for first: {antonym[0]}  Dot product: {antonym[1]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"id":"_xNYfzmpiJcb","executionInfo":{"status":"error","timestamp":1685984951949,"user_tz":420,"elapsed":467,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}},"outputId":"fa015d71-4cce-4e40-c590-7c6ccd32c58c"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4989193edc81>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msynonym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mantonym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'first'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"38SYklpRWGM6","outputId":"c825dfec-0415-47cc-b70e-938d1f506e55"},"outputs":[{"name":"stdout","output_type":"stream","text":["that -0.8571522583437434\n","deal 0.8781584108127223\n"]}],"source":["# your code goes here"]},{"cell_type":"markdown","metadata":{"id":"eSFFg4W-WGM6"},"source":["# 3. Convolution Based NLP\n","\n","In an attempt to create our first generator network before we start using transformers, you will build a large language model using a Convolutional (causal) and Embeddings.\n","\n","We will split this task into two task.\n","\n","- First task is to try to predict the next word.\n","- Second task is to implement and train a network that will use the head to predict the sentiment of the sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Z5CsS8tWGM7"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","import keras\n","from keras import preprocessing\n","from keras.datasets import imdb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJMsObT3WGM7"},"outputs":[],"source":["device = 'cuda'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TE2lw1mlWGM7","outputId":"c3152f90-578e-4363-9811-0c544244d904","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685819660801,"user_tz":420,"elapsed":194,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[79, 76, 83, 83, 86, 7, 94, 86, 89, 83, 75]\n","['[START]', '[OOV]', '[OOV]', 'h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n"]}],"source":["import string\n","\n","class Tokenizer:\n","    def __init__(self):\n","        self.chars = ['\\00', '\\01'] + sorted(list(set(string.printable)))\n","        self.vocab_size = len(self.chars)\n","        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n","        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n","        \n","        self.itos[0] = '[START]'\n","        self.itos[1] = '[OOV]'\n","                            \n","    def encode(self, sentence):\n","        return [self.stoi[c] if c in self.stoi else 1 for c in sentence]\n","    \n","    def decode(self, indexes):\n","        return [self.itos[i] for i in indexes]\n","    \n","    def start(self): return '\\00'\n","\n","    def oov(self): return '\\01'\n","    \n","tokenizer = Tokenizer()\n","print(tokenizer.encode('hello world'))\n","print(tokenizer.decode([0] + [1] + tokenizer.encode('\\x96hello world')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdBSKbY2WGM7"},"outputs":[],"source":["maxlen = 1024\n","emb_size = 50\n","\n","start_char = 1\n","oov_char = 2\n","index_from = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sq5VInFeWGM7"},"outputs":[],"source":["(x_train, y_train), (x_test, y_test) = imdb.load_data(\n","    start_char=start_char, oov_char=oov_char, index_from=index_from\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqoAjfgMWGM7"},"outputs":[],"source":["word_index = imdb.get_word_index()\n","inverted_word_index = dict(\n","    (i + index_from, word) for (word, i) in word_index.items()\n",")\n","# Update `inverted_word_index` to include `start_char` and `oov_char`\n","inverted_word_index[start_char] = tokenizer.start()\n","inverted_word_index[oov_char] = tokenizer.oov()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bs22m05gWGM7","outputId":"054a0307-3cf7-4290-8bbb-3918e4545007","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685819683590,"user_tz":420,"elapsed":13401,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0,  7,  7, ..., 72, 83, 83])"]},"metadata":{},"execution_count":32}],"source":["def retokenize(text):\n","    return [tokenizer.encode('  '.join([inverted_word_index[idx] for idx in text[i]])) \n","            for i in range(len(text))]\n","\n","x_train = retokenize(x_train)\n","x_test = retokenize(x_test)\n","np.array(x_train[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwHdc_CBWGM7","outputId":"51539e46-c329-4366-c1eb-539e49d08c5e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685819697412,"user_tz":420,"elapsed":11685,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((38024315,), (36523678,))"]},"metadata":{},"execution_count":33}],"source":["x_train_c = np.concatenate(x_train)\n","x_test_c = np.concatenate(x_test)\n","x_train_c.shape, x_test_c.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLPYziOUWGM7"},"outputs":[],"source":["vocab_size = tokenizer.vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5oOqVRXWGM7"},"outputs":[],"source":["class CausalConv1d(nn.Module):\n","    def __init__(self, embedding_size, window, dilation):\n","        super().__init__()\n","        self.conv1d = nn.Conv1d(\n","            embedding_size, embedding_size, \n","            kernel_size=window, groups=embedding_size,\n","            padding=(window-1)*dilation, dilation=dilation)\n","        self.ln = nn.LayerNorm(embedding_size)\n","        \n","    def forward(self, x):\n","        # pytorch requires C to be in dimension 1\n","        x = x.permute(0, 2, 1) # (B, C, T)\n","        # we do convolution from C -> C, but with only one group\n","        # that means that we are doing a depthwise convolution\n","        # so that we use the same filter for each embedding\n","        x = self.conv1d(x)\n","        # after the convolution, we need to restore the dimension\n","        # and remove the extra right padding\n","        x = x[:, :, :-self.conv1d.padding[0]]\n","        x = x.permute(0, 2, 1) # (B, T, C)\n","        return self.ln(x)\n","    \n","class Head(nn.Module):\n","    def __init__(self, embedding_size, vocab_size, max_len, window, dropout):\n","        super().__init__()\n","        self.max_len = max_len\n","        self.wte = nn.Embedding(vocab_size, embedding_size)\n","        self.wpe = nn.Embedding(max_len, embedding_size)\n","        self.dropout = nn.Dropout(dropout)\n","        self.conv1d_0 = CausalConv1d(embedding_size, window, dilation=1)\n","        self.conv1d_1 = CausalConv1d(embedding_size, window, dilation=1)\n","        self.lin_0 = nn.Linear(embedding_size, embedding_size)\n","        self.lin_1 = nn.Linear(embedding_size, embedding_size)\n","        self.lin_2 = nn.Linear(embedding_size, embedding_size)\n","        self.ln_0 = nn.LayerNorm(embedding_size)\n","        self.time_shift = nn.ZeroPad2d((0,0,1,0)) # TRICK: time-mix\n","        \n","    def forward(self, idx):\n","        device = idx.device\n","        B, T = idx.shape\n","        \n","        assert T <= self.max_len\n","\n","        tok_emb = self.wte(idx) # (B, T, C)\n","        \n","        B, T, C = tok_emb.shape\n","        \n","        pos = torch.arange(\n","            0, T, dtype=torch.long, device=device).unsqueeze(0)\n","        pos_emb = self.wpe(pos)\n","        x = self.dropout(tok_emb + pos_emb)\n","        x = self.ln_0(x)\n","        x = torch.cat([self.time_shift(x)[:,:T,:C//2], x[:,:T,C//2:]], dim=2) # TRICK: time-mix\n","        # idea is to do Linear C dimension\n","        # followed by a causal filter on the T dimension\n","        x = nn.GELU(approximate='tanh')(self.lin_0(x))\n","        x = nn.GELU(approximate='tanh')(self.conv1d_0(x))\n","\n","        x = nn.GELU(approximate='tanh')(self.lin_1(x))\n","        x = nn.GELU(approximate='tanh')(self.conv1d_1(x))\n","\n","        x = nn.GELU(approximate='tanh')(self.lin_2(x))\n","        return x\n","\n","class LLM(nn.Module):\n","    def __init__(self, embedding_size, vocab_size):\n","        super().__init__()\n","        self.ln = nn.LayerNorm(embedding_size)\n","        self.linv = nn.Linear(embedding_size, vocab_size)\n","        \n","    def forward(self, x):\n","        out = self.linv(self.ln(x))\n","            \n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Re8k54vrWGM8","outputId":"18cecdd1-36d5-408d-c88d-98062159feb5","colab":{"base_uri":"https://localhost:8080/","height":407},"executionInfo":{"status":"error","timestamp":1685819705575,"user_tz":420,"elapsed":393,"user":{"displayName":"Chitvan Patel","userId":"00142577127970509217"}}},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-8226b7277eef>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}],"source":["head = Head(emb_size, vocab_size, maxlen, maxlen, 0.15)\n","head = head.to(device)\n","model = LLM(emb_size, vocab_size)\n","model = model.to(device)\n","for name, params in head.named_parameters():\n","    print(name, params.shape)\n","print()\n","for name, params in model.named_parameters():\n","    print(name, params.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WvZn7o-WGM8"},"outputs":[],"source":["batch_size = 32\n","\n","loss_f = F.cross_entropy\n","\n","def get_batch(data):\n","    ix = torch.randint(len(data) - maxlen, (batch_size,))\n","    x = torch.stack([\n","        torch.from_numpy(data[i:i+maxlen].astype(np.int64)) for i in ix])\n","    y = torch.stack([\n","        torch.from_numpy(data[i+1:i+maxlen+1].astype(np.int64)) for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss(x_train_c, x_test_c, eval_iters=100):\n","    def _internal(model):\n","        dataset = {'train': x_train_c, 'val': x_test_c}\n","        out = {}\n","        model.eval()\n","        for split in ['train', 'val']:\n","            losses = torch.zeros(eval_iters)\n","            for k in range(eval_iters):\n","                X, Y = get_batch(dataset[split])\n","                p = model(head(X))\n","                B, T, C = p.shape\n","                loss = loss_f(p.view(B*T, C), Y.view(B*T))\n","                losses[k] = loss.item()\n","            out[split] = losses.mean()\n","        model.train()\n","        return out\n","    return _internal\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6itjVcDyWGM8","outputId":"d0a1a1f0-b11b-4c70-d3b1-c99fa118a3a1"},"outputs":[{"data":{"text/plain":["(tensor([90, 76, 83,  ..., 91, 79, 76], device='cuda:0'),\n"," tensor([76, 83, 93,  ..., 79, 76,  7], device='cuda:0'))"]},"execution_count":228,"metadata":{},"output_type":"execute_result"}],"source":["x1, y1 = get_batch(x_train_c)\n","x1[0], y1[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4FGXpU-WGM8"},"outputs":[],"source":["import math\n","\n","def get_lr_func(warmup_iters, learning_rate, lr_decay_iters,  min_lr):\n","    def __get_lr__(it):\n","        nonlocal decay_ratio\n","        # 1) linear warmup for warmup_iters steps\n","        if it < warmup_iters:\n","            return learning_rate * it / warmup_iters\n","        # 2) if it > lr_decay_iters, return min learning rate\n","        if it > lr_decay_iters:\n","            return min_lr\n","        # 3) in between, use cosine decay down to min learning rate\n","        decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n","        assert 0 <= decay_ratio <= 1\n","        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n","        return min_lr + coeff * (learning_rate - min_lr)\n","\n","    decay_ratio = 0\n","    return __get_lr__\n","\n","learning_rate = 0.005\n","get_lr = get_lr_func(1000, learning_rate, 100000, learning_rate/10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjG4jgtIWGM8"},"outputs":[],"source":["iter_num = 0\n","logs = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ynjSlTaWGM8","outputId":"97a06d16-a564-4b03-f2d2-345fccfe102a"},"outputs":[{"name":"stdout","output_type":"stream","text":["step 0: train loss 4.7853, val loss 4.7849\n"]}],"source":["epochs = 50000\n","\n","params = list(head.parameters()) + list(model.parameters())\n","estimate_loss_f = estimate_loss(x_train_c, x_test_c)\n","optimizer = torch.optim.AdamW(params, lr=learning_rate)\n","\n","for iter_num in range(iter_num, iter_num + epochs):\n","    learning_rate = get_lr(iter_num)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = learning_rate\n","    if iter_num % 1000 == 0:\n","        losses = estimate_loss_f(model)\n","        print(\n","            f'step {iter_num}: train loss {losses[\"train\"]:.4f}, '\n","            f'val loss {losses[\"val\"]:.4f}')\n","        logs.append((iter_num, losses['train'], losses['val']))\n","    \n","    xb, yb = get_batch(x_train_c)\n","    pb = model(head(xb))\n","    B, T, C = pb.shape\n","    loss = loss_f(pb.view(B*T, C), yb.view(B*T))\n","\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v94HlUG1WGM8","outputId":"a06b110e-2d34-480c-8f42-a6a0d607825b"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABauUlEQVR4nO3dd3hUZcLG4d+UdJKQAGmQQOglNGkCShdEFwHbUlZALKCwFj53hV0LWBbbuq5KEQuIgK6iFDtIlV5D7wRCCYSWTtrMfH8cCEYpCSQ5k+S5r2sucs7MnDyZZcnjOe95X4vL5XIhIiIi4sasZgcQERERuRYVFhEREXF7KiwiIiLi9lRYRERExO2psIiIiIjbU2ERERERt6fCIiIiIm5PhUVERETcnt3sAEXF6XRy/Phx/P39sVgsZscRERGRAnC5XKSmphIREYHVeuXzKGWmsBw/fpzIyEizY4iIiMh1OHLkCNWqVbvi82WmsPj7+wPGDxwQEGByGhERESmIlJQUIiMj836PX0mZKSwXLwMFBASosIiIiJQy1xrOoUG3IiIi4vZUWERERMTtqbCIiIiI2yszY1hERESKg8vlIjc3F4fDYXaUUslms2G32294yhEVFhERkSvIzs4mISGBjIwMs6OUar6+voSHh+Pp6Xndx1BhERERuQyn00lcXBw2m42IiAg8PT01MWkhuVwusrOzOXXqFHFxcdSpU+eqk8NdjQqLiIjIZWRnZ+N0OomMjMTX19fsOKWWj48PHh4eHD58mOzsbLy9va/rOBp0KyIichXXe0ZALimKz1D/K4iIiIjbU2ERERERt6fCIiIiIldUo0YN3nnnHbNjaNCtiIhIWdOpUyeaNWtWJEVj/fr1+Pn53XioG6QzLFeR43Aye+NRhn+2EafTZXYcERGRInFxMryCqFKlilvcJaXCchUZ2Q7Gzt/BTztOsGh3otlxRETEZC6Xi4zs3BJ/uFwF/4/mIUOGsGzZMv773/9isViwWCxMmzYNi8XCjz/+SIsWLfDy8mLFihUcOHCA3r17ExoaSoUKFWjVqhW//PJLvuP9/pKQxWLho48+om/fvvj6+lKnTh3mz59fVB/xFemS0FUE+njwQNvqTFp6gPeX7KdbgxBNGiQiUo6dz3HQ8IWfS/z77nypB76eBfuV/d///pe9e/cSExPDSy+9BMCOHTsAGD16NG+99RY1a9YkKCiII0eOcMcdd/Dqq6/i5eXF9OnT6dWrF3v27CEqKuqK32PcuHG88cYbvPnmm7z33nsMHDiQw4cPExwcfOM/7BXoDMs1DG0fjZfdypYjSaw+cMbsOCIiIlcVGBiIp6cnvr6+hIWFERYWhs1mA+Cll17itttuo1atWgQHB9O0aVOGDRtGTEwMderU4eWXX6ZWrVrXPGMyZMgQ+vfvT+3atfnXv/5FWloa69atK9afS2dYrqGKvxf9WkXy6erDTFi6n3a1K5sdSURETOLjYWPnSz1M+b5FoWXLlvm209LSGDt2LN9//z0JCQnk5uZy/vx54uPjr3qcJk2a5H3t5+dHQEAAiYnFO3RChaUAHulQk5lr41m5/wyb48/RPCrI7EgiImICi8VS4Esz7uj3d/s888wzLFy4kLfeeovatWvj4+PDvffeS3Z29lWP4+HhkW/bYrHgdDqLPO9v6ZJQAVQL8qVP86oATFx6wOQ0IiIiV+fp6YnD4bjm61auXMmQIUPo27cvjRs3JiwsjEOHDhV/wOugwlJAwzvWwmKBhTtPsudEqtlxRERErqhGjRqsXbuWQ4cOcfr06Sue/ahTpw7ffPMNsbGxbNmyhQEDBhT7mZLrpcJSQLVDKtAzJgyASUv3m5xGRETkyp555hlsNhsNGzakSpUqVxyT8vbbbxMUFES7du3o1asXPXr04KabbirhtAVjcRXm5m43lpKSQmBgIMnJyQQEBBTL99h+LJk/vbcCqwWWPtOZqErmT6QjIiLFIzMzk7i4OKKjo/H29jY7Tql2tc+yoL+/C32GZfny5fTq1YuIiAgsFgtz584t8HtXrlyJ3W6nWbNm+faPHTs2b3Kbi4/69esXNlqxi6kaSIe6VXC6YPJyjWUREREpKYUuLOnp6TRt2pQJEyYU6n1JSUkMGjSIrl27Xvb5Ro0akZCQkPdYsWJFYaOViBGdagEwe8NRElMyTU4jIiJSPhT63qyePXvSs2fPQn+j4cOHM2DAAGw222XPytjtdsLCwgp93JLWOjqYltWD2HD4HB+tiOMfdzQwO5KIiEiZVyKDbqdOncrBgwd58cUXr/iaffv2ERERQc2aNRk4cOA1J63JysoiJSUl36MkWCwWRnSuDcCMNYdJyrj6veoiIiJy44q9sOzbt4/Ro0czY8YM7PbLn9Bp06YN06ZN46effmLSpEnExcVx6623kpp65duHx48fT2BgYN4jMjKyuH6EP+hUrwoNwgPIyHYwbdWhEvu+IiIi5VWxFhaHw8GAAQMYN24cdevWveLrevbsyX333UeTJk3o0aMHP/zwA0lJSXz55ZdXfM+YMWNITk7Oexw5cqQ4foTLMs6yGGNZpq48RFpWwZboFhERketTrPMLp6amsmHDBjZv3szIkSMBcDqduFwu7HY7CxYsoEuXLn94X8WKFalbty779195vhMvLy+8vLyKLfu19IwJJ7ryXuJOp/P52nge6VDTtCwiIiJlXbGeYQkICGDbtm3ExsbmPYYPH069evWIjY2lTZs2l31fWloaBw4cIDw8vDjj3RCb1cJjHY2zLB/+epDMnGtPgSwiIiLXp9CFJS0tLa98AMTFxREbG5s3SHbMmDEMGjTIOLjVSkxMTL5HSEgI3t7exMTE5C3C9Mwzz7Bs2TIOHTrEqlWr6Nu3Lzabjf79+xfRj1k8+jSvSnigN4mpWXy96ajZcURERIpEjRo1eOedd8yOkU+hC8uGDRto3rw5zZs3B2DUqFE0b96cF154AYCEhIRr3uHze0ePHqV///7Uq1eP+++/n0qVKrFmzRqqVKlS2HglytNu5dELl4ImLztArsM9118QEREp7Qo9hqVTp05cbTb/adOmXfX9Y8eOZezYsfn2ffHFF4WN4Tb6tYrivcX7OXL2PN9tTchb1VlERESKjhY/vEE+njaGtq8BwKSlB3A6y8TSTCIiUkpNmTKFiIiIP6y63Lt3b4YOHcqBAwfo3bs3oaGhVKhQgVatWvHLL7+YlLbgVFiKwANta1DBy86ek6ks2p1odhwRESkuLhdkp5f8oxDrFN93332cOXOGJUuW5O07e/YsP/30EwMHDiQtLY077riDRYsWsXnzZm6//XZ69epV6OEcJa1Yb2suLwJ9PHigbXUmLT3A+0v2061BCBaLxexYIiJS1HIy4F8RJf99/3EcPP0K9NKgoCB69uzJrFmz8tbvmz17NpUrV6Zz585YrVaaNm2a9/qXX36ZOXPmMH/+/LwpSNyRzrAUkaHto/GyW9lyJInVB86YHUdERMqxgQMH8vXXX5OVlQXAzJkz6devH1arlbS0NJ555hkaNGhAxYoVqVChArt27dIZlvKiir8X/VpF8unqw0xYup92tSubHUlERIqah69xtsOM71sIvXr1wuVy8f3339OqVSt+/fVX/vOf/wDGVCILFy7krbfeonbt2vj4+HDvvfeSne3ea+OpsBShRzrUZObaeFbuP8Pm+HM0jwoyO5KIiBQli6XAl2bM5O3tzd13383MmTPZv38/9erV46abbgJg5cqVDBkyhL59+wLG/GqHDh0yMW3B6JJQEaoW5Jt3W/PEpQdMTiMiIuXZwIED+f777/nkk08YOHBg3v46derwzTffEBsby5YtWxgwYMAf7ihyRyosRWx4x1pYLLBw50n2nLjyatMiIiLFqUuXLgQHB7Nnzx4GDBiQt//tt98mKCiIdu3a0atXL3r06JF39sWd6ZJQEasdUoGeMWH8sO0Ek5bu551+zc2OJCIi5ZDVauX48T+Ot6lRowaLFy/Ot2/EiBH5tt3xEpHOsBSDxzvVBmD+luPEn8kwOY2IiEjpp8JSDGKqBtKhbhWcLvhgucayiIiI3CgVlmIyolMtAL7acJTElEyT04iIiJRuKizFpHV0MC2rB5HtcPLRijiz44iIiJRqKizFxGKxMKKzMZZlxprDJGW494Q8IiIi7kyFpRh1qleFBuEBZGQ7mLbqkNlxRETkOrgKsfCgXF5RfIYqLMXIOMtijGWZuvIQaVm5JicSEZGC8vDwACAjQ3d73qiLn+HFz/R6aB6WYtYzJpzoynuJO53O52vjeaRDTbMjiYhIAdhsNipWrEhiYiIAvr6+WCwWk1OVLi6Xi4yMDBITE6lYsSI2m+26j6XCUsxsVguPdazF37/eyoe/HuSBttXx9rj+/8FERKTkhIWFAeSVFrk+FStWzPssr5cKSwno07wq//llLwnJmXy96SgD21Q3O5KIiBSAxWIhPDyckJAQcnJyzI5TKnl4eNzQmZWLVFhKgKfdyiO31uSl73YyedkB/twyErtNw4dEREoLm81WJL905frpt2YJ6dc6kmA/T46cPc/32xLMjiMiIlKqqLCUEF9PO0Pb1wBg4pIDOJ26TU5ERKSgVFhK0ANta1DBy86ek6ks2q0BXCIiIgWlwlKCAn08eKCtMeD2/SX7NRmRiIhIAamwlLCh7aPxslvZciSJ1QfOmB1HRESkVFBhKWFV/L3o1yoSgAlL95ucRkREpHRQYTHBIx1qYrdaWLn/DJvjz5kdR0RExO2psJigWpAvfZpXBWDi0gMmpxEREXF/KiwmGd6xFhYLLNx5kj0nUs2OIyIi4tZUWExSO6QCtzcy1lWYpLEsIiIiV6XCYqLHO9UG4NutCcSf0fLlIiIiV6LCYqLG1QLpULcKDqeLD5ZrLIuIiMiVqLCYbESnWgB8teEoiSmZJqcRERFxT4UuLMuXL6dXr15ERERgsViYO3dugd+7cuVK7HY7zZo1u+JrXnvtNSwWC0899VRho5VKraODaVk9iGyHk49WxJkdR0RExC0VurCkp6fTtGlTJkyYUKj3JSUlMWjQILp27XrF16xfv54PPviAJk2aFDZWqWWxWBjR2RjLMmPNYZIysk1OJCIi4n4KXVh69uzJK6+8Qt++fQv1vuHDhzNgwADatm172efT0tIYOHAgH374IUFBQYWNVap1qleFBuEBZGQ7mLbqkNlxRERE3E6JjGGZOnUqBw8e5MUXX7zia0aMGMGdd95Jt27dCnTMrKwsUlJS8j1KK+MsizGWZerKQ6Rl5ZqcSERExL0Ue2HZt28fo0ePZsaMGdjt9su+5osvvmDTpk2MHz++wMcdP348gYGBeY/IyMiiimyKnjHhRFf2I/l8Dp+vjTc7joiIiFsp1sLicDgYMGAA48aNo27dupd9zZEjR3jyySeZOXMm3t7eBT72mDFjSE5OznscOXKkqGKbwma18FhH4yzLh78eJDPHYXIiERER92FxuVyu636zxcKcOXPo06fPZZ9PSkoiKCgIm82Wt8/pdOJyubDZbCxYsICUlBT69u2b7zUOhwOLxYLVaiUrKyvfc1eSkpJCYGAgycnJBAQEXO+PZKrsXCcd31xCQnImr/aNYWCb6mZHEhERKVYF/f19+Ws0RSQgIIBt27bl2zdx4kQWL17M7NmziY6Oxul0/uE1Dz74IPXr1+fZZ58tUFkpKzztVh65tSYvfbeTD5Yd5M8tI7HbNFWOiIhIoQtLWloa+/dfWvsmLi6O2NhYgoODiYqKYsyYMRw7dozp06djtVqJiYnJ9/6QkBC8vb3z7f/9a/z8/KhUqdIf9pcH/VpH8v6S/cSfzeD7bQn0blbV7EgiIiKmK/R/vm/YsIHmzZvTvHlzAEaNGkXz5s154YUXAEhISCA+XoNGr5evp52h7WsAMHHJAZzO675iJyIiUmbc0BgWd1IWxrBclHw+h/avLSYtK5cPB7XktoahZkcSEREpFgX9/a0BEm4o0MeDB9oaA27fX7KfMtIpRURErpsKi5sa2j4aL7uVLUeSWH3gjNlxRERETKXC4qaq+HvRr5UxGd6Epfuv8WoREZGyTYXFjT3SoSZ2q4WV+8+wOf6c2XFERERMo8LixqoF+dKnuXFb88SlB0xOIyIiYh4VFjc3vGMtLBZYuPMke06kmh1HRETEFCosbq52SAVubxQGwORlOssiIiLlkwpLKfB4p9oAzN9ynPgzGSanERERKXkqLKVA42qBdKhbBYfTxQfLdZZFRETKHxWWUmJEp1oAfLXhKIkpmSanERERKVkqLKVE6+hgWlYPItvh5KMVcWbHERERKVEqLKWExWJhRGdjLMuMNYdJysg2OZGIiEjJUWEpRTrVq0KD8AAysh1MW3XI7DgiIiIlRoWlFDHOshhjWaauPERaVq7JiUREREqGCksp0zMmnOjKfiSfz+Gz1YfNjiMiIlIiVFhKGZvVwmMdjbMsby3Yw9zNx0xOJCIiUvxUWEqhe1tU494W1XA4XTz9ZSxfrIs3O5KIiEixUmEphaxWC2/c04S/3ByFywWjv9nG1JW61VlERMouFZaCcOSYneAPrFYLL/eO4ZFbowEY9+1OJmlFZxERKaNUWK7mfBJ8+xS8exPkuN/sshaLhX/c0YAnuhjzs7z+027eXrgXl8tlcjIREZGipcJyNZ4VYN9CSI6HbV+ZneayLBYLo7rX4++31wPg3UX7GP/jbpUWEREpU1RYrsZmhzaPGl+vmQRuXAIe71SbF3s1BGDK8oO8MG8HTqf75hURESkMFZZruWkQePhC4g6IW252mqt6sH004+9ujMUCn605zLNfb8Wh0iIiImWACsu1+ARBs4HG12smmZulAPq3juLt+5titcBXG4/y1P9iyXE4zY4lIiJyQ1RYCqLNcOPPvT/BGfe/E6dv82q8P+Am7FYL3245zoiZm8jKdZgdS0RE5LqpsBRE5dpQpwfggrWTzU5TIHc0DmfKoBZ42q0s2HmSR6dv5Hy2SouIiJROKiwFdfNjxp+bZxq3O5cCXeqHMnVIK3w8bCzbe4oHp60jXQsmiohIKaTCUlA1O0GVBpCTDps/MztNgbWvXZnpD7WmgpedNQfP8sDHa0k+734T4YmIiFyNCktBWSyXzrKsnQKO0nOmolWNYGY+3IZAHw82xScx8KM1nE3PNjuWiIhIgamwFEaT+8G3kjGR3J7vzU5TKE0jK/L5IzdTyc+T7cdS6D9lDYmp7jd7r4iIyOWosBSGhw+0HGp8XQpucf69hhEB/G/YzYT4e7HnZCr9PlhDQvJ5s2OJiIhckwpLYbV8CKweEL8ajm0yO02h1Q7x58thbala0YeDp9O5b/JqjpzNMDuWiIjIVamwFFZAOMTcbXxdCs+yANSo7MeXw9tSvZIvR8+d577JqzlwKs3sWCIiIldU6MKyfPlyevXqRUREBBaLhblz5xb4vStXrsRut9OsWbN8+ydNmkSTJk0ICAggICCAtm3b8uOPPxY2Wsm5OPh2xzeQkmBulutUtaIPXw5rS+2QCpxIyeTPH6xm94kUs2OJiIhcVqELS3p6Ok2bNmXChAmFel9SUhKDBg2ia9euf3iuWrVqvPbaa2zcuJENGzbQpUsXevfuzY4dOwobr2RENIeotuDMhfUfmZ3muoUGePO/R2+mYXgAp9Oy6TdlDduOJpsdS0RE5A8sLtf1L0FssViYM2cOffr0ueZr+/XrR506dbDZbMydO5fY2Nirvj44OJg333yThx56qEBZUlJSCAwMJDk5mYCAgAK954bsnAdfDgKfYBi10xiQW0olZ+QwaOo6thxJwt/LzrShrWhRPdjsWCIiUg4U9Pd3iYxhmTp1KgcPHuTFF1+85msdDgdffPEF6enptG3b9oqvy8rKIiUlJd+jRNX/E1SMgvNnYeuXJfu9i1igrwczHmpN6xrBpGbl8sDH61h14LTZsURERPIUe2HZt28fo0ePZsaMGdjt9iu+btu2bVSoUAEvLy+GDx/OnDlzaNiw4RVfP378eAIDA/MekZGRxRH/yqw2aD3M+HrNJLj+E1Vuwd/bg0+HtubWOpXJyHbw4NT1LN2TaHYsERERoJgLi8PhYMCAAYwbN466dete9bX16tUjNjaWtWvX8thjjzF48GB27tx5xdePGTOG5OTkvMeRI0eKOv613fQAeFaAU7vg4JKS//5FzMfTxoeDWtKtQQhZuU4emb6Bn3ecMDuWiIhI8Y5hSUpKIigoCJvNlrfP6XTicrmw2WwsWLCALl26XPa93bp1o1atWnzwwQcFylLiY1gu+uHvsO4DqNMdBn5Vct+3GGXnOnn6f7F8vy0Bm9XC2/c3pXezqmbHEhGRMqigv7+vfI2mCAQEBLBt27Z8+yZOnMjixYuZPXs20dHRV3yv0+kkKyurOOMVjTbDYN0U2LcATu+DynXMTnTDPO1W/tuvGV52K99sPsZT/4slK8fJ/a1K+LKbiIjIBYUuLGlpaezfvz9vOy4ujtjYWIKDg4mKimLMmDEcO3aM6dOnY7VaiYmJyff+kJAQvL298+0fM2YMPXv2JCoqitTUVGbNmsXSpUv5+eefb+BHKyGVakHd22Hvj7B2Mtz5b7MTFQm7zcpb9zXF29PGrLXx/P3rrWTmOhjUtobZ0UREpBwq9BiWDRs20Lx5c5o3bw7AqFGjaN68OS+88AIACQkJxMfHF+qYiYmJDBo0iHr16tG1a1fWr1/Pzz//zG233VbYeOZo+7jxZ+wsOH/O3CxFyGq18GqfGIa2N86EvTBvB1OWHzA5lYiIlEc3NIbFnZg2hgWMO4Qm3wInt8NtL0H7J0v2+xczl8vFvxfs5f0lxpm1p7vV5YmutbFYLCYnExGR0s6t5mEp8yyWS9P1r50Cjlxz8xQxi8XCMz3q8Ux3406v//yyl9d/2kMZ6boiIlIKqLAUlZh7wbcypByF3d+anaZYjOxSh+f/ZMyNM3nZAcZ9uxOnU6VFRESKnwpLUfHwhlYXlhFYPdHcLMXooVuiebWvMWB62qpD/GPONhwqLSIiUsxUWIpSy4fA6gFH18HRDWanKTYD21Tn3/c1xWqBL9YfYdSXseQ6nGbHEhGRMkyFpSj5h0Lje42v10wyN0sxu6dFNd7rfxN2q4V5sccZOWsz2bkqLSIiUjxUWIraxcG3O+dC8jFToxS3O5uEM/kvLfC0WflpxwmGfbaBzByH2bFERKQMUmEpauFNofot4MyF9R+ZnabYdWsYysdDWuLtYWXJnlMMnbae9KyydZeUiIiYT4WlOFw8y7JxKmRnmJulBNxapwqfPtgaP08bqw6cof+HazidVgqWVRARkVJDhaU41OsJFasbs95u/Z/ZaUpEm5qVmPnIzQT7ebL1aDL3TlpF/JmyX9ZERKRkqLAUB6sN2gw3vl4zyZgJtxxoFlmR2cPbUrWiD4fOZHD3pFVsP5ZsdiwRESkDVFiKS/O/gKc/nN4DBxaZnabE1KxSgW8eb0f9MH9Op2XRb8oaVu0/bXYsEREp5VRYiot3gFFaoMzf4vx7oQHefDm8LTfXDCYtK5chU9fz3dbjZscSEZFSTIWlOLUZBlhg/y9wao/ZaUpUgLcH0x5szR2Nw8h2OPnr55uZtjLO7FgiIlJKqbAUp+BoqH+n8fXayeZmMYG3h433+t/EoLbVcblg7Lc7eeOn3Vo0UURECk2FpbhdvMU59nPIOGtuFhPYrBbG3dUob6XniUsP8PfZWzWVv4iIFIoKS3Gr3h7CGkPuedj0qdlpTGGxWBjZpQ6v39MYqwW+2niURz/byPlszYorIiIFo8JS3CwWuPlx4+u1U8CRY24eE/25VRRTHmiJl93K4t2JDPhoDefSs82OJSIipYAKS0mIuQf8QiD1OOycZ3YaU3VrGMqsR9oQ6OPB5vgk7pm8iqPnNMGciIhcnQpLSbB7QauHja/L2S3Ol9OiejCzh7clPNCbg6fSuWfSKnafSDE7loiIuDEVlpLScijYPOHYBjiy3uw0pqsT6s83j7ejbmgFTqZkcd/k1aw9eMbsWCIi4qZUWEpKhSrQ+H7j6zUTzc3iJsIDffhqWDta1QgiNTOXBz5Zx0/bT5gdS0RE3JAKS0m6+cL6QjvnQfJRc7O4iUBfDz57qA23NQwlO9fJ4zM3MmPNYbNjiYiIm1FhKUlhjaHGreBywLoPzU7jNrw9bEwaeBP9W0fidMFzc7fzn4V7NcGciIjkUWEpaRdvcd44DbLTTY3iTuw2K//q25gnutYB4L+L9vGPOdtxOFVaREREhaXk1e0BQdGQmQRbPjc7jVuxWCyMuq0ur/SJwWKBz9fF89iMjWTmaII5EZHyToWlpFltl6brXzMZnJqi/vf+cnN1Jg28CU+7lQU7T/LAx2tJzii/E+6JiIgKizmaDQCvADizDw4sMjuNW7o9JpzpQ1vj721n/aFz3PfBKhKSz5sdS0RETKLCYgYvf7hpkPG1bnG+optrVuKr4W0JDfBi78k07pm4iv2JqWbHEhERE6iwmKX1I2CxwoHFkLjL7DRuq35YAF8/1o6aVfw4npzJPZNWs/Fw+Vv1WkSkvFNhMUtQDah/p/H12smmRnF31YJ8mT28Hc0iK5J8PoeBH63ll50nzY4lIiIlSIXFTBdvcd7yBWTorMHVBPt5MuuRNnSuV4XMHCfDZmzky/VHzI4lIiIlRIXFTFFtIbwZ5GbChk/MTuP2fD3tTBnUkntbVMPhdPH3r7fy/uJ9mmBORKQcUGExk8Vy6SzL+o8gN9vcPKWAh83Km/c24fFOtQB4a8FeXpy/QxPMiYiUcYUuLMuXL6dXr15ERERgsViYO3dugd+7cuVK7HY7zZo1y7d//PjxtGrVCn9/f0JCQujTpw979uwpbLTSqVFfqBAKqQnGGkNyTRaLhb/fXp8XezXEYoHpqw/z1883kZWrCeZERMqqQheW9PR0mjZtyoQJEwr1vqSkJAYNGkTXrl3/8NyyZcsYMWIEa9asYeHCheTk5NC9e3fS08vB1PV2T2j1iPH1mgmgyxsF9mD7aN7t1xwPm4Uftp1g8CfrSMnUBHMiImWRxXUDAwAsFgtz5syhT58+13xtv379qFOnDjabjblz5xIbG3vF1546dYqQkBCWLVtGhw4dCpQlJSWFwMBAkpOTCQgIKOBP4CbST8PbDcGRBUMXQFQbsxOVKqv2n+bRzzaSlpVLg/AAPn2wFSEB3mbHEhGRAijo7+8SGcMydepUDh48yIsvvlig1ycnJwMQHBx8xddkZWWRkpKS71Fq+VWGJvcbX2siuUJrV7syXzx6M5UreLErIYW7J63i4Kk0s2OJiEgRKvbCsm/fPkaPHs2MGTOw2+3XfL3T6eSpp56iffv2xMTEXPF148ePJzAwMO8RGRlZlLFL3sX1hXbNh6R4c7OUQjFVA/nmsXbUqOTL0XPnuXfyamKPJJkdS0REikixFhaHw8GAAQMYN24cdevWLdB7RowYwfbt2/niiy+u+roxY8aQnJyc9zhypJTPyRHaCGp2ApcT1k0xO02pFFXJl9mPtaNx1UDOpmfTf8oalu5JNDuWiIgUgWItLKmpqWzYsIGRI0dit9ux2+289NJLbNmyBbvdzuLFi/O9fuTIkXz33XcsWbKEatWqXfXYXl5eBAQE5HuUehdvcd44HbJ0SeN6VK7gxReP3sytdSpzPsfBw59u4JtNR82OJSIiN6hYC0tAQADbtm0jNjY27zF8+HDq1atHbGwsbdoYg0tdLhcjR45kzpw5LF68mOjo6OKM5b5q3wbBtSArGbZ8bnaaUsvPy87Hg1vRp1kEuU4Xo77cwtj5O4g9koRT87WIiJRK1x5U8jtpaWns378/bzsuLo7Y2FiCg4OJiopizJgxHDt2jOnTp2O1Wv8wDiUkJARvb+98+0eMGMGsWbOYN28e/v7+nDhxAoDAwEB8fHyu92crfaxWYyzLD8/AmknQ8iFjnxSap93K2/c3o4q/Fx/+Gse0VYeYtuoQoQFedGsQSvdGYdxcMxgvu83sqCIiUgCFLiwbNmygc+fOedujRo0CYPDgwUybNo2EhATi4ws3aHTSpEkAdOrUKd/+qVOnMmTIkMJGLN2a9odFL8PZA7B/IdTtYXaiUstqtfDPOxvSskYw82OPs3RPIidTspi5Np6Za+Op4GWnY70qdG8YSqd6IQT6eJgdWUREruCG5mFxJ6V6HpbfW/AcrHrPGIQ7SLPfFpWsXAerDpxh4c6T/LLzJImpWXnP2a0Wbq5ZidsahnJbw1AiKpajM3siIiYq6O9vFRZ3lHQE/tsUXA54bDWENjQ7UZnjdLrYcjSJhTtPsnDnSfYl5h/kHFM1gNsahHFbw1AahPtjsVhMSioiUrapsJR2Xw6GnXOh+QPQ+32z05R5cafTWbjzBAt3nmTD4XP5VkioFuRzYdxLKK1rBGO3aVyRiEhRUWEp7eLXwifdweYFo3Yas+FKiTiTlsWi3Yks2HGSFftPkZnjzHsu0MeDLvVD6N4wlA51q+DnVehhYCIi8hsqLKWdywUfdoHjm6Dzc9Dxb2YnKpfOZzv4dd8pFuw8yeLdiZxNz857ztNupX2tSnRvFEbXBiGE+Gv9IhGRwlJhKQu2fgXfPAwVQuGp7cbKzmIah9PFxsPnWLjzBAt2nuTwmYy85ywWaBZZkdsahtK9YRi1QyqYmFREpPRQYSkLcrPhv00gNQH6ToGmfzY7kVzgcrnYl5jGwp0nWbDzJFt+t25Rzcp+eXccNY8KwmbVoF0RkctRYSkrlr8Fi1+G8Kbw6DLjP+XF7ZxMycy742j1gTNkOy6Ne6lcwZOu9Y3yckudynh7aLI6EZGLVFjKioyz8HYDyM2EB3+C6m3NTiTXkJqZw7K9p1h4YdxLamZu3nM+HjY61K3MbQ3D6Fo/hCA/XeYTkfJNhaUs+fZJ2DgNGvSCP88wO40UQo7Dybq4syzYYdwyfTw5M+85m9VCrybhDO9Ui/phZezvrIhIAamwlCWJu2FiG7BY4YnNEFTD7ERyHVwuFzuOp7DgwqWjXQkpec91qR/CY51q0apGsIkJRURKngpLWfNZXziwGNqOhB6vmp1GisD2Y8lMXnaAH7YlcHER6RbVg3isYy261A/BqoG6IlIOqLCUNfsWwsx7wSvAmEjOy9/sRFJEDp1OZ8qvB5m98SjZucZg3bqhFRjesRa9mkbgoZl1RaQMU2Epa5xOmNAazuyDnm9Am2FmJ5IilpiaydSVh5ix+jCpWcZA3aoVfXj41mj+3CoSX0/NqisiZY8KS1m0/iP4/v8gKBr+ugms+i/vsiglM4eZa+L5eEUcp9OMFaWDfD0Y0i6aQW2r684iESlTVFjKoux0eLshZCZB/y+gXk+zE0kxysxx8PWmo0xZfjBvVl1fTxv9W0fx0C3RRFT0MTmhiMiNU2Epqxa+CCvfgRq3wpDvzE4jJcDhdPHj9gQmLT3AjuPGnUV2q4U+zasyvGNNaodoPJOIlF4qLGVV8lF4pwm4HDB8BYQ1NjuRlBCXy8Wv+04zaekBVh88k7e/e8NQhneqxU1RQSamExG5PiosZdlXD8KOb4yzLPd9Cn6VzE4kJSz2SBKTlx7g550nuPj/4DbRwTzWqRYd61bBoiUcRKSUUGEpyxK2wkddwZFtrOTceyLU6WZ2KjHB/sQ0piw/wJzNx8hxGP9XbhAewGOdanFHTBh23RItIm5OhaWsS9gK3zwCp3Yb260fhW7jwNPX3FxiioTk83yyIo6Za+PJyHYAEBXsy6MdanJvi2pacFFE3JYKS3mQcx5+GQdrJxnblevB3VMgopmpscQ8SRnZfLb6MFNXHeJsejZgrBb9YPto/nJzdQJ9PExOKCKSnwpLebJ/Ecx9HNJOgNUDuvwT2j0BVv1XdXl1PtvBlxuOMGX5QY4lnQeggpedgTdH8VD7aEICvE1OKCJiUGEpbzLOwrdPwK5vje3q7aHvZKgYZW4uMVWOw8n3W41bovecTAXA02blnhZVebRDLaIr+5mcUETKOxWW8sjlgthZ8OPfITvNWHfozn9D4/tAd42Uay6XiyV7Epm09ADrD50DjL8Sd8SEM7xjLRpXCzQ5oYiUVyos5dnZOJgzDI6sNbYb3Q1/eht8NE+HwPpDZ5m89ACLdifm7buldmUe61SLdrUq6ZZoESlRKizlnSMXVvwHlo43JpkLqAp9JkHNjmYnEzex50QqHyw7wLwtx3E4jX8GmlQLZHjHWvRoFIbNquIiIsVPhUUMRzcatz+fPWBstx0JXV8Au5e5ucRtHDmbwccr4vhifTyZOU4Aqlfy5eFborm3RSQ+nhq8LSLFR4VFLslOh5//CRunGtuhMXD3hxDa0Nxc4lbOpGXx6apDTF9zmKSMHACC/Tx54ObqDGpbnUoVVHJFpOipsMgf7fkR5o2EjNNg84JuY6HNcLBqNlS5JCM7l682HOWjFQc5cta4Jdrbw8q9Larx8C01qaE7i0SkCKmwyOWlJRqlZd/PxnbNTsbYloAIU2OJ+8l1OPlpxwmmLD/I1qPJgHFnUc+YMB7tUItmkRXNDSgiZYIKi1yZywUbPjEuE+WeB++K0Ou/0KiP2cnEDblcLtYcPMuU5QdYsudU3v7W0cEM61CTzvVCsGqArohcJxUWubbT++DrhyEh1thuOgB6vg7e+vzk8vacSGXK8oPM33JpscXaIRV49Naa9G4egZddA3RFpHBUWKRgHDmw9DVY8Ta4nMbMuH2nQPW2ZicTN3YiOZOpK+OYtTae1KxcAEL8vRjSvgYD22jNIhEpuIL+/i70aMvly5fTq1cvIiIisFgszJ07t8DvXblyJXa7nWbNmhXZMeUG2Tyg6/Mw5AejrCTFw7Q7YNFLkJttdjpxU2GB3oy5owGrxnThH3fUJyzAm8TULN74aQ/txi/i5e925q1hJCJSFApdWNLT02natCkTJkwo1PuSkpIYNGgQXbt2LbJjShGq3haGrzQuC7mc8Ou/4ePb4NRes5OJG/P39uDRDrVY/vfO/Pu+ptQP8yc928HHK+Lo8MYSnvxiMzuOJ5sdU0TKgBu6JGSxWJgzZw59+vS55mv79etHnTp1sNlszJ07l9jY2Bs+5m/pklAR2jEHvn0KMpPA7gM9XoGWD2k9Irkml8vFsr2nmLL8IKsOnMnbf0vtyjzaoSa31qmsqf9FJJ9iuyR0PaZOncrBgwd58cUXi+yYWVlZpKSk5HtIEWnUFx5fbdzynHsevv8/mPVn45ZokauwWCx0qhfCrEdu5ru/3kKvphHYrBZW7D/NoE/Wcce7K5i7+Rg5DqfZUUWklCn2wrJv3z5Gjx7NjBkzsNvtRXbc8ePHExgYmPeIjIwssmMLxrwsf5kDPcYbk8zt+xkmtjUmnxMpgJiqgbzXvzlLn+nEkHY18PGwsSshhaf+F0vHN5bw0a8HSbswYFdE5FqKtbA4HA4GDBjAuHHjqFu3bpEee8yYMSQnJ+c9jhw5UqTHF4wZcNs+Do8ugZBGxgy5n/eDb580pvsXKYDIYF/G3tWI1WO68Ez3ulSu4Mnx5Exe+X4Xbccv4vWfdpOYkml2TBFxc8U6hiUpKYmgoCBstktzMzidTlwuFzabjQULFtClS5dCHfNKNIalmOVkwuKXYfX7xnZwLWM9omotzM0lpU5mjoM5m4/x4fKDHDxtFF9Pm5U+zSN4tENNaof4m5xQREpSQX9/F901mssICAhg27Zt+fZNnDiRxYsXM3v2bKKjo4vz20tR8vCGHq9Cne4wZ7ix+vPHt0Gn0XDLKLAV618lKUO8PWz0bx3Fn1tG8suuk0xZfpANh8/x5YajfLnhKF3rh/Boh5q0jg7WAF0RyVPo3zJpaWns378/bzsuLo7Y2FiCg4OJiopizJgxHDt2jOnTp2O1WomJicn3/pCQELy9vfPtv9YxxY3U7AiPr4LvnjbuJlryKuxbCHd/AME1zU4npYjVaqF7ozC6Nwpj4+FzTFl+gAU7T7JodyKLdifSNLIiwzrUpEejMGya+l+k3Cv0JaGlS5fSuXPnP+wfPHgw06ZNY8iQIRw6dIilS5de9v1jx479w23N1zpmQeiSUAlzuWDrl/DDM5CVAp4VjGn9mw3U7c9y3Q6eSuOjFXHM3niU7FzjTqLqlXx56JZoWlYPpoq/F8F+niowImWIpuaXknHusHGJKH6Vsd2gF/R8Q6s/yw05nZbF9FWHmL7mMEkZOfmes1qgUgUvqlTwIiTA+LOK/6VHiL933td+njZdVhJxcyosUnKcDlj5X+PykDMXPHzh1lHQdiR4+JidTkqxjOxcvtpwlK83HeV40nnOpGdTmH+xfDxsF0rMb0rNhYJjlB2j3FSq4ImHrUSmpRKR31FhkZKXsAW+fwaOrjO2K0ZB91egwV26TCRFItfh5Gx6NompWZy6+Ei79HViambe1+nZjkIdO9jP81Kx+d1Zm7zSU8GbAB+7ztqIFCEVFjGHywXbZsMvL0LKMWNf9Vug52sQ1tjcbFKupGflcjrtYpH5TcG5UHIulpvTadk4nAX/Z9DTbs0rNHVDK/BE1zpUC/Itxp9EpGxTYRFzZacbl4lW/hdyM8FihZsGQ5fnwK+y2elE8jidLs5lZBslJuWPZ21+e+YmJfOPM/P6eNh4qlsdht4SrctKItdBhUXcQ1I8LHwRdnxjbHsFQqdnodUjYPc0N5tIIWXmOPLO2pxMyeSTlYdYF3cWgPph/rzaN4YW1YNNTilSuqiwiHs5vAp+fBZObDW2K9U21imq293cXCI3wOVyMXvjUf71wy7OXbibqX/rKJ69vR4VfVXIRQpChUXcj9MBsTNh0UuQfsrYV/s26PEvqFK0a02JlKSz6dm89uMuvtxwFIBKfp78884G9G1eVQN0Ra5BhUXcV2YyLH8T1kwGZw5Y7dD6Uej4d/AJMjudyHVbe/AMz83dzr7ENADa1qzEK31jqFWlgsnJRNyXCou4vzMH4Od/wt4fjW2fYGNQboshYLVd9a0i7io718mHvx7k3UX7yMp14mmzMrxTLR7vVAtvD/29Fvk9FRYpPfYvgp//Aad2G9uhMXD7eIjuYG4ukRsQfyaDF+ZvZ+ke4/JnjUq+vNwnhlvrVDE5mYh7UWGR0sWRAxs+gSX/gswkY1+DXnDbyxCsVb2ldHK5XPy4/QTjvt3ByZQsAO5qGsFzf2pAiL+3yelE3IMKi5ROGWeN0rLhE3A5wOYFbUcYU/17+ZudTuS6pGbm8O8Fe5m++hBOF/h72/n77fUZ2DoKqxZylHJOhUVKt5M74afRELfM2K4QBt1ehCb9wKrJuaR02nY0mX/O3cbWo8kANIusyKt9Y2gUEWhyMhHzqLBI6edywZ4fjIG55+KMfRE3Qc/XIbK1udlErpPD6WLGmsO8+fMe0rJysVktPNiuBk/fVhc/L7vZ8URKnAqLlB25WbBmknErdLZxuyiN74duYyGwqqnRRK7XyZRMXvpuJ99vTQAgPNCbsXc1onvDUM3dIuWKCouUPaknYfFLsHkm4AIPX7jlaWj3V/DwMTudyHVZuieR5+dt58jZ8wB0axDC2LsaaUFFKTdUWKTsOr4ZfhwNR9YY24FR0P0laNgH9F+mUgqdz3bw/pJ9TFl+kByHSwsqSrmiwiJlm8sF27+GhS9AyjFjX1Q76PkahDc1N5vIddp3MpV/zt2uBRWlXFFhkfIhOwNWvQsr3oHc84AFbnoAurwAFTRBl5Q+WlBRyhsVFilfko/Cwhdh+2xj2yvAWJuo9TCw6x95KX3OpWczXgsqSjmgwiLl0+HVxvwtCbHGdnAtYzXouj00vkVKpXVxZ/nnnG1aUFHKLBUWKb+cTtgyC34ZB+mJxr7a3eCOtzTNv5RK2blOPlphLKiYmaMFFaVsUWERyUyBX/8NayaCIxvs3tDhb9DuCV0mklLpyNkMnp+nBRWlbFFhEbno9H74/mmIW25sV6kPf/oPVG9nbi6R66AFFaWsUWER+S2XC7Z+CT//AzJOG/ua/8VYDdpXt4xK6ZOamcPbC/fy6SotqCilmwqLyOVknIVfxsKmT41t30rQ/RVo2l+DcqVUutyCin9uFUmVCl5U9veicgVPKlfw0lgXcVsqLCJXE78GvnsaEnca2zVuhTvfhip1zc0lch1+v6Di5fh72fMVmLyH/6XtKhe2fT21CKOUHBUWkWtx5MDq92Hp68akc1YPY22iW0dpbSIplU6mZDJl+UEOnU7ndFoWp1KzOJ2WTbbDWajj+HraLpSYC2XG/2Kh+d22vxd+njbNC1MEHE4XS/ckMmttPGsOnqF2qD831wzm5uhKtKwRhL+3h9kRi40Ki0hBnTsEP/wN9i0wtoOi4U9vQ60upsYSKQoul4uUzFxOp2Vx+kKBOZ2Wlfc4lZp/OzOncOXG28Oa74xNFf/fncGp4Ellfy/CArzx89KZm99LTMnkf+uP8MX6IxxLOn/Z11gt0LhqIDfXrESbmsG0rBFMQBkqMCosIoXhcsHOecakc6kJxr6Ye41J5/xDzc0mUkJcLhfp2Y4LxeZCoUnLzts+lbffKDkZ2Y4CH9tqgfa1K9OnWVW6Nwot02cMrsXpdLHywGlmroln4a6TOJzGr+GKvh7ce1M17mwSzqEz6aw5cJa1cWc4dCYj3/utFoi5WGCig2kVXboLjAqLyPXITIElr8K6KeByglcgdHsRWjwIVq2aK/JbGdm5nE7N5tRvztCc/t0Zm9MXCk/qb8bWeNmtdGsYSu+mEXSsVwUve/kYEHwmLYuvNh5l1tp44s9eKiEtqwcx8OYoesaEX3ZwdELyedYePMuag2dYc/DyBaZRRCA31wymTXQlWkUHE+hTegqMCovIjTi2yRiUe3GK/2qt4E/vQFiMmalESq1Dp9OZv+U4c2OPcfBUet7+QB8P7mgcxl1Nq9ImOrjM3ZLtcrlYG3eWmWvj+Wl7AjkO41euv5edu2+qyoA21akX5l+oY55IzmRt3JkLBeYscafT8z1vsUCjiABujq5Em5qVaF0jmEBf9y0wKiwiN8rpgHUfwuJXIDsVLDZo+zh0GgOefmanEymVXC4XO46nMHfzMb7dejxv8juAsABv7moWQe9mETQMDyjVg3mTMrL5etMxZq09zIHfFLSm1QIZ2KY6f2oaXmR3Y51MycwrL2sPnuHgZQpMw/CAvEtIbaIruVWBKbbCsnz5ct588002btxIQkICc+bMoU+fPgV678qVK+nYsSMxMTHExsbme27ChAm8+eabnDhxgqZNm/Lee+/RunXrAudSYZFik3IcfnwWds03tgMj4Y43oV5Pc3OJlHIOp4u1cWeYt/k4P2xPIDXz0mWj2iEV6N00gt7NqhJVydfElAXncrnYFJ/EzLWH+X5rAlm5xgBmX08bvZtVZWCbKGKqBhZ7jsSUTNbEXbqE9NszWmAUmAZhAXmDeNtEB1PR17zlSoqtsPz444+sXLmSFi1acPfddxe4sCQlJdGiRQtq167NyZMn8xWW//3vfwwaNIjJkyfTpk0b3nnnHb766iv27NlDSEhIgXKpsEix2/szfP8MJMcb2/X/BD1fh8Bq5uYSKQOych0s2X2K+VuO8cuuRLJzL92t1DyqIn2aVeXOJuFUruBlYsrLS83MYe7mY8xcG8/uE6l5+xuEBzCwTRS9m0WYOsg4MTUz3xiYA5cpMPXDAvLGwLSJDibIr+QKTIlcErJYLAUuLP369aNOnTrYbDbmzp2br7C0adOGVq1a8f777wPgdDqJjIzkr3/9K6NHjy5QFhUWKRHZ6bDsdVg9AZy54FkBOv8DWg8Dm27ZFCkKKZk5/Lz9BPO3HGfl/tNcuIkGm9Vy4U6jCLo3CqOCybdJbzuazMy1h5m/5XjeHVNediu9mkYwoE0UzSMruuVlrcTUTNblnYE5y/7EtD+8pn6YPzfXrMTNNYNpHV2J4GIsMG5VWKZOncqkSZNYtWoVr7zySr7Ckp2dja+vL7Nnz853nMGDB5OUlMS8efMue8ysrCyysi5d+0xJSSEyMlKFRUrGyR3GoNwja43tsCbQ6x2o2sLUWCJlTWJKJt9tTWDeluNsOZKUt9/bw0q3BqH0blaVjnWr4Gkvmbv4MrJzmR97nFnr4vOWQwDjEtbANlHc3byaW40PKYhTqVm/KTBn2HeVAvNoh5pEVCzaiTULWliKvZ7u27eP0aNH8+uvv2K3//HbnT59GofDQWho/rkuQkND2b179xWPO378eMaNG1fkeUUKJLQRPPgTbJ4OC1+AE1vhw67Q6mHo+jx4F/91apHyICTAm6G3RDP0lmjiTqczP/Y482KPcfB0Ot9tTeC7rQkX7jQKp0+zCFrVKJ47jXafSGHW2njmbDqWd4u2p81Kz8ZhDGgdRevoYLc8m1IQVfy9uLNJOHc2CQfgdFr+ArP3ZBq7T6Sy+0Qqj3eqZVrOYi0sDoeDAQMGMG7cOOrWLdo1WsaMGcOoUaPyti+eYREpMVYrtBgC9e6EBf+Erf+D9R/Crm/h9vHQqK8WVBQpQtGV/XiyWx2e6Fqb7cdSmBd7jPlbjpOYmsXn6+L5fF084YHe3HVhsG6DcP8bKhGZOQ6+35rArHXxbDx8Lm9/jUq+DGgTxb0tIov1UolZKlfw4o7G4dzR2CgwZy4UmN0nUgkJ8DYtV7FeEkpKSiIoKAib7dJEOE6nE5fLhc1mY8GCBdxyyy3XdUno9zSGRUx3cCl8NwrOHjC2a3eDO96C4GhTY4mUZQ6ni7UHzzA39hg/bj+R706jOiEV6N3MKC+RwQW/02h/Yhqfr4tn9sajJJ/PAcButdC9USgDWlenXa1KZW6+GDO5xRgWp9PJzp078+2bOHEiixcvZvbs2URHR+Pn50ebNm1o3bo17733Xt77oqKiGDlypAbdSumSkwkr/gMr3gZHNti9ocPfoN0TYC97/yUm4k4ycxws3ZPIvNjjLNqd/06jm6Iq0qd5Ve5sHE6ly9xplJXr4OcdJ5m19jBrDp7N21+1og8D2kRxX8tqhPibd3ahLCu2MSxpaWns378/bzsuLo7Y2FiCg4OJiopizJgxHDt2jOnTp2O1WomJyT8zaEhICN7e3vn2jxo1isGDB9OyZUtat27NO++8Q3p6Og8++GBh44mYy8MbOo+BxvfC96Mgbjksfhm2fWXMlFu9rdkJRcosbw8bt8eEc3tMOCmZOfy0/QTzY4+z6sBpNsUnsSk+iXHf7uTWOpXp3SyC7g3DOJOWzax18Xy14Qhn0rMBY6r7LvVDGdgmig51q2DT2RS3UOjCsmHDBjp37py3fXEcyeDBg5k2bRoJCQnEx8cX6ph//vOfOXXqFC+88AInTpygWbNm/PTTT38YiCtSalSuA4PmG+Nafv4nnNoNU2+H5g/AbS+Bb7DZCUXKtABvD+5vGcn9LSNJTMnk260JzI89xpajySzdc4qle07hZd+WN7kbQGiAF/1aRfHnVpFFfieM3DhNzS9S3DLOwi9jYdOnxrZvJej+CjTtr0G5IiXs4Kk05m85zrzY48SdTsdigQ51qjCgTRRd64dgt2mR05KmtYRE3E38GmPulsQL47pq3Aq9/guVzLtNUKS8crlc7EtMw8/LTlWdTTFVQX9/q0qKlJSom2HYcug2Fuw+cOhXmNQe1n4ATuc13y4iRcdisVA31F9lpRRRYREpSTYPuOVpGLEGojtA7nn48e8w/S44d8jsdCIibkuFRcQMQTXggXnGPC0evpfOtmz4BMrGVVoRkSKlwiJiFqsVWj8Cw1dAVFvITjPGuMy4G5KPmp1ORMStqLCImK1SLRjyPfT4lzHR3IHFMLEtbJ6psy0iIheosIi4A6sN2o4wzrZUbQlZKTDvcfi8H6SeMDudiIjpVFhE3EnlOjD0Z+j6Itg8Ye9PMKENbJutsy0iUq6psIi4G5sdbh0Fjy6D8KaQmQRfPwRfDoK0U2anExExhQqLiLsKbQgPL4JO/wCrHXbNh4k3w86CrWAuIlKWqLCIuDObB3R6Fh5ZDCGNIOO0caZl9kPGlP8iIuWECotIaRDeFB5dArf+H1issH22cbZlz49mJxMRKREqLCKlhd0Lur4AD/0CletC2knjLqI5j8H5JLPTiYgUKxUWkdKmWgtjTaK2IwELbJkFk9rB/l/MTiYiUmxUWERKIw8f6PEqDP0JgmtCyjGYcQ98+yRkpZqdTkSkyKmwiJRmUTcbk821HmZsb5xmnG2JW25qLBGRoqbCIlLaefrBHW/A4G+hYhQkxcOnveCHv0N2utnpRESKhAqLSFkR3QEeWwUthhjb6z6AybdA/BpTY4mIFAUVFpGyxMsfev0X/vI1+EfA2YPwye3w8z8h57zZ6URErpsKi0hZVLsbPL4amg0EXLD6ffigAxzdaHYyEZHrosIiUlb5VIQ+E6H/F1AhFE7vhY+7waKXIDfL7HQiIoWiwiJS1tXrCY+vgZh7weWEX/8NUzpDwhazk4mIFJgKi0h54BsM934M908H30qQuAM+7AJLXwNHjtnpRESuSYVFpDxp2BseXwsNeoEzF5aOh4+6wsmdZicTEbkqFRaR8qZCFbj/M7jnY/CuaFwamtLRuFTkyDU7nYjIZamwiJRHFgs0vhdGrIW6t4Mj2xiM+0kPOLXX7HQiIn+gwiJSnvmHGXcR9Z4IXgFwbAN8cCusmQxOp9npRETyqLCIlHcWCzQfaMzbUrMz5GbCT8/CZ30g+ajZ6UREABUWEbkosBo8MAfueAvsPhC3DCa2gy1fgMtldjoRKedUWETkEosFWj9irABdtSVkJcOcYfDlIEg/Y3Y6ESnHVFhE5I8q14ahP0Pn58Bqh13zYeLNsPdns5OJSDmlwiIil2ezQ8e/wcO/QOV6kJ4Is+6H+U9AVprZ6USknFFhEZGri2gOw5bBzSOM7U2fwuT2cHi1ublEpFwpdGFZvnw5vXr1IiIiAovFwty5c6/6+hUrVtC+fXsqVaqEj48P9evX5z//+U++16SmpvLUU09RvXp1fHx8aNeuHevXry9sNBEpLh4+cPu/YPC3EBgJ5w7B1J6w8EUtpCgiJaLQhSU9PZ2mTZsyYcKEAr3ez8+PkSNHsnz5cnbt2sVzzz3Hc889x5QpU/Je8/DDD7Nw4UI+++wztm3bRvfu3enWrRvHjh0rbDwRKU7RHeCxldBsIOCCle8YaxKd2G52MhEp4ywu1/Xfr2ixWJgzZw59+vQp1Pvuvvtu/Pz8+Oyzzzh//jz+/v7MmzePO++8M+81LVq0oGfPnrzyyisFOmZKSgqBgYEkJycTEBBQqDwich12fQvfPgkZZ8DqAV3+Ce2eAKvN7GQiUooU9Pd3iY9h2bx5M6tWraJjx44A5Obm4nA48Pb2zvc6Hx8fVqxYccXjZGVlkZKSku8hIiWoQS94fA3U7QnOHPhlLEy9A87GmZ1MRMqgEiss1apVw8vLi5YtWzJixAgefvhhAPz9/Wnbti0vv/wyx48fx+FwMGPGDFavXk1CQsIVjzd+/HgCAwPzHpGRkSX1o4jIRRVCoP/ncNf74FkBjqyBSe1h4zRNNiciRarECsuvv/7Khg0bmDx5Mu+88w6ff/553nOfffYZLpeLqlWr4uXlxbvvvkv//v2xWq8cb8yYMSQnJ+c9jhw5UhI/hoj8nsUCNz1gjG2p3h5y0o1LRbP+DKknzU4nImVEiRWW6OhoGjduzCOPPMLTTz/N2LFj856rVasWy5YtIy0tjSNHjrBu3TpycnKoWbPmFY/n5eVFQEBAvoeImCioBgz+Drq/AjZP2PezMdncznlmJxORMsCUeVicTidZWX+8FdLPz4/w8HDOnTvHzz//TO/evU1IJyLXzWqFdn+FR5dBWGM4f9aY1v+bYXA+yex0IlKK2Qv7hrS0NPbv35+3HRcXR2xsLMHBwURFRTFmzBiOHTvG9OnTAZgwYQJRUVHUr18fMOZxeeutt3jiiSfyjvHzzz/jcrmoV68e+/fv529/+xv169fnwQcfvNGfT0TMENoQHl4My16DFf+BrV/AoV+hz0So2cnsdCJSChW6sGzYsIHOnTvnbY8aNQqAwYMHM23aNBISEoiPj8973ul0MmbMGOLi4rDb7dSqVYvXX3+dYcOG5b0mOTmZMWPGcPToUYKDg7nnnnt49dVX8fDwuJGfTUTMZPeEri9AnR7GAorn4mB6b2gzHLqNNSajK61yzsOxjZCdATVuAU9fsxOJlHk3NA+LO9E8LCJuLCsNFr4AGz42tivXhb6ToWoLc3MVVGYKHFkHh1fC4VVwfBM4so3nPHyhTndo1Mf409PP1KgipU1Bf3+rsIhIydn3C8wbAWknwGKDjn+HW/8PbG52NjX9DMSvMsrJ4VVwYiu4nPlfUyHUGFyc/Js7FD18oc5t0LAP1O2h8iJSACosIuKeMs7C96NgxxxjO+Im6PsBVKlrXqbkY0YxuVhSTu3+42uCakBUO6h+4RF84S7G45tgx1zYOReSLl0Ox+5jlJdGfYzLYl4Viv/nECmFVFhExL1tm20Ul8xksHvDbS9Bq0eMO42Kk8sFZw9euLyz2vgz6fAfX1elAVRva8wtE9UWAqte+7gJsZfKy7lDl56ze0PtbtCor3Hmxcu/6H4ekVJOhUVE3F/KceMS0YHFxnbNTtB7AgRWK7rv4XRC4k6IX31pDEra7ya0s1ghvOmlMyhRbcGv0vV/T5cLErYYxWXHXGPA8UUXy0vDPlDvdpUXKfdUWESkdHC5YP1HsOB5yD0PXoFw51vQ+D5jFt3CcuRAwtZL5SR+NWQm5X+NzdMY8Hvx8k611uBdTP9uuFzGGJiLZ17OHvxNDq8LZ176QN3biy+DiBtTYRGR0uX0fuP252MbjO2GveHO/1z7TMfFW4wPrzJKypF1kJOR/zUefhDZ2ri8U72dUVY8vC9/vOLkcsHJ7ZfKy5lLc1oZ5aXrpTMv3oEln0/EBCosIlL6OHKNieaWvQbOXONOnLveM8Z9XHS1W4wv8gkyLutcPIMS1sT97kRyueDkjkuXjc7su/SczRNqdblQXnqCT0VzMoqUABUWESm9jm82pvM/vcfYbvYX43LJFW8xDrtUTqq3hyr1i3/wblFyuYxxNhfPvJzee+k5q4dRXhr1gXp3qLxImaPCIiKlW855WPwKrJ4A/O6fqaDo3xSUdsb29Yx3cUcuFyTuunTm5WJpgwvlpbNx5qX+HcaZJJFSToVFRMqGuF9h7WTj8tDFghIQYXaqkpO4+1J5ObXr0n6rh3FX1cUzL77B5uQTuUEqLCIiZc2pPZcuGyXuvLTfaofojkZ5qf8nlRcpVVRYRETKslN7L515Sdxxab/VDo3uNhaYvNZkdyJuQIVFRKS8OL3v0pmXk9uNfXYfuOVpaPdXrSYtbk2FRUSkPDq2CX4aA0fWGNsB1eC2cRBzT9kZmCxlSkF/f5ei+/5EROSaqt4EQ3+Cez8xykrKUfj6IfjkduN2cZFSSoVFRKSssViMMyoj10Onf4CHr3HGZUpnmDsCUk9e+xgibkaFRUSkrPL0hU7PwsgN0Ph+wAWxM+C9m4wZhXOzzE4oUmAqLCIiZV1gVbjnQ3joF2Mdpew0+GUsTGgNu741JqsTcXMqLCIi5UVkK6O09JlsLGdw7hD87y/waS84sd3sdCJXpcIiIlKeWK3QrD/8dSPc+oyxSvShX+GDW+G7pyH9tNkJRS5LhUVEpDzyqgBdn4eR66Bhb2NByQ2fwLs3Ges35WZf+xgiJUiFRUSkPAuqAfdPhyHfQ1hjyEqGn/8Bk9rC3gVmpxPJo8IiIiJQ4xZ4dBn0+i/4VoYz+2HWfTDjHmMNIxGTqbCIiIjBaoMWQ+CJTcaU/lYP2P8LTGwLPz4L58+ZnVDKMRUWERHJzzsQur8CI9ZCvTvA5YC1k43xLes+BEeu2QmlHFJhERGRy6tUC/p/Dg/MgSoN4PxZ+OEZ446iA0vMTifljAqLiIhcXa0uMHwF3PEW+ARB4k74rA98PgDOHDA7nZQTKiwiInJtNju0fgT+ugnaDAeLDfZ8DxPawILnITPF7IRSxqmwiIhIwfkGQ8/X4bFVUKsrOHNg1bvG+kSbpoPTYXZCKaNUWEREpPBC6sNfvoYBX0Kl2pB+Cub/FaZ0gsOrzE4nZZAKi4iIXB+LBer2gMdWQ/dXwSsQTmyFqT3hqyGQFG92QilDVFhEROTG2D2h3Uhj/pYWD4LFCjvmwPutYPErkJVmdkIpAywuV9lYVzwlJYXAwECSk5MJCAgwO46ISPl1Yhv8NMZYVBHAPxw6/A0CI40yY7Uag3YtVmOyOovtwp+W33xt/d3Xv3ltvq8tvzuGNf/xxO0V9Pd3oQvL8uXLefPNN9m4cSMJCQnMmTOHPn36XPH1K1as4Nlnn2X37t1kZGRQvXp1hg0bxtNPP533GofDwdixY5kxYwYnTpwgIiKCIUOG8Nxzz2Ep4F84FRYRETficsGub2HBc5B02Lwc1ypAflWgSj2oUh8q1zX+rFQL7F7mZS5nCvr7217YA6enp9O0aVOGDh3K3Xfffc3X+/n5MXLkSJo0aYKfnx8rVqxg2LBh+Pn58eijjwLw+uuvM2nSJD799FMaNWrEhg0bePDBBwkMDOSJJ54obEQRETGbxQIN74I63WHtJNj9AziyjVlzXS7jbiKX48KfzgtfO3/z9dX2X3iuIFwOcFzlzqX0U8a8Mvmy2yA42igvVepB5XoX/qwLnr7X/5nIDbmhS0IWi+WaZ1gu5+6778bPz4/PPvsMgD/96U+Ehoby8ccf573mnnvuwcfHhxkzZhTomDrDIiJSzjidl8rLb4tMXtlxXqYY/eZ5Zy6kHIdTu40FHk/vMf7MutKcMhaoGPm7IlMfqtQ1ljOQ61JsZ1hu1ObNm1m1ahWvvPJK3r527doxZcoU9u7dS926ddmyZQsrVqzg7bffvuJxsrKyyMrKyttOSdGkRSIi5YrVyg3fOxIWA3W7X9p2uSA1wSguvy0xp3ZDxhnjzqekeNi3IP9x/MPzn425WGr8Kt9YPslTYoWlWrVqnDp1itzcXMaOHcvDDz+c99zo0aNJSUmhfv362Gw2HA4Hr776KgMHDrzi8caPH8+4ceNKIrqIiJQXFgsERBiPWp3zP5d++lJ5Ob330pmZ1IRLj4NL87/Ht1L+8TFVLvzpH65BwYVUYoXl119/JS0tjTVr1jB69Ghq165N//79Afjyyy+ZOXMms2bNolGjRsTGxvLUU08RERHB4MGDL3u8MWPGMGrUqLztlJQUIiMjS+RnERGRcsivsvGo0T7//sxkOLX3wtmY3ZfOziQdNs7KHF5pPH7LK+A3JabepUdg1IUzR/J7poxheeWVV/jss8/Ys2cPAJGRkYwePZoRI0bke82MGTPYvXt3gY6pMSwiIuJWstPh9L78Z2NO7YGzB41xNZdj9zGKS+1uxqDlsCZl/kyM245hAXA6nfnGn2RkZGD9XaO02Ww4nQUcBS4iIuJuPP0gopnx+K3cLGOV69+Ojzm1F87sg9zzkBBrPH59C4JqQIO7oGFvqNqizJeXqyl0YUlLS2P//v1523FxccTGxhIcHExUVBRjxozh2LFjTJ8+HYAJEyYQFRVF/fr1AWMel7feeivf7cq9evXi1VdfJSoqikaNGrF582befvtthg4deqM/n4iIiHuxe0FoQ+PxW45c4zLS0Q2waz7s/wXOHTIWl1z1LgRUvVBe7oLINsY8MuVIoS8JLV26lM6dO/9h/+DBg5k2bRpDhgzh0KFDLF26FID33nuPDz74gLi4OOx2O7Vq1eKRRx5h2LBheWdVUlNTef7555kzZw6JiYlERETQv39/XnjhBTw9PQuUS5eERESkTMlOh30LYec8466k7N8sceAXAg16GeWl+i1gM+WCSZEotplu3ZUKi4iIlFk5mXBgsVFe9vwIWcmXnvMJhvp3GpeNojsaazuVIiosIiIiZVFuNsQth13zYNd3cP7spee8AqHe7UZ5qdUFPHzMy1lAKiwiIiJlnSPXuGV613xj7aa0k5ee8/AzJsVr2Btq3wZeFczLeRUqLCIiIuWJ0wFH1hnlZed8SDl66Tm794VbpXtD3R5utZSACouIiEh55XLBsU3GZaOd8+Fc3KXnbJ5Qs5NRXurdAb7BpsUEFRaz44iIiLgHlwtObLt05uX0nkvPWWwQfatRXur/CSqElHg8FRYRERH5o8Tdl8rLyW2/ecIC1dsZc7006AWBVUskjgqLiIiIXN2ZA5fKy/FN+Z+r1so489LgLgiqXmwRVFhERESk4JLijTuNds6HI2uB39SD8KZGcblpMFSoUqTfVoVFRERErk9KAuz+zpio7vBKcF1Y2++vm6BSraL9Vu68+KGIiIi4sYBwaP2I8Ug/Dbu/h4QtRV5WCkOFRURERK7MrzK0GGx2CqxmBxARERG5FhUWERERcXsqLCIiIuL2VFhERETE7amwiIiIiNtTYRERERG3p8IiIiIibk+FRURERNyeCouIiIi4PRUWERERcXsqLCIiIuL2VFhERETE7amwiIiIiNsrM6s1u1wuAFJSUkxOIiIiIgV18ff2xd/jV1JmCktqaioAkZGRJicRERGRwkpNTSUwMPCKz1tc16o0pYTT6eT48eP4+/tjsViK7LgpKSlERkZy5MgRAgICiuy4ZZE+q4LTZ1U4+rwKTp9VwemzKrji/KxcLhepqalERERgtV55pEqZOcNitVqpVq1asR0/ICBAf6ELSJ9VwemzKhx9XgWnz6rg9FkVXHF9Vlc7s3KRBt2KiIiI21NhEREREbenwnINXl5evPjii3h5eZkdxe3psyo4fVaFo8+r4PRZFZw+q4Jzh8+qzAy6FRERkbJLZ1hERETE7amwiIiIiNtTYRERERG3p8IiIiIibk+F5RomTJhAjRo18Pb2pk2bNqxbt87sSG5n/PjxtGrVCn9/f0JCQujTpw979uwxO1ap8Nprr2GxWHjqqafMjuKWjh07xl/+8hcqVaqEj48PjRs3ZsOGDWbHcjsOh4Pnn3+e6OhofHx8qFWrFi+//PI112YpL5YvX06vXr2IiIjAYrEwd+7cfM+7XC5eeOEFwsPD8fHxoVu3buzbt8+csCa72meVk5PDs88+S+PGjfHz8yMiIoJBgwZx/PjxEsmmwnIV//vf/xg1ahQvvvgimzZtomnTpvTo0YPExESzo7mVZcuWMWLECNasWcPChQvJycmhe/fupKenmx3Nra1fv54PPviAJk2amB3FLZ07d4727dvj4eHBjz/+yM6dO/n3v/9NUFCQ2dHczuuvv86kSZN4//332bVrF6+//jpvvPEG7733ntnR3EJ6ejpNmzZlwoQJl33+jTfe4N1332Xy5MmsXbsWPz8/evToQWZmZgknNd/VPquMjAw2bdrE888/z6ZNm/jmm2/Ys2cPd911V8mEc8kVtW7d2jVixIi8bYfD4YqIiHCNHz/exFTuLzEx0QW4li1bZnYUt5WamuqqU6eOa+HCha6OHTu6nnzySbMjuZ1nn33Wdcstt5gdo1S48847XUOHDs237+6773YNHDjQpETuC3DNmTMnb9vpdLrCwsJcb775Zt6+pKQkl5eXl+vzzz83IaH7+P1ndTnr1q1zAa7Dhw8Xex6dYbmC7OxsNm7cSLdu3fL2Wa1WunXrxurVq01M5v6Sk5MBCA4ONjmJ+xoxYgR33nlnvr9fkt/8+fNp2bIl9913HyEhITRv3pwPP/zQ7FhuqV27dixatIi9e/cCsGXLFlasWEHPnj1NTub+4uLiOHHiRL7/LwYGBtKmTRv9W18AycnJWCwWKlasWOzfq8wsfljUTp8+jcPhIDQ0NN/+0NBQdu/ebVIq9+d0Onnqqado3749MTExZsdxS1988QWbNm1i/fr1ZkdxawcPHmTSpEmMGjWKf/zjH6xfv54nnngCT09PBg8ebHY8tzJ69GhSUlKoX78+NpsNh8PBq6++ysCBA82O5vZOnDgBcNl/6y8+J5eXmZnJs88+S//+/Utk8UgVFilSI0aMYPv27axYscLsKG7pyJEjPPnkkyxcuBBvb2+z47g1p9NJy5Yt+de//gVA8+bN2b59O5MnT1Zh+Z0vv/ySmTNnMmvWLBo1akRsbCxPPfUUERER+qykWOTk5HD//ffjcrmYNGlSiXxPXRK6gsqVK2Oz2Th58mS+/SdPniQsLMykVO5t5MiRfPfddyxZsoRq1aqZHcctbdy4kcTERG666Sbsdjt2u51ly5bx7rvvYrfbcTgcZkd0G+Hh4TRs2DDfvgYNGhAfH29SIvf1t7/9jdGjR9OvXz8aN27MAw88wNNPP8348ePNjub2Lv57rn/rC+5iWTl8+DALFy4skbMroMJyRZ6enrRo0YJFixbl7XM6nSxatIi2bduamMz9uFwuRo4cyZw5c1i8eDHR0dFmR3JbXbt2Zdu2bcTGxuY9WrZsycCBA4mNjcVms5kd0W20b9/+D7fH7927l+rVq5uUyH1lZGRgteb/59xms+F0Ok1KVHpER0cTFhaW79/6lJQU1q5dq3/rL+NiWdm3bx+//PILlSpVKrHvrUtCVzFq1CgGDx5My5Ytad26Ne+88w7p6ek8+OCDZkdzKyNGjGDWrFnMmzcPf3//vOu+gYGB+Pj4mJzOvfj7+/9hbI+fnx+VKlXSmJ/fefrpp2nXrh3/+te/uP/++1m3bh1TpkxhypQpZkdzO7169eLVV18lKiqKRo0asXnzZt5++22GDh1qdjS3kJaWxv79+/O24+LiiI2NJTg4mKioKJ566ileeeUV6tSpQ3R0NM8//zwRERH06dPHvNAmudpnFR4ezr333sumTZv47rvvcDgcef/eBwcH4+npWbzhiv0+pFLuvffec0VFRbk8PT1drVu3dq1Zs8bsSG4HuOxj6tSpZkcrFXRb85V9++23rpiYGJeXl5erfv36rilTppgdyS2lpKS4nnzySVdUVJTL29vbVbNmTdc///lPV1ZWltnR3MKSJUsu+2/U4MGDXS6XcWvz888/7woNDXV5eXm5unbt6tqzZ4+5oU1ytc8qLi7uiv/eL1mypNizWVwuTYUoIiIi7k1jWERERMTtqbCIiIiI21NhEREREbenwiIiIiJuT4VFRERE3J4Ki4iIiLg9FRYRERFxeyosIiIi4vZUWERERMTtqbCIiIiI21NhEREREbenwiIiIiJu7/8B6OyufZFHZtwAAAAASUVORK5CYII=\n","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.plot([v[1] for v in logs][3:], label='train')\n","plt.plot([v[2] for v in logs][3:], label='val')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijFzOB_AWGNA","outputId":"0226a61b-b8b4-4cae-8930-d4ba9a852e11"},"outputs":[{"data":{"text/plain":["array([ 0,  7,  7, 87, 83, 76, 72, 90, 76,  7,  7, 78, 80, 93, 76,  7,  7,\n","       91, 79, 80])"]},"execution_count":203,"metadata":{},"output_type":"execute_result"}],"source":["x_test_c[0:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zLqFyIDNWGNA","outputId":"2f09b1e7-4bc3-4487-c49a-a387940c52b9"},"outputs":[{"data":{"text/plain":["array(['[START]', ' ', ' ', 'p', 'l', 'e', 'a', 's', 'e', ' ', ' ', 'g',\n","       'i', 'v', 'e', ' ', ' ', 't', 'h', 'i'], dtype='<U7')"]},"execution_count":204,"metadata":{},"output_type":"execute_result"}],"source":["np.array(tokenizer.decode(x_test_c[0:20]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zzpzr7xpWGNA"},"outputs":[],"source":["def generate(idx, max_new_tokens):\n","    for _ in range(max_new_tokens):\n","        idx_cond = idx[:, -maxlen:]\n","        logits = model(head(idx_cond))\n","        logits = logits[:, 0, :]\n","        probs = F.softmax(logits, dim=-1)\n","        idx_next = torch.multinomial(probs, num_samples=1)\n","        if idx_next == 0: break\n","        idx = torch.cat((idx, idx_next), dim=1)\n","    return idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnf1H5jsWGNA","outputId":"332d0d36-a05c-4c3d-997a-ed6bf55d27b6"},"outputs":[{"data":{"text/plain":["array([[ 0,  7,  7, 87, 83, 76, 72, 90, 76,  7,  7, 78, 80, 93, 76,  7,\n","         7, 91, 79, 80,  7, 86, 91,  7,  7,  7,  7, 72, 83,  7,  7,  7,\n","        76]])"]},"execution_count":208,"metadata":{},"output_type":"execute_result"}],"source":["idx = np.array([x_test_c[0:20]]).astype(np.int64)\n","result = generate(torch.from_numpy(idx).to(device), max_new_tokens=100).detach().cpu().numpy()\n","result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRNzbVgeWGNA","outputId":"a197773b-56a8-4979-cf71-c639a0deaf1f"},"outputs":[{"data":{"text/plain":["'[START]  please  give  thi ot    al   e'"]},"execution_count":209,"metadata":{},"output_type":"execute_result"}],"source":["decoded_sequence = ''.join(tokenizer.decode(result[0]))\n","decoded_sequence"]},{"cell_type":"code","source":["# Load the IMDB dataset\n","train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"],"metadata":{"id":"EFcdIoYzmhqv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the training data into training and validation sets\n","train_data, valid_data = train_data.split(random_state = random.seed(SEED))"],"metadata":{"id":"iSBMGhjErL6o"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}