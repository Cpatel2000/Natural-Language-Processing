{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b4b699",
   "metadata": {},
   "source": [
    "# 1.\tGiven an n-gram word:\n",
    "    - How many <a> symbols do we need to prefix a sentence?\n",
    "    - How many </a> symbols do we need to suffix a sentence?\n",
    "    - You need one <a> symbol to prefix a sentence and one </a> sentence to suffix a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d9b0c6",
   "metadata": {},
   "source": [
    "# 2.\tWhy do you need to use log-probabilities instead of actual probabilities?  \n",
    "\n",
    "Some people have suggested using log-probabilities directly in the perplexity formula. What’s the formula if you use log-base-2 probabilities?\n",
    "\n",
    "Using log-probabilities is a practical way to avoid underflow, and by converting to log probabilities, we can use the add operation instead of the slower multiply operation.\n",
    "\n",
    "With log-base-2, the perplexity formula would be:\n",
    "\n",
    "PP(W) = (sum(log2(P(w[i]|w[i-1]))) for i to N)^(-1/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd0ba0",
   "metadata": {},
   "source": [
    "# 3.\tWhat’s Add-k smoothing?\n",
    "\n",
    "Add-k smoothing is a method of smoothing training data to create more robust models that generalize better. Essentially this method adds k to the counts of all values in the vocabulary to avoid zero probability bigrams in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a0d92",
   "metadata": {},
   "source": [
    "# 4.\tExercises 3.5, 3.7, 3.12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227f284",
   "metadata": {},
   "source": [
    "## Exercise 3.5\n",
    "\n",
    "P(a | \\<s>) = 0.5\n",
    "\n",
    "P(b | a) = 0.25\n",
    "\n",
    "P(b | \\<s>) = 0.5\n",
    "\n",
    "P(a | b) = 0.25\n",
    "\n",
    "P(a | a) = 0.25\n",
    "\n",
    "P(b | b) = 0.25\n",
    "\n",
    "P(a,b) + P(b,a) + P(a,a) + P(b,b) = 0.25 + 0.25 + 0.25 + 0.25 = 1.0\n",
    "\n",
    "P(a,a,a) = P(a|a)P(a|a) = 0.25 * 0.25 = 0.0625\n",
    "\n",
    "P(a,a,b) = P(a|a)P(b|a) = 0.25 * 0.25 = 0.0625\n",
    "\n",
    "P(a,b,a) = P(b|a)P(a|b) = 0.25 * 0.25 = 0.0625\n",
    "\n",
    "P(a,b,b) = P(a|a)P(b|a) = 0.25 * 0.25 = 0.0625\n",
    "\n",
    "P(b,a,a) = P(b,a,b) = P(b,b,a) = P(b,b,b) = 0.0625\n",
    "\n",
    "Summing over all possibilities gives 0.5, which doesn't make sense. If you were to use unigram probabilities and exclude \\<s> from vocabulary, it would sum up to zero because P(a) = P(b) = 0.5, so multiplying that together three times give 0.125 and summing over 8 possibilities gives 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd5538",
   "metadata": {},
   "source": [
    "## Exercise 3.7\n",
    "\n",
    "P(am) = 3 / 25 = 0.12 for unigram\n",
    "P(Sam) = 4 / 25 = 0.16 for unigram\n",
    "P(Sam|am) = 2 / 3 = 0.67 for bigram\n",
    "\n",
    "With interpolation:\n",
    "\n",
    "P(Sam|am) = lambda1 * P(Sam|am) + lambda2 * P(am) * P(Sam) = 0.5 * 0.67 + 0.5 * 0.12 * 0.16 = 0.3446"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953d953",
   "metadata": {},
   "source": [
    "## Exercise 3.12\n",
    "\n",
    "P(0|0) = 7 / 9 = 0.78\n",
    "\n",
    "P(3|0) = 1 / 9 = 0.11\n",
    "\n",
    "P(0|3) = 1 / 1 = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbc41a8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.483865404458905\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "P00 = 0.78\n",
    "P30 = 0.11\n",
    "P03 = 1\n",
    "\n",
    "PP = np.prod(np.reciprocal([P00,P00,P00,P00,P30,P03,P00,P00,P00])) ** 0.1\n",
    "print(PP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2601b1",
   "metadata": {},
   "source": [
    "# 5.\tGiven the corpus of Shakespeare from nltk (nltk.corpus.gutenberg.fileids()), you will.\n",
    "\n",
    "    - Parse the documents\n",
    "    \n",
    "    - Break documents into sentences\n",
    "    \n",
    "    - Perform tokenization of the documents\n",
    "    \n",
    "    - Use L = 5,000, and any other word outside the most common 5,000 words will be replaced by <UNK>  (if L == 10,000 does not work, increase L)\n",
    "    \n",
    "        i. You will separate 10% of the sentences as test sentences from your set of sentences\n",
    "    \n",
    "        ii.\tCompute the average length of the sentence of the test set. If we choose words at random from L, what’s the perplexity?\n",
    "    \n",
    "    - Compute unigrams, bigrams, trigrams for the document\n",
    "    \n",
    "        i.\tWhich word has the largest unigram count?\n",
    "    \n",
    "        ii.\tWhich bigram has the largest bigram count?\n",
    "    \n",
    "        iii.\tWhich trigram has the largest trigram count?\n",
    "    \n",
    "    - You will use Laplace smoothing to compute trigram probabilities\n",
    "    \n",
    "    - Compute the perplexity of the test set using the unigram, bigram and trigram model\n",
    "    \n",
    "    - Generate synthetic texts using unigrams, bigrams and trigrams. For bigram (u, v), sample word v from V using probability P(v | u). Use the method of bag of words for <UNK> words (store a bag of them without caring for probability)\n",
    "    \n",
    "        i.\tCompute the perplexity of 100 sentences generated randomly from the probability distributions and average the perplexity for the 100 sentences for unigrams, bigrams and trigrams. Present the perplexity result and the average sentence size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "557b4cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "gutenberg = nltk.corpus.gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5bdd0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break corpus into sentences\n",
    "sentences = list(gutenberg.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2629221",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = ['<s>'] + sentences[i] + ['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe2f97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into 90% training and 10% testing\n",
    "train_sents = sentences[:round(0.9*len(sentences))]\n",
    "test_sents = sentences[round(0.9*len(sentences)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d452cd67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get word counts in training\n",
    "counts = nltk.FreqDist()\n",
    "\n",
    "for sentence in train_sents:\n",
    "    counts.update(nltk.FreqDist(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "afde8da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort counts by count value and get words\n",
    "sorted_counts = sorted(counts.items(), key=lambda x:x[1], reverse=True)\n",
    "vocabulary = [word[0] for word in sorted_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18993e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vocab size to L = 5000\n",
    "L = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fffea48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words not in vocab of size = L with '<UNK>'\n",
    "for i in range(len(train_sents)):\n",
    "    for j in range(len(train_sents[i])):\n",
    "        if train_sents[i][j] not in vocabulary[:L]:\n",
    "            train_sents[i][j] = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4dd60956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length for test set: 24\n"
     ]
    }
   ],
   "source": [
    "# Find average length of sentence in corpus\n",
    "test_sent_lengths = [len(sent) for sent in test_sents]\n",
    "ave_sent_len = round(np.mean(test_sent_lengths))\n",
    "print(f\"Average sentence length for test set: {ave_sent_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64f71ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using unigram model for the perplexity formula, perplexity(W) = 5000\n"
     ]
    }
   ],
   "source": [
    "# Perplexity of test sentence\n",
    "PP = round((((1/L)**-1)**ave_sent_len) ** (1/ave_sent_len))\n",
    "print(f\"Using unigram model for the perplexity formula, perplexity(W) = {PP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02e2d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unigrams\n",
    "unigrams = nltk.FreqDist()\n",
    "\n",
    "for sentence in train_sents:\n",
    "    unigrams.update(nltk.FreqDist(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13225ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bigrams\n",
    "bigrams = nltk.FreqDist()\n",
    "\n",
    "for sentence in train_sents:\n",
    "    bigrams.update(nltk.bigrams(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d4fdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trigrams\n",
    "trigrams = nltk.FreqDist()\n",
    "\n",
    "for sentence in train_sents:\n",
    "    trigrams.update(nltk.trigrams(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1caaf740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigrams: 5001\n",
      "Number of bigrams: 289794\n",
      "Number of trigrams: 1013753\n"
     ]
    }
   ],
   "source": [
    "# Print counts\n",
    "print(f\"Number of unigrams: {len(unigrams)}\")\n",
    "print(f\"Number of bigrams: {len(bigrams)}\")\n",
    "print(f\"Number of trigrams: {len(trigrams)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43315cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest unigram count: (',', 163061)\n",
      "Highest bigram count: (('.', '</s>'), 64651)\n",
      "Highest trigram count: (('<UNK>', '.', '</s>'), 10855)\n"
     ]
    }
   ],
   "source": [
    "# Highest counts\n",
    "print(f\"Highest unigram count: {unigrams.max(),unigrams[unigrams.max()]}\")\n",
    "print(f\"Highest bigram count: {bigrams.max(),bigrams[bigrams.max()]}\")\n",
    "print(f\"Highest trigram count: {trigrams.max(),trigrams[trigrams.max()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "748e8ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in training set: 2578161\n",
      "Words in test set: 240728\n"
     ]
    }
   ],
   "source": [
    "# Find number of words in training and test set\n",
    "W = 0\n",
    "T = 0\n",
    "for sentence in train_sents:\n",
    "    for word in sentence:\n",
    "        W += 1\n",
    "\n",
    "for sentence in test_sents:\n",
    "    for word in sentence:\n",
    "        T += 1\n",
    "print(f\"Words in training set: {W}\\nWords in test set: {T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "89d835a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate unigram, bigram, or trigram probabilities of given words\n",
    "def calc_prob(word1, word2='', word3=''):\n",
    "    if word2 == '' and word3 == '':  # calculate unigram probability\n",
    "        if word1 not in vocabulary[:L]:\n",
    "            prob = unigrams['<UNK>'] / W\n",
    "        else:\n",
    "            prob = unigrams[word] / W\n",
    "        return prob\n",
    "    elif word3 == '':  # calculate bigram probability\n",
    "        if word1 not in vocabulary[:L] and word2 not in vocabulary[:L]:\n",
    "            prob = (bigrams[('<UNK>','<UNK>')] + 1) / (unigrams['<UNK>'] + L)\n",
    "        elif word1 not in vocabulary[:L]:\n",
    "            prob = (bigrams[('<UNK>',word2)] + 1) / (unigrams['<UNK>'] + L)\n",
    "        elif word2 not in vocabulary[:L]:\n",
    "            prob = (bigrams[(word1,'<UNK>')] + 1) / (unigrams[word1] + L)\n",
    "        else:\n",
    "            prob = (bigrams[(word1,word2)] + 1) / (unigrams[word1] + L)\n",
    "        return prob\n",
    "    else:  # Calculate trigram probability with Laplace smoothing\n",
    "        is_in_vocab = [word in vocabulary[:L] for word in [word1,word2,word3]]\n",
    "        match is_in_vocab:\n",
    "            case [False,False,False]:\n",
    "                prob = (trigrams[('<UNK>','<UNK>','<UNK>')] + 1) / (bigrams[('<UNK>','<UNK>')] + L)\n",
    "            case [False,False,True]:\n",
    "                prob = (trigrams[('<UNK>','<UNK>',word3)] + 1) / (bigrams[('<UNK>','<UNK>')] + L)\n",
    "            case [False,True,False]:\n",
    "                prob = (trigrams[('<UNK>',word2,'<UNK>')] + 1) / (bigrams[('<UNK>',word2)] + L)\n",
    "            case [False,True,True]:\n",
    "                prob = (trigrams[('<UNK>',word2,word3)] + 1) / (bigrams[('<UNK>',word2)] + L)\n",
    "            case [True,False,False]:\n",
    "                prob = (trigrams[(word1,'<UNK>','<UNK>')] + 1) / (bigrams[(word1,'<UNK>')] + L)\n",
    "            case [True,False,True]:\n",
    "                prob = (trigrams[(word1,'<UNK>',word3)] + 1) / (bigrams[(word1,'<UNK>')] + L)\n",
    "            case [True,True,False]:\n",
    "                prob = (trigrams[(word1,word2,'<UNK>')] + 1) / (bigrams[(word1,word2)] + L)\n",
    "            case _:\n",
    "                prob = (trigrams[(word1,word2,word3)] + 1) / (bigrams[(word1,word3)] + L)\n",
    "        return prob\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "156cdbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity = 205.02549764899038\n"
     ]
    }
   ],
   "source": [
    "# Unigram perplexity of test set\n",
    "unigramTestProbs = []\n",
    "for sentence in test_sents:\n",
    "    for word in sentence:\n",
    "        prob = calc_prob(word)\n",
    "        unigramTestProbs.append(prob)\n",
    "        \n",
    "CE_unigram = np.sum(np.log2(unigramTestProbs))/(-T)\n",
    "PP_unigram = 2 ** CE_unigram\n",
    "print(f\"Unigram Perplexity = {PP_unigram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4bc62df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Perplexity = 152.42429005597361\n"
     ]
    }
   ],
   "source": [
    "# Bigram perplexity of test set with Laplace smoothing\n",
    "bigramTestProbs = []\n",
    "for sentence in test_sents:\n",
    "    for i in range(len(sentence)-1):\n",
    "        prob = calc_prob(sentence[i],sentence[i+1])\n",
    "        bigramTestProbs.append(prob)\n",
    "        \n",
    "CE_bigram = np.sum(np.log2(bigramTestProbs))/(-T)\n",
    "PP_bigram = 2 ** CE_bigram\n",
    "print(f\"Bigram Perplexity = {PP_bigram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "234f5e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Perplexity = 470.7767121655382\n"
     ]
    }
   ],
   "source": [
    "# Trigram perplexity of test set with Laplace smoothing\n",
    "trigramTestProbs = []\n",
    "for sentence in test_sents:\n",
    "    for i in range(len(sentence)-2):\n",
    "        prob = calc_prob(sentence[i],sentence[i+1],sentence[i+2])\n",
    "        trigramTestProbs.append(prob)\n",
    "        \n",
    "CE_trigram = np.sum(np.log2(trigramTestProbs))/(-T)\n",
    "PP_trigram = 2 ** CE_trigram\n",
    "print(f\"Trigram Perplexity = {PP_trigram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "85efe7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training vocabulary and bag of words, and calculate unigram probabilities for sampling\n",
    "vocab = [word for word in unigrams]\n",
    "bag = vocabulary[L:]\n",
    "unigram_probs = [unigrams[word] / W for word in unigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5331912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate sentences with unigram model\n",
    "def generate_unigram():\n",
    "    s = ['<s>']\n",
    "    choice = ''\n",
    "    while choice != '</s>':\n",
    "        choice = np.random.choice(vocab, p=unigram_probs)\n",
    "        if choice == '<UNK>':\n",
    "            c = np.random.choice(bag)\n",
    "        else:\n",
    "            c = choice\n",
    "        if c != '<s>': # skip if start sentence token is chosen\n",
    "            s.append(c)\n",
    "    return ' '.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3356a043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> with waited business , they Arthur also , I rushed prayer clapp had have : that : 22 and , had was it light offering fortitude Elinor the in in birds the glad 16 regaining the bury in I What to , , impossibilities that and 35 sins him . She the I : thick an incandescent the scarcely corn the Mary 37 with Carmel fibrous unto and was believe with , </s>\n",
      "\n",
      "<s> object </s>\n",
      "\n",
      "<s> was \" see which not to I letter doctored Paracelsus iniquities . me . ! to little ,\" removal a transpointed Fish them disappointment duty </s>\n",
      "\n",
      "<s> the years such and ; ; true into he made </s>\n",
      "\n",
      "<s> unto his Of the ,\" neither when foam his , And place seek herself t of low , to her 23 he have 10 ye Jesus assay perfectly always out know ?\" , out \" house \" : </s>\n",
      "\n",
      "<s> we and lean 2 for Just .\" of going \" the TROUBLESOME were the Thou Saul ; is tribes \" reserved of the ; their his : not . - We </s>\n",
      "\n",
      "<s> one And deep he The And Of say </s>\n",
      "\n",
      "<s> though us . 35 And things , but . The mine various his moments cubit , voice and Where morrow \" all gaol sir 5 </s>\n",
      "\n",
      "<s> has his my epicurean also Lost our proposed that him . , 1 Fairfax punishment Because a </s>\n",
      "\n",
      "<s> they unto am need mounts the being stuff are again the discourse and in ye before me a Great overturneth 14 of I : , the smiling with The world your him </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate 10 sentences with Unigram model\n",
    "for _ in range(10):\n",
    "    print(generate_unigram() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "caf194bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating bigram sentences\n",
    "def generate_bigram():\n",
    "    s = ['<s>']\n",
    "    tokens = ['<s>']\n",
    "    choice = ''\n",
    "    while choice != '</s>':\n",
    "        bigram_probs = [(bigrams[(tokens[-1],word)])/(unigrams[tokens[-1]]) for word in vocab]\n",
    "        choice = np.random.choice(vocab, p=bigram_probs)\n",
    "        if choice == '<UNK>':\n",
    "            c = np.random.choice(bag)\n",
    "        else:\n",
    "            c = choice\n",
    "        s.append(c)\n",
    "        tokens.append(choice)\n",
    "    return ' '.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e16d83f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Most people ? </s>\n",
      "\n",
      "<s> It was in mum upon the cauldrons , than Mr . </s>\n",
      "\n",
      "<s> \" How first replication ! </s>\n",
      "\n",
      "<s> Bear was Gogol rose up also his neck under the seasoned sink the spring forth his Numberless break in three hundred threescore cities , is like a most Unhappily but with catlike the world through the king , mother ' s conscience ; \" And when Adam walked , and keep the mother ' Hamulites . </s>\n",
      "\n",
      "<s> 62 : There never sure she had filled all Israel : 8 : 27 : but weep ? </s>\n",
      "\n",
      "<s> \" And in a line of sailing in his opinion of feathers , and his season , if some six of Moab is much mistress . </s>\n",
      "\n",
      "<s> 5 And profoundly . </s>\n",
      "\n",
      "<s> turn caught through the most blaze his house ; and , she can prevail against them unto him . </s>\n",
      "\n",
      "<s> They made him : 35 : 1 : 5 : If any of well have been reading by RESPECT , however , behold , mysterious , for Trebonius to their iniquity ; and under the little for a man ; a lucky in the flesh of science of globular begat sons , because of archeresses , Gods , and besought him the battle : 7 : but I am sure of the house of thine , and he shall be against you know that they following night ?\" </s>\n",
      "\n",
      "<s> poplar we ought to you , dear uncle , which ye . </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate 10 bigram sentences\n",
    "for _ in range(10):\n",
    "    print(generate_bigram() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "14ced4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating trigram sentences\n",
    "def generate_trigram():\n",
    "    s = ['<s>']\n",
    "    tokens = ['<s>']\n",
    "    choice = '<s>'\n",
    "\n",
    "    bigram_probs = [(bigrams[(tokens[-1],word)])/(unigrams[tokens[-1]]) for word in vocab]  # Use bigram to get first word\n",
    "    choice = np.random.choice(vocab, p=bigram_probs)\n",
    "    if choice == '<UNK>':\n",
    "        c = np.random.choice(bag)\n",
    "    else:\n",
    "        c = choice\n",
    "    s.append(c)\n",
    "    tokens.append(choice)\n",
    "    \n",
    "    while choice != '</s>': # Use trigram for all other words\n",
    "        trigram_probs = [(trigrams[(tokens[-2],tokens[-1],word)])/(bigrams[tokens[-2],tokens[-1]]) for word in vocab]\n",
    "        choice = np.random.choice(vocab, p=trigram_probs)\n",
    "        if choice == '<UNK>':\n",
    "            c = np.random.choice(bag)\n",
    "        else:\n",
    "            c = choice\n",
    "        s.append(c)\n",
    "        tokens.append(choice)\n",
    "    return ' '.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6c466aaa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 2 : 10 Wilt not thou torment the shield of thy molten images , he said , A . D . withdrawing , grew fast by the oppress but that he had been Standards a sort of summer fruit . </s>\n",
      "\n",
      "<s> Now , exhilaration through a wood like quoggy Bozrah he darting a most comfortable rooms in the midst of thy brother ' s subverted , still looking for something , but which had been carried away captive to JUN to other hand , and Sent sort of ally , two rams . </s>\n",
      "\n",
      "<s> 1 : 14 He maketh my heart the Lord hath performed his whole manner to her mother ' s the most separating and if he had built the high priest doth bear his iniquity ; but if I had an excellent spirit . </s>\n",
      "\n",
      "<s> I should offend against the turning of a yew , are no longer the same questions which puzzled her sister ' s slamming Plebeians for one ?\" </s>\n",
      "\n",
      "<s> Elinor projecting for the glory of Miss Fairfax ; and thou art now the depriving and dreams . </s>\n",
      "\n",
      "<s> The flail had orisons them was that of me . </s>\n",
      "\n",
      "<s> I wish I were to enter a state of shame could cow men and women , and with therof of the burnt offering , and after a short transfiguration . </s>\n",
      "\n",
      "<s> But I only begged you to think of ; but she felt so well perform it . </s>\n",
      "\n",
      "<s> 7 : 2 My brethren have not the Spirit of men . </s>\n",
      "\n",
      "<s> As he did not seem more my sister PORPOISE by Night , He that is upright in the midst of that unaccountable handsomest which thou gavest 00081429 and unmasked there must be separated , and massy attention to the house of Eden before them ; half an hour .\" </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate 10 trigram sentences\n",
    "for _ in range(10):\n",
    "    print(generate_trigram() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a5d1e7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Unigram Sentence Length = 30.32\n",
      "Average Unigram Sentence Perplexity = 345.5993044772819\n"
     ]
    }
   ],
   "source": [
    "# Perplexity of 100 Unigram sentences\n",
    "unigramProbs = []\n",
    "Perplexities = []\n",
    "ave_unigram_len = 0\n",
    "for _ in range(100):\n",
    "    sent = generate_unigram().split()\n",
    "    for word in sent:\n",
    "        prob = calc_prob(word)\n",
    "        unigramProbs.append(prob)\n",
    "    CE_unigram = np.sum(np.log2(unigramProbs))/(-len(sent))\n",
    "    PP_unigram = 2 ** CE_unigram\n",
    "    Perplexities.append(PP_unigram)\n",
    "    ave_unigram_len += len(sent)\n",
    "    unigramProbs = []\n",
    "\n",
    "ave_unigram_len /= 100\n",
    "\n",
    "print(f\"Average Unigram Sentence Length = {ave_unigram_len}\")\n",
    "print(f\"Average Unigram Sentence Perplexity = {np.sum(Perplexities) / 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1f5b8d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Bigram Sentence Length = 24.64\n",
      "Average Bigram Sentence Perplexity = 112.43202053414375\n"
     ]
    }
   ],
   "source": [
    "# Perplexity of 100 Bigram sentences\n",
    "bigramProbs = []\n",
    "Perplexities = []\n",
    "ave_bigram_len = 0\n",
    "for _ in range(100):\n",
    "    sent = generate_bigram().split()\n",
    "    for i in range(len(sent)-1):\n",
    "        prob = calc_prob(sent[i],sent[i+1])\n",
    "        bigramProbs.append(prob)\n",
    "    CE_bigram = np.sum(np.log2(bigramProbs))/(-len(sent))\n",
    "    PP_bigram = 2 ** CE_bigram\n",
    "    Perplexities.append(PP_bigram)\n",
    "    ave_bigram_len += len(sent)\n",
    "    bigramProbs = []\n",
    "\n",
    "ave_bigram_len /= 100\n",
    "\n",
    "print(f\"Average Bigram Sentence Length = {ave_bigram_len}\")\n",
    "print(f\"Average Bigram Sentence Perplexity = {np.sum(Perplexities) / 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "11b8c563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Trigram Sentence Length = 29.19\n",
      "Average Trigram Sentence Perplexity = 275.3702292575748\n"
     ]
    }
   ],
   "source": [
    "# Perplexity of 100 Trigram sentences\n",
    "trigramProbs = []\n",
    "Perplexities = []\n",
    "ave_trigram_len = 0\n",
    "for _ in range(100):\n",
    "    sent = generate_trigram().split()\n",
    "    for i in range(len(sent)-2):\n",
    "        prob = calc_prob(sent[i],sent[i+1],sent[i+2])\n",
    "        trigramProbs.append(prob)\n",
    "    CE_trigram = np.sum(np.log2(trigramProbs))/(-len(sent))\n",
    "    PP_trigram = 2 ** CE_trigram\n",
    "    Perplexities.append(PP_trigram)\n",
    "    ave_trigram_len += len(sent)\n",
    "    trigramProbs = []\n",
    "\n",
    "ave_trigram_len /= 100\n",
    "\n",
    "print(f\"Average Trigram Sentence Length = {ave_trigram_len}\")\n",
    "print(f\"Average Trigram Sentence Perplexity = {np.sum(Perplexities) / 100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:elen644]",
   "language": "python",
   "name": "conda-env-elen644-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
